{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBwKWYPt-B8k"
      },
      "source": [
        "# Team USA Olympic & Paralympic Data Pipeline\n",
        "### From Raw CSVs â†’ BigQuery-Ready Unified Tables\n",
        "\n",
        "**Purpose:** Acquire, profile, clean, merge, and enrich Olympic + Paralympic data for Team USA athletes across all available Games years.\n",
        "\n",
        "**Output Tables:**\n",
        "- `team_usa_athletes` â€” One row per unique athlete (identity, physical attributes, classification codes, career stats, AI-generated profiles, embeddings)\n",
        "- `team_usa_results` â€” One row per athlete Ã— event Ã— Games (the fact table)\n",
        "\n",
        "**Pipeline Phases:**\n",
        "- **Phase 0:** Data acquisition & profiling (assess raw sources, coverage, quality)\n",
        "- **Phase 1:** Olympic athlete backbone (keithgalli bios + results â†’ career stats)\n",
        "- **Phase 2:** Paralympic athlete backbone (piterfm athletes + katiepress medalists â†’ classification codes)\n",
        "- **Phase 3:** Unify athletes table (merge Olympic + Paralympic, normalize, deduplicate)\n",
        "- **Phase 4:** Unified results table (all medal/event records)\n",
        "- **Phase 5:** Enrichment (Gemini profile summaries, vector embeddings)\n",
        "- **Phase 6:** Final validation & GCS export\n",
        "\n",
        "**Data sources:** gs://class-demo/team-usa/raw/  \n",
        "**Processed output:** gs://class-demo/team-usa/processed/  \n",
        "**Enriched output:** gs://class-demo/team-usa/enriched/\n",
        "\n",
        "Lets get started with:\n",
        "## Phase 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Q2vxoa-B8m"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "Our raw data is staged in GCS at `gs://class-demo/team-usa/raw/`. We pull it into the local runtime for pandas processing. Colab Enterprise handles GCS authentication automatically through the attached service account â€” no keys needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU6j3NA6-B8m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Configuration\n",
        "BUCKET = 'gs://class-demo/team-usa'\n",
        "RAW_PATH = f'{BUCKET}/raw'\n",
        "LOCAL_DIR = '/tmp/olympic-data'\n",
        "\n",
        "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
        "\n",
        "PROJECT_ID = \"qwiklabs-gcp-01-bafc8841fc77\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "print(f'Project: {PROJECT_ID}')\n",
        "print(f'Region: {REGION}')\n",
        "print(f'Source: {RAW_PATH}')\n",
        "print(f'Local: {LOCAL_DIR}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm7c21af-B8m"
      },
      "source": [
        "## 2. Pull Raw Data from GCS\n",
        "\n",
        "Download all raw CSV files from the staging bucket. The `-m` flag enables parallel transfers. This should take under a minute for the full set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n42A6W6v-B8n"
      },
      "outputs": [],
      "source": [
        "!gsutil -m cp -r {RAW_PATH}/* {LOCAL_DIR}/\n",
        "\n",
        "# Count what we got\n",
        "csv_count = !find {LOCAL_DIR} -name \"*.csv\" | wc -l\n",
        "print(f'\\nTotal CSV files downloaded: {csv_count[0].strip()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dV-ewcx-B8n"
      },
      "source": [
        "## 3. Inventory All Files\n",
        "\n",
        "Before diving into profiling, let's see exactly what landed â€” file names, sizes, and directory structure. This confirms the downloads are complete and gives us a roadmap for the profiling loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19gqdpZ8-B8n"
      },
      "outputs": [],
      "source": [
        "# Walk the directory tree and catalog every CSV\n",
        "file_inventory = []\n",
        "\n",
        "for root, dirs, files in os.walk(LOCAL_DIR):\n",
        "    # Skip .git directories\n",
        "    dirs[:] = [d for d in dirs if d != '.git']\n",
        "    for f in sorted(files):\n",
        "        if f.endswith('.csv'):\n",
        "            full_path = os.path.join(root, f)\n",
        "            size_mb = os.path.getsize(full_path) / (1024 * 1024)\n",
        "            rel_path = full_path.replace(LOCAL_DIR + '/', '')\n",
        "            dataset = rel_path.split('/')[0]\n",
        "            file_inventory.append({\n",
        "                'dataset': dataset,\n",
        "                'file': f,\n",
        "                'rel_path': rel_path,\n",
        "                'full_path': full_path,\n",
        "                'size_mb': size_mb\n",
        "            })\n",
        "\n",
        "inv_df = pd.DataFrame(file_inventory)\n",
        "\n",
        "# Summary by dataset\n",
        "print('FILES BY DATASET:\\n')\n",
        "for dataset, group in inv_df.groupby('dataset'):\n",
        "    total_mb = group['size_mb'].sum()\n",
        "    print(f'{dataset}/ ({len(group)} files, {total_mb:.1f} MB total)')\n",
        "    for _, row in group.iterrows():\n",
        "        print(f'  {row[\"file\"]:45s} {row[\"size_mb\"]:8.2f} MB')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp5Bt-dR-B8n"
      },
      "source": [
        "## 4. Profile Every CSV\n",
        "\n",
        "This is the core profiling loop. For each file we capture:\n",
        "\n",
        "- **Shape** â€” row and column counts\n",
        "- **Schema** â€” column names, data types, and null percentages\n",
        "- **Sample data** â€” first 3 rows to understand format and content\n",
        "- **Team USA count** â€” how many rows match USA after filtering (our target population)\n",
        "- **Event samples** â€” particularly important for Paralympic files, where classification codes like T54 or S6 may appear inside event names rather than in a dedicated column\n",
        "\n",
        "The output is intentionally verbose. We need the full picture before making merge decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpsWxF6N-B8n"
      },
      "outputs": [],
      "source": [
        "# Store profiling results for later summary\n",
        "profiles = {}\n",
        "\n",
        "for _, row in inv_df.iterrows():\n",
        "    csv_file = row['full_path']\n",
        "    rel_path = row['rel_path']\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file, low_memory=False)\n",
        "    except Exception as e:\n",
        "        print(f'\\nERROR reading {rel_path}: {e}')\n",
        "        continue\n",
        "\n",
        "    # Store for later use\n",
        "    profiles[rel_path] = {\n",
        "        'rows': df.shape[0],\n",
        "        'cols': df.shape[1],\n",
        "        'columns': list(df.columns),\n",
        "        'dtypes': {col: str(df[col].dtype) for col in df.columns},\n",
        "        'null_pcts': {col: round(df[col].isnull().mean() * 100, 1) for col in df.columns}\n",
        "    }\n",
        "\n",
        "    # ---- Print profile ----\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f'FILE: {rel_path}')\n",
        "    print(f'Shape: {df.shape[0]:,} rows x {df.shape[1]} columns')\n",
        "\n",
        "    # Schema with null counts\n",
        "    print(f'\\nSchema:')\n",
        "    for col in df.columns:\n",
        "        null_count = df[col].isnull().sum()\n",
        "        null_pct = df[col].isnull().mean() * 100\n",
        "        print(f'  {col:35s} {str(df[col].dtype):10s} nulls: {null_count:>8,} ({null_pct:5.1f}%)')\n",
        "\n",
        "    # Sample rows\n",
        "    print(f'\\nFirst 3 rows:')\n",
        "    print(df.head(3).to_string())\n",
        "\n",
        "    # ---- Team USA filter ----\n",
        "    country_cols = [c for c in df.columns if any(\n",
        "        term in c.lower() for term in ['country', 'noc', 'nationality', 'team']\n",
        "    )]\n",
        "\n",
        "    usa_found = False\n",
        "    for col in country_cols:\n",
        "        usa_mask = df[col].astype(str).str.contains(\n",
        "            r'\\bUSA\\b|United States|United States of America',\n",
        "            case=False, na=False, regex=True\n",
        "        )\n",
        "        usa_count = usa_mask.sum()\n",
        "        if usa_count > 0:\n",
        "            usa_found = True\n",
        "            profiles[rel_path]['usa_count'] = usa_count\n",
        "            profiles[rel_path]['usa_filter_col'] = col\n",
        "            print(f'\\n  ðŸ‡ºðŸ‡¸ USA rows (filtered on \\'{col}\\'): {usa_count:,}')\n",
        "            print(f'  USA sample:')\n",
        "            print(df[usa_mask].head(3).to_string())\n",
        "            break\n",
        "\n",
        "    if not usa_found:\n",
        "        print(f'\\n  âš ï¸  No USA rows found. Country-like columns checked: {country_cols}')\n",
        "\n",
        "    # ---- Event/discipline samples ----\n",
        "    event_cols = [c for c in df.columns if any(\n",
        "        term in c.lower() for term in ['event', 'discipline', 'sport']\n",
        "    )]\n",
        "    for col in event_cols:\n",
        "        unique_vals = df[col].dropna().unique()\n",
        "        print(f'\\n  Sample values from \\'{col}\\' ({len(unique_vals):,} unique):')\n",
        "        for val in unique_vals[:15]:\n",
        "            print(f'    {val}')\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgotMYi5-B8n"
      },
      "source": [
        "## 5. Summary Dashboard\n",
        "\n",
        "Now that we've profiled everything, let's consolidate the key findings into a single view. This shows each file's row count, column count, and Team USA coverage at a glance â€” making it easy to compare sources and spot gaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1XgYggz-B8n"
      },
      "outputs": [],
      "source": [
        "# Build summary table\n",
        "summary_rows = []\n",
        "for path, profile in profiles.items():\n",
        "    summary_rows.append({\n",
        "        'File': path,\n",
        "        'Rows': profile['rows'],\n",
        "        'Columns': profile['cols'],\n",
        "        'USA Rows': profile.get('usa_count', 0),\n",
        "        'USA Filter': profile.get('usa_filter_col', 'N/A'),\n",
        "        'Key Columns': ', '.join(profile['columns'][:6]) + ('...' if len(profile['columns']) > 6 else '')\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_df = summary_df.sort_values('USA Rows', ascending=False)\n",
        "\n",
        "print('DATASET SUMMARY (sorted by USA row count):\\n')\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "print(f'\\n--- TOTALS ---')\n",
        "print(f'Total files profiled: {len(profiles)}')\n",
        "print(f'Total rows across all files: {sum(p[\"rows\"] for p in profiles.values()):,}')\n",
        "print(f'Total USA rows (sum, includes duplicates across datasets): {sum(p.get(\"usa_count\", 0) for p in profiles.values()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXODpgF--B8o"
      },
      "source": [
        "## 6. Critical Questions: Paralympic Classification Codes\n",
        "\n",
        "This is the single most important unknown. Do the Paralympic datasets include classification codes (T54, S6, F20, etc.) as structured data? Or are they embedded in event names like \"Men's 100m T54\"? Or missing entirely?\n",
        "\n",
        "We look at both Paralympic sources and search for classification patterns in all text columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjhxEgQc-B8o"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Classification code pattern: letter(s) followed by numbers, e.g., T54, S6, F20, SB14\n",
        "classification_pattern = re.compile(r'\\b[A-Z]{1,2}\\d{1,2}\\b')\n",
        "\n",
        "# Check Paralympic datasets specifically\n",
        "para_files = [row['full_path'] for _, row in inv_df.iterrows()\n",
        "              if 'paralympic' in row['dataset'].lower() or 'para' in row['dataset'].lower()]\n",
        "\n",
        "print('PARALYMPIC CLASSIFICATION CODE SEARCH\\n')\n",
        "\n",
        "for csv_file in para_files:\n",
        "    rel_path = csv_file.replace(LOCAL_DIR + '/', '')\n",
        "    df = pd.read_csv(csv_file, low_memory=False)\n",
        "\n",
        "    print(f'\\n--- {rel_path} ---')\n",
        "    print(f'Columns: {list(df.columns)}')\n",
        "\n",
        "    # Check if any column name suggests classification\n",
        "    class_cols = [c for c in df.columns if any(\n",
        "        term in c.lower() for term in ['class', 'category', 'impairment', 'disability']\n",
        "    )]\n",
        "    if class_cols:\n",
        "        print(f'  âœ… Possible classification columns: {class_cols}')\n",
        "        for col in class_cols:\n",
        "            print(f'     Sample values: {df[col].dropna().unique()[:20]}')\n",
        "    else:\n",
        "        print(f'  âŒ No dedicated classification column found')\n",
        "\n",
        "    # Search all text columns for classification patterns\n",
        "    print(f'\\n  Searching all text columns for classification patterns (e.g., T54, S6):')\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        matches = df[col].fillna('').astype(str).apply(\n",
        "            lambda x: classification_pattern.findall(x)\n",
        "        )\n",
        "        flat_matches = [m for sublist in matches for m in sublist]\n",
        "        if flat_matches:\n",
        "            unique_codes = sorted(set(flat_matches))\n",
        "            print(f'    \\'{col}\\': Found {len(unique_codes)} unique codes')\n",
        "            print(f'      Examples: {unique_codes[:25]}')\n",
        "            # Show sample rows with codes\n",
        "            has_code = matches.apply(len) > 0\n",
        "            print(f'      Sample rows:')\n",
        "            print(df.loc[has_code].head(3)[[col]].to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZf24HTQ-B8o"
      },
      "source": [
        "## 7. Critical Questions: Physical Attributes\n",
        "\n",
        "The \"find athletes with my build\" similarity search requires height, weight, and ideally age. Which datasets have these fields, and how well-populated are they â€” especially for Team USA athletes in the modern era?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGtDiK2G-B8o"
      },
      "outputs": [],
      "source": [
        "print('PHYSICAL ATTRIBUTE COVERAGE\\n')\n",
        "\n",
        "physical_terms = ['height', 'weight', 'age', 'birth', 'born', 'dob', 'bmi']\n",
        "\n",
        "for _, row in inv_df.iterrows():\n",
        "    csv_file = row['full_path']\n",
        "    rel_path = row['rel_path']\n",
        "    df = pd.read_csv(csv_file, low_memory=False)\n",
        "\n",
        "    phys_cols = [c for c in df.columns if any(term in c.lower() for term in physical_terms)]\n",
        "\n",
        "    if phys_cols:\n",
        "        print(f'\\n{rel_path}')\n",
        "\n",
        "        # Filter to USA if possible\n",
        "        country_cols = [c for c in df.columns if any(\n",
        "            term in c.lower() for term in ['country', 'noc', 'nationality', 'team']\n",
        "        )]\n",
        "        usa_df = df\n",
        "        for col in country_cols:\n",
        "            mask = df[col].astype(str).str.contains(\n",
        "                r'\\bUSA\\b|United States', case=False, na=False, regex=True\n",
        "            )\n",
        "            if mask.sum() > 0:\n",
        "                usa_df = df[mask]\n",
        "                print(f'  (Filtered to {usa_df.shape[0]:,} USA rows)')\n",
        "                break\n",
        "\n",
        "        for col in phys_cols:\n",
        "            non_null = usa_df[col].notna().sum()\n",
        "            total = len(usa_df)\n",
        "            pct = (non_null / total * 100) if total > 0 else 0\n",
        "            print(f'  {col:25s} {non_null:>6,} / {total:>6,} populated ({pct:.1f}%)')\n",
        "\n",
        "            # Show sample values\n",
        "            sample_vals = usa_df[col].dropna().head(5).tolist()\n",
        "            print(f'    Sample values: {sample_vals}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_CxxbWw-B8o"
      },
      "source": [
        "## 8. Critical Questions: Winter Paralympics\n",
        "\n",
        "We want both Summer and Winter. The Olympic datasets clearly cover both. But do the Paralympic datasets include Winter Games (PyeongChang 2018, Beijing 2022, Sochi 2014, etc.)? This determines whether we need additional sources for Winter Paralympic coverage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paYyKQyc-B8o"
      },
      "outputs": [],
      "source": [
        "print('WINTER PARALYMPIC COVERAGE SEARCH\\n')\n",
        "\n",
        "winter_terms = ['winter', 'beijing 2022', 'pyeongchang', 'sochi', 'vancouver 2010',\n",
        "                'turin', 'torino', 'salt lake', 'nagano', 'lillehammer']\n",
        "\n",
        "for csv_file in para_files:\n",
        "    rel_path = csv_file.replace(LOCAL_DIR + '/', '')\n",
        "    df = pd.read_csv(csv_file, low_memory=False)\n",
        "\n",
        "    print(f'\\n--- {rel_path} ---')\n",
        "\n",
        "    # Check all text columns for winter indicators\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        col_text = df[col].fillna('').astype(str).str.lower()\n",
        "        for term in winter_terms:\n",
        "            matches = col_text.str.contains(term, na=False)\n",
        "            if matches.sum() > 0:\n",
        "                print(f'  âœ… Found \\'{term}\\' in \\'{col}\\': {matches.sum()} rows')\n",
        "                print(f'     Sample: {df.loc[matches, col].unique()[:5]}')\n",
        "\n",
        "    # Check year/season columns\n",
        "    year_cols = [c for c in df.columns if any(\n",
        "        term in c.lower() for term in ['year', 'season', 'games', 'edition']\n",
        "    )]\n",
        "    for col in year_cols:\n",
        "        print(f'  \\'{col}\\' unique values: {sorted(df[col].dropna().unique())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqoliILZ-B8o"
      },
      "source": [
        "## 9. Schema Comparison Across Datasets\n",
        "\n",
        "Before we can merge anything, we need to understand how column names and data representations differ across sources. This comparison identifies which fields can be directly joined and which need mapping or renaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ew3uwRJH-B8o"
      },
      "outputs": [],
      "source": [
        "print('COLUMN NAMES ACROSS ALL DATASETS\\n')\n",
        "\n",
        "# Group files by dataset\n",
        "datasets = inv_df.groupby('dataset')['full_path'].apply(list).to_dict()\n",
        "\n",
        "# For each dataset, get the union of all column names from its files\n",
        "dataset_columns = {}\n",
        "for dataset_name, file_list in datasets.items():\n",
        "    all_cols = set()\n",
        "    for f in file_list:\n",
        "        try:\n",
        "            df = pd.read_csv(f, nrows=0)  # Just read headers\n",
        "            all_cols.update(df.columns)\n",
        "        except:\n",
        "            pass\n",
        "    dataset_columns[dataset_name] = sorted(all_cols)\n",
        "\n",
        "for dataset_name, cols in dataset_columns.items():\n",
        "    print(f'\\n{dataset_name}/ ({len(cols)} unique columns):')\n",
        "    for col in cols:\n",
        "        print(f'  {col}')\n",
        "\n",
        "# Find common columns across all datasets\n",
        "print(f'\\n{\"=\"*70}')\n",
        "print('COLUMN OVERLAP ANALYSIS:')\n",
        "all_col_sets = list(dataset_columns.values())\n",
        "if all_col_sets:\n",
        "    common = set(all_col_sets[0])\n",
        "    for s in all_col_sets[1:]:\n",
        "        common &= set(s)\n",
        "    print(f'\\nColumns present in ALL datasets: {sorted(common) if common else \"None\"}')\n",
        "\n",
        "    # Columns by frequency\n",
        "    col_freq = defaultdict(int)\n",
        "    for cols in all_col_sets:\n",
        "        for c in cols:\n",
        "            col_freq[c] += 1\n",
        "\n",
        "    print(f'\\nColumns by frequency (how many datasets contain them):')\n",
        "    for col, freq in sorted(col_freq.items(), key=lambda x: -x[1]):\n",
        "        datasets_with = [name for name, cols in dataset_columns.items() if col in cols]\n",
        "        joined = ', '.join(datasets_with)\n",
        "        print(f'  {col:35s} {freq}/{len(dataset_columns)}  ({joined})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp9LSwnz-B8o"
      },
      "source": [
        "## 10. Overlap and Duplication Check\n",
        "\n",
        "Several of these datasets cover the same time periods (e.g., KeithGalli and heesoo37 both cover pre-2016 Olympics). Before merging, we need to understand where coverage overlaps and which source is more complete in the overlap zones. This prevents double-counting athletes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovL82r3y-B8o"
      },
      "outputs": [],
      "source": [
        "print('TEMPORAL COVERAGE BY DATASET\\n')\n",
        "\n",
        "for _, row in inv_df.iterrows():\n",
        "    csv_file = row['full_path']\n",
        "    rel_path = row['rel_path']\n",
        "    df = pd.read_csv(csv_file, low_memory=False)\n",
        "\n",
        "    # Look for year-like columns\n",
        "    year_cols = [c for c in df.columns if any(\n",
        "        term in c.lower() for term in ['year', 'games', 'edition', 'date']\n",
        "    )]\n",
        "\n",
        "    if year_cols:\n",
        "        print(f'\\n{rel_path}')\n",
        "        for col in year_cols:\n",
        "            vals = df[col].dropna().unique()\n",
        "            # Try to extract years\n",
        "            years = set()\n",
        "            for v in vals:\n",
        "                found = re.findall(r'\\b(19\\d{2}|20\\d{2})\\b', str(v))\n",
        "                years.update(found)\n",
        "            if years:\n",
        "                year_range = sorted(years)\n",
        "                print(f'  \\'{col}\\': {year_range[0]} â€” {year_range[-1]} ({len(year_range)} distinct years)')\n",
        "                print(f'    Years: {year_range}')\n",
        "            else:\n",
        "                print(f'  \\'{col}\\': {sorted(vals)[:10]} (could not extract years)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmkuIvNi-B8o"
      },
      "source": [
        "## 11. Findings Summary\n",
        "\n",
        "Compile the key findings that will drive the merge strategy. This cell generates a structured report we can bring back to the lab planning conversation.\n",
        "\n",
        "**Key questions this must answer:**\n",
        "1. Do we have structured Paralympic classification codes?\n",
        "2. Which datasets have physical attributes (height/weight) for Team USA?\n",
        "3. What's the total Team USA record count (Olympic + Paralympic, Summer + Winter)?\n",
        "4. Do we have Winter Paralympic coverage?\n",
        "5. What time period does our data span?\n",
        "6. What's missing that we expected to have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkgimQUU-B8o"
      },
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('FINDINGS SUMMARY')\n",
        "print('='*70)\n",
        "\n",
        "print('\\n1. DATASET INVENTORY')\n",
        "print(f'   Total CSV files: {len(profiles)}')\n",
        "print(f'   Total rows (all countries): {sum(p[\"rows\"] for p in profiles.values()):,}')\n",
        "print(f'   Total USA rows (pre-dedup): {sum(p.get(\"usa_count\", 0) for p in profiles.values()):,}')\n",
        "\n",
        "print('\\n2. USA ROWS BY SOURCE')\n",
        "for path, profile in sorted(profiles.items(), key=lambda x: x[1].get('usa_count', 0), reverse=True):\n",
        "    usa = profile.get('usa_count', 0)\n",
        "    if usa > 0:\n",
        "        print(f'   {path:55s} {usa:>6,} USA rows')\n",
        "\n",
        "print('\\n3. ACTION ITEMS')\n",
        "print('   Review the output above and document:')\n",
        "print('   [ ] Paralympic classification code status (Section 6)')\n",
        "print('   [ ] Physical attribute availability (Section 7)')\n",
        "print('   [ ] Winter Paralympic coverage (Section 8)')\n",
        "print('   [ ] Preferred source for overlapping time periods (Section 10)')\n",
        "print('   [ ] Any unexpected gaps or data quality issues')\n",
        "print('\\n   Bring these findings back to plan the unified schema and merge strategy.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Phase 1: Olympic Athletes\n",
        "\n",
        "Build the Olympic athlete backbone from keithgalli (bios + results through 2022), then identify any Paris 2024 athletes not already covered.\n",
        "\n",
        "**Sources:**\n",
        "- `olympic-keithgalli/bios.csv` â†’ USA athlete identification via NOC\n",
        "- `olympic-keithgalli/bios_locs.csv` â†’ Structured birth, height, weight\n",
        "- `olympic-keithgalli/results.csv` â†’ Career stats (Games appearances, medals, sports)\n",
        "- `olympic-paris2024/athletes.csv` â†’ Gap check for 2024 debut athletes"
      ],
      "metadata": {
        "id": "eH-ZH0QFls9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 1, Step 1: Load keithgalli backbone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# bios.csv: filter by NOC to catch USA athletes born abroad\n",
        "bios = pd.read_csv(os.path.join(LOCAL_DIR, 'olympic-keithgalli', 'bios.csv'), low_memory=False)\n",
        "usa_bios = bios[bios['NOC'].astype(str).str.contains('United States', case=False, na=False)].copy()\n",
        "\n",
        "# bios_locs.csv: structured birth/physical data (all countries)\n",
        "bios_locs = pd.read_csv(os.path.join(LOCAL_DIR, 'olympic-keithgalli', 'bios_locs.csv'), low_memory=False)\n",
        "\n",
        "# Join USA athletes with their structured data\n",
        "olympic_backbone = usa_bios[['athlete_id', 'Sex', 'Used name', 'Roles']].merge(\n",
        "    bios_locs[['athlete_id', 'name', 'born_date', 'born_city', 'born_region',\n",
        "                'born_country', 'height_cm', 'weight_kg']],\n",
        "    on='athlete_id', how='left'\n",
        ")\n",
        "\n",
        "# Prefer 'Used name' from bios, fall back to bios_locs 'name'\n",
        "olympic_backbone['display_name'] = olympic_backbone['Used name'].fillna(olympic_backbone['name'])\n",
        "\n",
        "has_h = olympic_backbone['height_cm'].notna() & (olympic_backbone['height_cm'] > 0)\n",
        "has_w = olympic_backbone['weight_kg'].notna() & (olympic_backbone['weight_kg'] > 0)\n",
        "\n",
        "print(f\"Keithgalli USA athletes: {len(olympic_backbone):,}\")\n",
        "print(f\"  With birth_date:  {olympic_backbone['born_date'].notna().sum():,}\")\n",
        "print(f\"  With height (>0): {has_h.sum():,}\")\n",
        "print(f\"  With weight (>0): {has_w.sum():,}\")\n",
        "print(f\"\\nSex distribution:\\n{olympic_backbone['Sex'].value_counts().to_string()}\")\n",
        "print(f\"\\nRoles (sample):\\n{olympic_backbone['Roles'].value_counts().head(5).to_string()}\")\n",
        "print(f\"\\nSample rows:\")\n",
        "olympic_backbone[['athlete_id', 'display_name', 'Sex', 'born_date',\n",
        "                   'born_city', 'height_cm', 'weight_kg']].head(5)"
      ],
      "metadata": {
        "id": "uWDZ8ntnlvX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cLMPAsscl17S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 1, Step 2: Career stats from keithgalli results â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "results = pd.read_csv(os.path.join(LOCAL_DIR, 'olympic-keithgalli', 'results.csv'), low_memory=False)\n",
        "usa_results = results[results['noc'].str.upper().str.strip() == 'USA'].copy()\n",
        "\n",
        "print(f\"Results: {len(results):,} total â†’ {len(usa_results):,} USA\")\n",
        "print(f\"Unique USA athletes in results: {usa_results['athlete_id'].nunique():,}\")\n",
        "print(f\"Year range: {int(usa_results['year'].min())} â€” {int(usa_results['year'].max())}\")\n",
        "print(f\"\\n'type' unique values: {sorted(usa_results['type'].dropna().unique())}\")\n",
        "print(f\"'medal' unique values: {sorted(usa_results['medal'].dropna().unique())}\")\n",
        "print(f\"'discipline' sample values: {sorted(usa_results['discipline'].dropna().unique()[:10])}\")\n",
        "\n",
        "# Aggregate per athlete\n",
        "def most_common(s):\n",
        "    mode = s.dropna().mode()\n",
        "    return mode.iloc[0] if len(mode) > 0 else None\n",
        "\n",
        "career = usa_results.groupby('athlete_id').agg(\n",
        "    first_games_year=('year', 'min'),\n",
        "    last_games_year=('year', 'max'),\n",
        "    games_count=('year', 'nunique'),\n",
        "    primary_sport=('discipline', most_common),\n",
        "    gold_count=('medal', lambda x: (x == 'Gold').sum()),\n",
        "    silver_count=('medal', lambda x: (x == 'Silver').sum()),\n",
        "    bronze_count=('medal', lambda x: (x == 'Bronze').sum()),\n",
        "    games_types=('type', lambda x: ','.join(sorted(x.dropna().unique())))\n",
        ").reset_index()\n",
        "\n",
        "career['total_medals'] = career['gold_count'] + career['silver_count'] + career['bronze_count']\n",
        "\n",
        "print(f\"\\nCareer stats computed for {len(career):,} athletes\")\n",
        "print(f\"  With medals: {(career['total_medals'] > 0).sum():,}\")\n",
        "print(f\"  Multi-Games: {(career['games_count'] > 1).sum():,}\")\n",
        "print(f\"\\n'games_types' unique values (for season derivation):\")\n",
        "for gt in sorted(career['games_types'].unique()):\n",
        "    count = (career['games_types'] == gt).sum()\n",
        "    print(f\"  {gt:50s} â†’ {count:,} athletes\")"
      ],
      "metadata": {
        "id": "vzoGdMqQlz3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 1, Step 3: Merge backbone with career stats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "olympic_athletes = olympic_backbone.merge(career, on='athlete_id', how='left')\n",
        "\n",
        "# Drop the original 'name' from bios_locs before renaming display_name\n",
        "olympic_athletes = olympic_athletes.drop(columns=['name'], errors='ignore')\n",
        "\n",
        "# Standardize columns\n",
        "olympic_athletes = olympic_athletes.rename(columns={\n",
        "    'Sex': 'gender',\n",
        "    'display_name': 'name',\n",
        "    'born_date': 'birth_date',\n",
        "    'born_city': 'birth_city',\n",
        "    'born_region': 'birth_region',\n",
        "    'born_country': 'birth_country'\n",
        "})\n",
        "olympic_athletes['games_type'] = 'Olympic'\n",
        "\n",
        "# Drop intermediate columns\n",
        "olympic_athletes = olympic_athletes.drop(columns=['Used name', 'Roles'],\n",
        "                                          errors='ignore')\n",
        "\n",
        "print(f\"Olympic athletes table: {len(olympic_athletes):,} rows\")\n",
        "print(f\"  With career results:   {olympic_athletes['games_count'].notna().sum():,}\")\n",
        "print(f\"  Bios only (no results): {olympic_athletes['games_count'].isna().sum():,}\")\n",
        "\n",
        "# Verify no duplicate columns\n",
        "dupes = olympic_athletes.columns[olympic_athletes.columns.duplicated()].tolist()\n",
        "if dupes:\n",
        "    print(f\"\\nâš ï¸ DUPLICATE COLUMNS: {dupes}\")\n",
        "else:\n",
        "    print(f\"\\nâœ… No duplicate columns\")\n",
        "\n",
        "print(f\"\\nColumn completeness:\")\n",
        "for col in ['name', 'gender', 'birth_date', 'birth_city', 'birth_country',\n",
        "            'height_cm', 'weight_kg', 'primary_sport', 'games_count',\n",
        "            'first_games_year', 'last_games_year', 'total_medals', 'games_types']:\n",
        "    if col in olympic_athletes.columns:\n",
        "        non_null = int(olympic_athletes[col].notna().sum())\n",
        "        pct = non_null / len(olympic_athletes) * 100\n",
        "        if col in ['height_cm', 'weight_kg']:\n",
        "            non_zero = int((olympic_athletes[col].notna() & (olympic_athletes[col] > 0)).sum())\n",
        "            print(f\"  {col:25s} {non_null:>6,} non-null, {non_zero:>6,} non-zero ({non_zero/len(olympic_athletes)*100:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"  {col:25s} {non_null:>6,} / {len(olympic_athletes):,}  ({pct:.1f}%)\")"
      ],
      "metadata": {
        "id": "Rih0uKeXm_Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 1, Step 4: Paris 2024 gap analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "paris = pd.read_csv(os.path.join(LOCAL_DIR, 'olympic-paris2024', 'athletes.csv'), low_memory=False)\n",
        "paris_usa = paris[paris['country_code'] == 'USA'].copy()\n",
        "\n",
        "# Normalize names: uppercase, sort parts (handles LAST FIRST vs FIRST LAST)\n",
        "def norm_name(n):\n",
        "    if pd.isna(n): return ''\n",
        "    return ' '.join(sorted(str(n).upper().strip().split()))\n",
        "\n",
        "paris_usa['_norm'] = paris_usa['name'].apply(norm_name)\n",
        "olympic_athletes['_norm'] = olympic_athletes['name'].apply(norm_name)\n",
        "\n",
        "matched = paris_usa['_norm'].isin(olympic_athletes['_norm'])\n",
        "\n",
        "print(f\"Paris 2024 USA athletes: {len(paris_usa):,}\")\n",
        "print(f\"  Already in keithgalli: {matched.sum():,}\")\n",
        "print(f\"  New (not matched):     {(~matched).sum():,}\")\n",
        "\n",
        "if (~matched).sum() > 0:\n",
        "    print(f\"\\nSample unmatched (first 15):\")\n",
        "    print(paris_usa[~matched][['name', 'birth_date', 'disciplines']].head(15).to_string())\n",
        "\n",
        "# Paris 2024 height/weight quality check\n",
        "h_valid = paris_usa['height'].notna() & (paris_usa['height'] > 0)\n",
        "w_valid = paris_usa['weight'].notna() & (paris_usa['weight'] > 0)\n",
        "print(f\"\\nParis 2024 physical attributes:\")\n",
        "print(f\"  Non-zero height: {h_valid.sum():,} / {len(paris_usa):,} ({h_valid.sum()/len(paris_usa)*100:.1f}%)\")\n",
        "print(f\"  Non-zero weight: {w_valid.sum():,} / {len(paris_usa):,} ({w_valid.sum()/len(paris_usa)*100:.1f}%)\")\n",
        "if h_valid.sum() > 0:\n",
        "    print(f\"  Height range: {paris_usa.loc[h_valid, 'height'].min():.0f} â€” {paris_usa.loc[h_valid, 'height'].max():.0f}\")\n",
        "if w_valid.sum() > 0:\n",
        "    print(f\"  Weight range: {paris_usa.loc[w_valid, 'weight'].min():.0f} â€” {paris_usa.loc[w_valid, 'weight'].max():.0f}\")\n",
        "\n",
        "# Clean up temp column\n",
        "olympic_athletes = olympic_athletes.drop(columns=['_norm'])"
      ],
      "metadata": {
        "id": "j2WHN-iZnBaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ PHASE 1 QC REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print('=' * 70)\n",
        "print('PHASE 1 QC: OLYMPIC ATHLETES')\n",
        "print('=' * 70)\n",
        "\n",
        "print(f'\\nðŸ“Š COUNTS')\n",
        "print(f'  Total athletes:          {len(olympic_athletes):,}')\n",
        "has_results = olympic_athletes['games_count'].notna().sum()\n",
        "print(f'  With career results:     {has_results:,}')\n",
        "print(f'  Bios only (no results):  {len(olympic_athletes) - has_results:,}')\n",
        "\n",
        "print(f'\\nðŸ“… TEMPORAL RANGE')\n",
        "print(f'  Earliest Games year: {olympic_athletes[\"first_games_year\"].min()}')\n",
        "print(f'  Latest Games year:   {olympic_athletes[\"last_games_year\"].max()}')\n",
        "\n",
        "print(f'\\nðŸ“ PHYSICAL ATTRIBUTES')\n",
        "h = (olympic_athletes['height_cm'].notna() & (olympic_athletes['height_cm'] > 0)).sum()\n",
        "w = (olympic_athletes['weight_kg'].notna() & (olympic_athletes['weight_kg'] > 0)).sum()\n",
        "print(f'  Height: {h:,} / {len(olympic_athletes):,} ({h/len(olympic_athletes)*100:.1f}%)')\n",
        "print(f'  Weight: {w:,} / {len(olympic_athletes):,} ({w/len(olympic_athletes)*100:.1f}%)')\n",
        "\n",
        "print(f'\\nðŸ… MEDAL TOTALS (across all years)')\n",
        "for medal in ['gold_count', 'silver_count', 'bronze_count', 'total_medals']:\n",
        "    val = olympic_athletes[medal].sum()\n",
        "    print(f'  {medal:20s} {val:,.0f}')\n",
        "print(f'  Athletes with 1+ medal: {(olympic_athletes[\"total_medals\"] > 0).sum():,}')\n",
        "\n",
        "print(f'\\nâš§ GENDER SPLIT')\n",
        "print(olympic_athletes['gender'].value_counts().to_string())\n",
        "\n",
        "print(f'\\nðŸŸï¸ GAMES TYPES (season indicators)')\n",
        "print(olympic_athletes['games_types'].value_counts().head(10).to_string())\n",
        "\n",
        "print(f'\\nðŸ”€ PARIS 2024 GAP')\n",
        "print(f'  Paris USA athletes:      {len(paris_usa):,}')\n",
        "print(f'  Already in keithgalli:   {matched.sum():,}')\n",
        "print(f'  New 2024 athletes:       {(~matched).sum():,}')\n",
        "\n",
        "print(f'\\nðŸ“‹ SAMPLE ATHLETES (5 random with results):')\n",
        "sample_cols = ['name', 'gender', 'birth_date', 'height_cm', 'weight_kg',\n",
        "               'primary_sport', 'games_count', 'total_medals',\n",
        "               'first_games_year', 'last_games_year', 'games_types']\n",
        "sample_cols = [c for c in sample_cols if c in olympic_athletes.columns]\n",
        "print(olympic_athletes.dropna(subset=['games_count'])[sample_cols].sample(5, random_state=42).to_string())\n",
        "\n",
        "print(f'\\n{\"=\" * 70}')\n",
        "print('Paste this output back for review before proceeding to Phase 2.')\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "id": "Sy9tbsmDnJ0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Phase 2: Paralympic Athletes\n",
        "\n",
        "Build the Paralympic athlete backbone from three sources:\n",
        "- `piterfm/2020_Tokyo/athletes.csv` â†’ 199 USA, has dedicated `sport_class` column\n",
        "- `piterfm/2024_Paris/athletes.csv` â†’ 220 USA, rich biographical fields, classification in event strings\n",
        "- `katiepress/medal_athlete.csv` â†’ Historical medalists 1960â€“2018, classification in event strings\n",
        "\n",
        "**Key task:** Extract and normalize Paralympic classification codes from all three sources."
      ],
      "metadata": {
        "id": "huiGxzrKoV6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 2, Step 1: Tokyo 2020 Paralympic athletes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "tokyo_para = pd.read_csv(os.path.join(LOCAL_DIR, 'paralympic-piterfm', '2020_Tokyo', 'athletes.csv'), low_memory=False)\n",
        "tokyo_usa = tokyo_para[tokyo_para['birth_country'].astype(str).str.contains('United States', case=False, na=False) |\n",
        "                        tokyo_para['country_code'].astype(str).str.upper().eq('USA') |\n",
        "                        tokyo_para['country'].astype(str).str.contains('United States', case=False, na=False)].copy()\n",
        "\n",
        "print(f\"Tokyo 2020 Paralympic athletes: {len(tokyo_para):,} total â†’ {len(tokyo_usa):,} USA\")\n",
        "print(f\"\\nColumns: {list(tokyo_usa.columns)}\")\n",
        "print(f\"\\nsport_class coverage:\")\n",
        "print(f\"  Non-null: {tokyo_usa['sport_class'].notna().sum()} / {len(tokyo_usa)}\")\n",
        "print(f\"  Unique codes: {tokyo_usa['sport_class'].nunique()}\")\n",
        "print(f\"  Sample: {sorted(tokyo_usa['sport_class'].dropna().unique()[:20])}\")\n",
        "print(f\"\\ndiscipline values: {sorted(tokyo_usa['discipline'].dropna().unique())}\")\n",
        "print(f\"\\nSample rows:\")\n",
        "tokyo_usa[['name', 'gender', 'birth_date', 'discipline', 'sport_class']].head(10)"
      ],
      "metadata": {
        "id": "n-sq-_MaoXFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 2, Step 2: Paris 2024 Paralympic athletes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "paris_para = pd.read_csv(os.path.join(LOCAL_DIR, 'paralympic-piterfm', '2024_Paris', 'athletes.csv'), low_memory=False)\n",
        "paris_para_usa = paris_para[paris_para['country_code'].astype(str).str.upper().eq('USA')].copy()\n",
        "\n",
        "print(f\"Paris 2024 Paralympic athletes: {len(paris_para):,} total â†’ {len(paris_para_usa):,} USA\")\n",
        "\n",
        "# Check for dedicated classification column\n",
        "class_cols = [c for c in paris_para_usa.columns if 'class' in c.lower() or 'categ' in c.lower()]\n",
        "print(f\"\\nClassification-related columns: {class_cols}\")\n",
        "\n",
        "# Classification lives in 'events' field â€” extract it\n",
        "import re\n",
        "\n",
        "# Known Paralympic classification prefixes\n",
        "PARA_CLASS_PATTERN = re.compile(\n",
        "    r'\\b(T[1-5]\\d|F[1-5]\\d|S[1-9]\\d?|SB\\d{1,2}|SM\\d{1,2}|'\n",
        "    r'B[1-3]|BC[1-4]|C[1-5]|H[1-5]|'\n",
        "    r'KL[1-3]|VL[1-3]|'\n",
        "    r'PR[1-3]|PT[S]?[1-5]|'\n",
        "    r'SH[1-2]|SL[1-4]|SU5|'\n",
        "    r'W[1-2]|WH[1-2]|'\n",
        "    r'K4[1-4]|'\n",
        "    r'J[1-2]|'\n",
        "    r'M[DS]\\d{1,2}|W[DS]\\d{1,2}|XD\\d{1,2}|'\n",
        "    r'LW\\d{1,2}|LL[1-2]|'\n",
        "    r'VS[1-2]|PWL)\\b'\n",
        ")\n",
        "\n",
        "def extract_classifications(text):\n",
        "    \"\"\"Extract all unique classification codes from text.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return None\n",
        "    codes = set(PARA_CLASS_PATTERN.findall(str(text)))\n",
        "    return ','.join(sorted(codes)) if codes else None\n",
        "\n",
        "paris_para_usa['classification_code'] = paris_para_usa['events'].apply(extract_classifications)\n",
        "\n",
        "cc = paris_para_usa['classification_code']\n",
        "print(f\"\\nClassification extraction from 'events':\")\n",
        "print(f\"  Extracted: {cc.notna().sum()} / {len(paris_para_usa)} ({cc.notna().sum()/len(paris_para_usa)*100:.1f}%)\")\n",
        "print(f\"  Unique code sets: {cc.nunique()}\")\n",
        "print(f\"  Sample:\")\n",
        "for _, row in paris_para_usa[cc.notna()][['name', 'events', 'classification_code']].head(5).iterrows():\n",
        "    print(f\"    {row['name']:30s} events={str(row['events'])[:60]:60s} â†’ {row['classification_code']}\")\n",
        "\n",
        "# Bio fields check\n",
        "bio_fields = ['hobbies', 'occupation', 'education', 'family', 'coach', 'reason',\n",
        "              'hero', 'influence', 'philosophy', 'sporting_relatives', 'ritual', 'other_sports']\n",
        "print(f\"\\nBio field coverage (USA athletes):\")\n",
        "for field in bio_fields:\n",
        "    if field in paris_para_usa.columns:\n",
        "        n = paris_para_usa[field].notna().sum()\n",
        "        print(f\"  {field:25s} {n:>4} / {len(paris_para_usa)} ({n/len(paris_para_usa)*100:.1f}%)\")\n",
        "\n",
        "# Physical attributes\n",
        "h = paris_para_usa['height'].notna() & (paris_para_usa['height'] > 0)\n",
        "w = paris_para_usa['weight'].notna() & (paris_para_usa['weight'] > 0)\n",
        "print(f\"\\nPhysical attributes:\")\n",
        "print(f\"  Non-zero height: {h.sum()} / {len(paris_para_usa)} ({h.sum()/len(paris_para_usa)*100:.1f}%)\")\n",
        "print(f\"  Non-zero weight: {w.sum()} / {len(paris_para_usa)} ({w.sum()/len(paris_para_usa)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "kMnnWSwjoZl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 2, Step 3: Historical Paralympic medalists (katiepress) â”€â”€\n",
        "katie = pd.read_csv(os.path.join(LOCAL_DIR, 'paralympic-katiepress', 'medal_athlete.csv'), low_memory=False)\n",
        "\n",
        "# Filter to USA â€” check what field works\n",
        "print(f\"Total rows: {len(katie):,}\")\n",
        "print(f\"\\nCountry-like columns:\")\n",
        "for col in ['npc', 'npc_new', 'npc_name', 'games_country']:\n",
        "    vals = katie[col].dropna().unique()[:10]\n",
        "    print(f\"  {col}: {vals}\")\n",
        "\n",
        "# Filter to USA\n",
        "katie_usa = katie[katie['npc_name'].astype(str).str.contains('United States', case=False, na=False) |\n",
        "                   katie['npc'].astype(str).str.upper().eq('USA') |\n",
        "                   katie['npc_new'].astype(str).str.upper().eq('USA')].copy()\n",
        "\n",
        "print(f\"\\nUSA medal rows: {len(katie_usa):,}\")\n",
        "print(f\"Unique athlete names: {katie_usa['athlete_name'].nunique():,}\")\n",
        "print(f\"Year range: {katie_usa['games_year'].min()} â€” {katie_usa['games_year'].max()}\")\n",
        "print(f\"Seasons: {sorted(katie_usa['games_season'].dropna().unique())}\")\n",
        "print(f\"\\nSports ({katie_usa['sport'].nunique()} unique): {sorted(katie_usa['sport'].dropna().unique()[:15])}\")\n",
        "\n",
        "# Extract classification from event\n",
        "katie_usa['classification_code'] = katie_usa['event'].apply(extract_classifications)\n",
        "cc_k = katie_usa['classification_code']\n",
        "print(f\"\\nClassification extraction from 'event':\")\n",
        "print(f\"  Extracted: {cc_k.notna().sum()} / {len(katie_usa)} ({cc_k.notna().sum()/len(katie_usa)*100:.1f}%)\")\n",
        "print(f\"  Sample:\")\n",
        "for _, row in katie_usa[cc_k.notna()][['athlete_name', 'event', 'classification_code']].head(5).iterrows():\n",
        "    print(f\"    {row['athlete_name']:30s} event={str(row['event'])[:50]:50s} â†’ {row['classification_code']}\")\n",
        "\n",
        "# Check for events with NO classification (team sports, early games, etc.)\n",
        "no_class = katie_usa[cc_k.isna()]\n",
        "print(f\"\\n  No classification extracted: {len(no_class)} rows\")\n",
        "print(f\"  Sports without classification:\")\n",
        "print(no_class['sport'].value_counts().head(10).to_string())\n",
        "print(f\"\\n  Sample unextracted events:\")\n",
        "print(no_class[['event', 'sport']].drop_duplicates().head(10).to_string())"
      ],
      "metadata": {
        "id": "w5uaKHkcodqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 2, Step 4: Deduplicate katiepress to unique athletes â”€â”€â”€â”€\n",
        "# Each row is a medal record â€” aggregate to unique athletes\n",
        "\n",
        "def concat_unique(s):\n",
        "    vals = s.dropna().unique()\n",
        "    return ','.join(sorted(str(v) for v in vals)) if len(vals) > 0 else None\n",
        "\n",
        "katie_athletes = katie_usa.groupby('athlete_name').agg(\n",
        "    first_games_year=('games_year', 'min'),\n",
        "    last_games_year=('games_year', 'max'),\n",
        "    games_count=('games_year', 'nunique'),\n",
        "    games_seasons=('games_season', lambda x: ','.join(sorted(x.dropna().unique()))),\n",
        "    sports=('sport', lambda x: ','.join(sorted(x.dropna().unique()))),\n",
        "    primary_sport=('sport', most_common),\n",
        "    classification_codes=('classification_code', lambda x: concat_unique(x)),\n",
        "    gold_count=('medal', lambda x: (x.astype(str).str.lower() == 'gold').sum()),\n",
        "    silver_count=('medal', lambda x: (x.astype(str).str.lower() == 'silver').sum()),\n",
        "    bronze_count=('medal', lambda x: (x.astype(str).str.lower() == 'bronze').sum()),\n",
        ").reset_index()\n",
        "\n",
        "katie_athletes['total_medals'] = katie_athletes['gold_count'] + katie_athletes['silver_count'] + katie_athletes['bronze_count']\n",
        "\n",
        "# Pick most specific classification code (prefer single over compound)\n",
        "def pick_primary_classification(codes_str):\n",
        "    if pd.isna(codes_str):\n",
        "        return None\n",
        "    codes = [c.strip() for c in codes_str.split(',') if c.strip()]\n",
        "    codes = [c for c in codes if c.lower() != 'none']\n",
        "    return codes[0] if codes else None\n",
        "\n",
        "katie_athletes['classification_code'] = katie_athletes['classification_codes'].apply(pick_primary_classification)\n",
        "\n",
        "print(f\"Katiepress deduplicated: {len(katie_usa):,} medal rows â†’ {len(katie_athletes):,} unique athletes\")\n",
        "print(f\"  With classification: {katie_athletes['classification_code'].notna().sum():,}\")\n",
        "print(f\"  Year range: {katie_athletes['first_games_year'].min()} â€” {katie_athletes['last_games_year'].max()}\")\n",
        "print(f\"  Seasons: {katie_athletes['games_seasons'].value_counts().to_string()}\")\n",
        "print(f\"\\n  Medal distribution:\")\n",
        "print(f\"    Gold:   {katie_athletes['gold_count'].sum():,}\")\n",
        "print(f\"    Silver: {katie_athletes['silver_count'].sum():,}\")\n",
        "print(f\"    Bronze: {katie_athletes['bronze_count'].sum():,}\")\n",
        "print(f\"\\n  Multi-Games athletes: {(katie_athletes['games_count'] > 1).sum():,}\")\n",
        "print(f\"\\nSample:\")\n",
        "print(katie_athletes[['athlete_name', 'primary_sport', 'classification_code',\n",
        "                       'games_count', 'total_medals', 'first_games_year',\n",
        "                       'last_games_year', 'games_seasons']].head(10).to_string())"
      ],
      "metadata": {
        "id": "Lyh1Po45oj60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 2, Step 5: Overlap check across Paralympic sources â”€â”€â”€â”€â”€â”€\n",
        "# How many athletes appear in multiple sources?\n",
        "\n",
        "def norm_para_name(n):\n",
        "    if pd.isna(n): return ''\n",
        "    s = str(n).upper().strip()\n",
        "    s = re.sub(r'[^A-Z\\s]', '', s)\n",
        "    return ' '.join(sorted(s.split()))\n",
        "\n",
        "tokyo_names = set(tokyo_usa['name'].apply(norm_para_name))\n",
        "paris_names = set(paris_para_usa['name'].apply(norm_para_name))\n",
        "katie_names = set(katie_athletes['athlete_name'].apply(norm_para_name))\n",
        "\n",
        "print(f\"Unique normalized names per source:\")\n",
        "print(f\"  Tokyo 2020:       {len(tokyo_names):,}\")\n",
        "print(f\"  Paris 2024:       {len(paris_names):,}\")\n",
        "print(f\"  Katiepress hist:  {len(katie_names):,}\")\n",
        "\n",
        "print(f\"\\nOverlap:\")\n",
        "print(f\"  Tokyo âˆ© Paris:    {len(tokyo_names & paris_names):,}\")\n",
        "print(f\"  Tokyo âˆ© Katie:    {len(tokyo_names & katie_names):,}\")\n",
        "print(f\"  Paris âˆ© Katie:    {len(paris_names & katie_names):,}\")\n",
        "print(f\"  All three:        {len(tokyo_names & paris_names & katie_names):,}\")\n",
        "\n",
        "all_para = tokyo_names | paris_names | katie_names\n",
        "print(f\"\\n  Union (unique):   {len(all_para):,}\")\n",
        "\n",
        "# Note: katiepress names are abbreviated (e.g., \"WINTERS A\") so overlap will be low\n",
        "# Show format differences\n",
        "print(f\"\\nName format samples:\")\n",
        "print(f\"  Tokyo:  {list(tokyo_usa['name'].head(3))}\")\n",
        "print(f\"  Paris:  {list(paris_para_usa['name'].head(3))}\")\n",
        "print(f\"  Katie:  {list(katie_athletes['athlete_name'].head(3))}\")"
      ],
      "metadata": {
        "id": "pZl6R08mompc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ PHASE 2 QC REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print('=' * 70)\n",
        "print('PHASE 2 QC: PARALYMPIC ATHLETES')\n",
        "print('=' * 70)\n",
        "\n",
        "print(f'\\nðŸ“Š SOURCE COUNTS')\n",
        "print(f'  Tokyo 2020 USA athletes:   {len(tokyo_usa):,}')\n",
        "print(f'  Paris 2024 USA athletes:   {len(paris_para_usa):,}')\n",
        "print(f'  Katiepress USA athletes:   {len(katie_athletes):,} (deduplicated from {len(katie_usa):,} medal rows)')\n",
        "print(f'  Estimated union:           {len(all_para):,} (name-based, expect noise)')\n",
        "\n",
        "print(f'\\nðŸ“… TEMPORAL RANGE')\n",
        "print(f'  Katiepress: {katie_athletes[\"first_games_year\"].min()} â€” {katie_athletes[\"last_games_year\"].max()}')\n",
        "print(f'  Tokyo:      2020 (2021 actual)')\n",
        "print(f'  Paris:      2024')\n",
        "\n",
        "print(f'\\nðŸ·ï¸ CLASSIFICATION CODES')\n",
        "print(f'  Tokyo (sport_class):       {tokyo_usa[\"sport_class\"].notna().sum()} / {len(tokyo_usa)} ({tokyo_usa[\"sport_class\"].notna().sum()/len(tokyo_usa)*100:.1f}%)')\n",
        "paris_cc = paris_para_usa['classification_code'].notna().sum()\n",
        "print(f'  Paris (extracted):         {paris_cc} / {len(paris_para_usa)} ({paris_cc/len(paris_para_usa)*100:.1f}%)')\n",
        "katie_cc = katie_athletes['classification_code'].notna().sum()\n",
        "print(f'  Katiepress (extracted):    {katie_cc} / {len(katie_athletes)} ({katie_cc/len(katie_athletes)*100:.1f}%)')\n",
        "\n",
        "print(f'\\nðŸ“ PHYSICAL ATTRIBUTES (Paris 2024 only)')\n",
        "h = (paris_para_usa['height'].notna() & (paris_para_usa['height'] > 0)).sum()\n",
        "w = (paris_para_usa['weight'].notna() & (paris_para_usa['weight'] > 0)).sum()\n",
        "print(f'  Height: {h} / {len(paris_para_usa)} ({h/len(paris_para_usa)*100:.1f}%)')\n",
        "print(f'  Weight: {w} / {len(paris_para_usa)} ({w/len(paris_para_usa)*100:.1f}%)')\n",
        "\n",
        "print(f'\\nðŸ“ BIO FIELDS (Paris 2024 only)')\n",
        "bio_fields = ['reason', 'hero', 'philosophy', 'other_sports', 'coach', 'hobbies']\n",
        "for field in bio_fields:\n",
        "    if field in paris_para_usa.columns:\n",
        "        n = paris_para_usa[field].notna().sum()\n",
        "        print(f'  {field:25s} {n:>4} / {len(paris_para_usa)} ({n/len(paris_para_usa)*100:.1f}%)')\n",
        "\n",
        "print(f'\\nðŸ… MEDAL TOTALS (katiepress historical)')\n",
        "print(f'  Gold:   {katie_athletes[\"gold_count\"].sum():,}')\n",
        "print(f'  Silver: {katie_athletes[\"silver_count\"].sum():,}')\n",
        "print(f'  Bronze: {katie_athletes[\"bronze_count\"].sum():,}')\n",
        "print(f'  Total:  {katie_athletes[\"total_medals\"].sum():,}')\n",
        "\n",
        "print(f'\\nðŸ”€ OVERLAP')\n",
        "print(f'  Tokyo âˆ© Paris:    {len(tokyo_names & paris_names):,}')\n",
        "print(f'  Tokyo âˆ© Katie:    {len(tokyo_names & katie_names):,}')\n",
        "print(f'  Paris âˆ© Katie:    {len(paris_names & katie_names):,}')\n",
        "\n",
        "print(f'\\nðŸ“‹ SAMPLE ATHLETES (5 from each source):')\n",
        "print(f'\\n  -- Tokyo 2020 --')\n",
        "print(tokyo_usa[['name', 'gender', 'discipline', 'sport_class']].head(5).to_string())\n",
        "print(f'\\n  -- Paris 2024 --')\n",
        "print(paris_para_usa[['name', 'gender', 'disciplines', 'classification_code']].head(5).to_string())\n",
        "print(f'\\n  -- Katiepress (deduplicated) --')\n",
        "print(katie_athletes[['athlete_name', 'primary_sport', 'classification_code',\n",
        "                        'games_count', 'total_medals']].head(5).to_string())\n",
        "\n",
        "print(f'\\n{\"=\" * 70}')\n",
        "print('Paste this output back for review before proceeding to Phase 3.')\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "id": "MF8cxy8qoo4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Phase 3: Unify Athletes Table\n",
        "\n",
        "Merge Olympic and Paralympic athletes into a single `team_usa_athletes` table.\n",
        "\n",
        "**Merge strategy:**\n",
        "- **Olympic backbone** (10,332) + **Paris 2024 Olympic new arrivals** (~619, pending better name matching)\n",
        "- **Tokyo 2020 Paralympic** (252) + **Paris 2024 Paralympic** (220) â†’ merge on name, 108 overlap expected\n",
        "- **Katiepress historical Paralympic** (1,169) â†’ standalone cohort, no cross-matching\n",
        "- Stack all into unified schema, generate athlete_id\n",
        "\n",
        "**Key challenges:** Name format normalization, dedup across sources, column harmonization"
      ],
      "metadata": {
        "id": "Mxhz8pEIrZsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 3, Step 1: Merge Tokyo + Paris Paralympic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Tokyo has clean sport_class, Paris has rich bio fields\n",
        "# For overlapping athletes: keep both, merge attributes\n",
        "\n",
        "def norm_para(n):\n",
        "    if pd.isna(n): return ''\n",
        "    s = re.sub(r'[^A-Za-z\\s]', '', str(n)).upper().strip()\n",
        "    return ' '.join(sorted(s.split()))\n",
        "\n",
        "tokyo_usa['_norm'] = tokyo_usa['name'].apply(norm_para)\n",
        "paris_para_usa['_norm'] = paris_para_usa['name'].apply(norm_para)\n",
        "\n",
        "# Merge on normalized name\n",
        "para_merged = paris_para_usa.merge(\n",
        "    tokyo_usa[['_norm', 'sport_class', 'discipline']].rename(\n",
        "        columns={'sport_class': 'tokyo_sport_class', 'discipline': 'tokyo_discipline'}),\n",
        "    on='_norm', how='outer', indicator=True\n",
        ")\n",
        "\n",
        "print(f\"Paralympic merge results:\")\n",
        "print(para_merged['_merge'].value_counts().to_string())\n",
        "\n",
        "# For athletes in both: prefer Paris bio fields + Tokyo sport_class\n",
        "# For Paris-only: use extracted classification\n",
        "# For Tokyo-only: need to pull full Tokyo row\n",
        "\n",
        "# Handle Tokyo-only athletes (not in Paris)\n",
        "tokyo_only_norms = set(para_merged[para_merged['_merge'] == 'right_only']['_norm'])\n",
        "tokyo_only = tokyo_usa[tokyo_usa['_norm'].isin(tokyo_only_norms)].copy()\n",
        "\n",
        "print(f\"\\nTokyo-only athletes: {len(tokyo_only)}\")\n",
        "print(f\"Paris-only athletes: {(para_merged['_merge'] == 'left_only').sum()}\")\n",
        "print(f\"In both:             {(para_merged['_merge'] == 'both').sum()}\")\n",
        "\n",
        "# Build unified Paralympic recent (Tokyo + Paris)\n",
        "# Start with Paris athletes (richer data)\n",
        "paris_cols = {\n",
        "    'name': 'name', 'gender': 'gender', 'birth_date': 'birth_date',\n",
        "    'birth_place': 'birth_place', 'birth_country': 'birth_country',\n",
        "    'height': 'height_cm', 'weight': 'weight_kg',\n",
        "    'disciplines': 'primary_sport', 'classification_code': 'classification_code',\n",
        "    'events': 'events_raw',\n",
        "    'hobbies': 'hobbies', 'occupation': 'occupation', 'education': 'education',\n",
        "    'reason': 'reason', 'hero': 'hero', 'philosophy': 'philosophy',\n",
        "    'other_sports': 'other_sports', 'coach': 'coach',\n",
        "    'tokyo_sport_class': 'tokyo_sport_class', '_norm': '_norm'\n",
        "}\n",
        "\n",
        "# Filter to Paris athletes (both + left_only)\n",
        "paris_rows = para_merged[para_merged['_merge'].isin(['both', 'left_only'])].copy()\n",
        "available_cols = {k: v for k, v in paris_cols.items() if k in paris_rows.columns}\n",
        "para_recent = paris_rows[list(available_cols.keys())].rename(columns=available_cols)\n",
        "\n",
        "# For athletes in both sources, prefer Tokyo sport_class over extracted classification\n",
        "has_tokyo_class = para_recent['tokyo_sport_class'].notna()\n",
        "para_recent.loc[has_tokyo_class, 'classification_code'] = para_recent.loc[has_tokyo_class, 'tokyo_sport_class']\n",
        "print(f\"\\nUpgraded {has_tokyo_class.sum()} athletes with Tokyo sport_class\")\n",
        "\n",
        "# Add Tokyo-only athletes\n",
        "tokyo_only_std = tokyo_only.rename(columns={\n",
        "    'name': 'name', 'gender': 'gender', 'birth_date': 'birth_date',\n",
        "    'birth_place': 'birth_place', 'birth_country': 'birth_country',\n",
        "    'discipline': 'primary_sport', 'sport_class': 'classification_code'\n",
        "}).copy()\n",
        "tokyo_only_std['height_cm'] = None\n",
        "tokyo_only_std['weight_kg'] = None\n",
        "\n",
        "# Stack\n",
        "common_cols = ['name', 'gender', 'birth_date', 'birth_place', 'birth_country',\n",
        "               'height_cm', 'weight_kg', 'primary_sport', 'classification_code', '_norm']\n",
        "bio_cols = ['hobbies', 'occupation', 'education', 'reason', 'hero',\n",
        "            'philosophy', 'other_sports', 'coach', 'events_raw']\n",
        "for col in common_cols + bio_cols:\n",
        "    if col not in para_recent.columns:\n",
        "        para_recent[col] = None\n",
        "    if col not in tokyo_only_std.columns:\n",
        "        tokyo_only_std[col] = None\n",
        "\n",
        "para_recent_all = pd.concat([\n",
        "    para_recent[common_cols + bio_cols],\n",
        "    tokyo_only_std[common_cols + bio_cols]\n",
        "], ignore_index=True)\n",
        "\n",
        "para_recent_all['games_type'] = 'Paralympic'\n",
        "para_recent_all['source'] = 'piterfm'\n",
        "\n",
        "print(f\"\\nCombined recent Paralympic athletes: {len(para_recent_all)}\")\n",
        "print(f\"  With classification: {para_recent_all['classification_code'].notna().sum()}\")\n",
        "print(f\"  With bio fields:     {para_recent_all['reason'].notna().sum()}\")"
      ],
      "metadata": {
        "id": "w1WiEx-nragp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 3, Step 2: Prepare katiepress historical athletes â”€â”€â”€â”€â”€â”€â”€\n",
        "# Clean name artifacts, standardize columns\n",
        "\n",
        "def clean_katie_name(n):\n",
        "    if pd.isna(n): return None\n",
        "    s = str(n).strip()\n",
        "    s = re.sub(r'^[^A-Za-z]+', '', s)  # strip leading punctuation\n",
        "    s = s.strip()\n",
        "    return s if len(s) >= 3 else None\n",
        "\n",
        "katie_athletes['name_clean'] = katie_athletes['athlete_name'].apply(clean_katie_name)\n",
        "before = len(katie_athletes)\n",
        "katie_clean = katie_athletes[katie_athletes['name_clean'].notna()].copy()\n",
        "print(f\"Katiepress name cleanup: {before} â†’ {len(katie_clean)} (dropped {before - len(katie_clean)} bad names)\")\n",
        "\n",
        "# Show what was dropped\n",
        "dropped = katie_athletes[katie_athletes['name_clean'].isna()]\n",
        "if len(dropped) > 0:\n",
        "    print(f\"  Dropped names: {dropped['athlete_name'].tolist()[:10]}\")\n",
        "\n",
        "# Standardize to unified schema\n",
        "katie_std = katie_clean.rename(columns={\n",
        "    'name_clean': 'name',\n",
        "    'classification_code': 'classification_code',\n",
        "    'primary_sport': 'primary_sport',\n",
        "    'first_games_year': 'first_games_year',\n",
        "    'last_games_year': 'last_games_year',\n",
        "    'games_count': 'games_count',\n",
        "    'games_seasons': 'games_seasons',\n",
        "    'gold_count': 'gold_count',\n",
        "    'silver_count': 'silver_count',\n",
        "    'bronze_count': 'bronze_count',\n",
        "    'total_medals': 'total_medals'\n",
        "}).copy()\n",
        "\n",
        "katie_std['gender'] = None  # katiepress doesn't have gender\n",
        "katie_std['birth_date'] = None\n",
        "katie_std['birth_place'] = None\n",
        "katie_std['birth_country'] = None\n",
        "katie_std['height_cm'] = None\n",
        "katie_std['weight_kg'] = None\n",
        "katie_std['games_type'] = 'Paralympic'\n",
        "katie_std['source'] = 'katiepress'\n",
        "\n",
        "print(f\"\\nKatiepress standardized: {len(katie_std)} athletes\")\n",
        "print(f\"  With classification: {katie_std['classification_code'].notna().sum()}\")\n",
        "print(f\"  Year range: {katie_std['first_games_year'].min()} â€” {katie_std['last_games_year'].max()}\")\n",
        "print(f\"  Season split:\\n{katie_std['games_seasons'].value_counts().to_string()}\")"
      ],
      "metadata": {
        "id": "3HXrVg1orfwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 3, Step 3: Paris 2024 Olympic â€” improved name matching â”€â”€\n",
        "# Paris uses \"LAST First\" format, keithgalli uses \"Firstâ€¢Last\"\n",
        "# Strategy: normalize both to sorted uppercase tokens + cross-check birth_date\n",
        "\n",
        "def norm_olympic(n):\n",
        "    if pd.isna(n): return ''\n",
        "    s = str(n).replace('â€¢', ' ').replace('-', ' ')\n",
        "    s = re.sub(r'[^A-Za-z\\s]', '', s).upper().strip()\n",
        "    return ' '.join(sorted(s.split()))\n",
        "\n",
        "# Re-normalize with bullet handling\n",
        "olympic_athletes['_norm'] = olympic_athletes['name'].apply(norm_olympic)\n",
        "paris_usa['_norm'] = paris_usa['name'].apply(norm_olympic)\n",
        "\n",
        "# Pass 1: exact normalized name match\n",
        "matched_name = paris_usa['_norm'].isin(olympic_athletes['_norm'])\n",
        "print(f\"Paris 2024 Olympic name matching:\")\n",
        "print(f\"  Pass 1 (normalized name): {matched_name.sum()} / {len(paris_usa)}\")\n",
        "\n",
        "# Pass 2: for unmatched, try name + birth_date\n",
        "unmatched_paris = paris_usa[~matched_name].copy()\n",
        "if 'birth_date' in unmatched_paris.columns and 'birth_date' in olympic_athletes.columns:\n",
        "    # Normalize birth dates\n",
        "    olympic_athletes['_bd'] = pd.to_datetime(olympic_athletes['birth_date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
        "    unmatched_paris['_bd'] = pd.to_datetime(unmatched_paris['birth_date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    # Match on birth_date alone for remaining\n",
        "    bd_matched = unmatched_paris['_bd'].isin(olympic_athletes['_bd'].dropna())\n",
        "    print(f\"  Unmatched with valid birth_date: {unmatched_paris['_bd'].notna().sum()}\")\n",
        "    print(f\"  Pass 2 (birth_date exists in backbone): {bd_matched.sum()} (note: not unique, just a signal)\")\n",
        "\n",
        "# Pass 3: fuzzy match on last name + birth year\n",
        "unmatched_paris['_last'] = unmatched_paris['name'].apply(\n",
        "    lambda n: str(n).split()[0].upper() if pd.notna(n) else '')\n",
        "unmatched_paris['_by'] = pd.to_datetime(unmatched_paris['birth_date'], errors='coerce').dt.year\n",
        "\n",
        "olympic_athletes['_last'] = olympic_athletes['name'].apply(\n",
        "    lambda n: str(n).replace('â€¢', ' ').split()[-1].upper() if pd.notna(n) else '')\n",
        "olympic_athletes['_by'] = pd.to_datetime(olympic_athletes['birth_date'], errors='coerce').dt.year\n",
        "\n",
        "last_year_match = unmatched_paris.merge(\n",
        "    olympic_athletes[['_last', '_by']].drop_duplicates(),\n",
        "    on=['_last', '_by'], how='inner'\n",
        ")\n",
        "print(f\"  Pass 3 (last name + birth year): {len(last_year_match)} additional matches\")\n",
        "\n",
        "total_matched = matched_name.sum() + len(last_year_match)\n",
        "truly_new = len(paris_usa) - total_matched\n",
        "print(f\"\\n  Total matched:   {total_matched}\")\n",
        "print(f\"  Truly new 2024:  {truly_new}\")\n",
        "\n",
        "# Identify the truly new athletes\n",
        "matched_norms = set(olympic_athletes['_norm'])\n",
        "pass3_norms = set(last_year_match['_norm'])\n",
        "paris_new = paris_usa[(~paris_usa['_norm'].isin(matched_norms)) &\n",
        "                       (~paris_usa['_norm'].isin(pass3_norms))].copy()\n",
        "\n",
        "print(f\"\\nNew Paris 2024 athletes â€” discipline distribution:\")\n",
        "if 'disciplines' in paris_new.columns:\n",
        "    print(paris_new['disciplines'].value_counts().head(15).to_string())\n",
        "\n",
        "# Clean up temp columns\n",
        "for df in [olympic_athletes, paris_usa, unmatched_paris]:\n",
        "    for col in ['_norm', '_bd', '_last', '_by']:\n",
        "        if col in df.columns:\n",
        "            df.drop(columns=[col], inplace=True, errors='ignore')"
      ],
      "metadata": {
        "id": "1jPBTjg9ri7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 3, Step 4: Build rows for genuinely new Paris 2024 athletes â”€â”€\n",
        "\n",
        "paris_new_std = paris_new.rename(columns={\n",
        "    'name': 'name', 'gender': 'gender', 'birth_date': 'birth_date',\n",
        "    'birth_place': 'birth_place', 'birth_country': 'birth_country',\n",
        "    'disciplines': 'primary_sport'\n",
        "}).copy()\n",
        "\n",
        "# Handle height/weight (only keep non-zero)\n",
        "if 'height' in paris_new.columns:\n",
        "    paris_new_std['height_cm'] = paris_new['height'].where(paris_new['height'] > 0, None)\n",
        "else:\n",
        "    paris_new_std['height_cm'] = None\n",
        "\n",
        "if 'weight' in paris_new.columns:\n",
        "    paris_new_std['weight_kg'] = paris_new['weight'].where(paris_new['weight'] > 0, None)\n",
        "else:\n",
        "    paris_new_std['weight_kg'] = None\n",
        "\n",
        "paris_new_std['games_type'] = 'Olympic'\n",
        "paris_new_std['classification_code'] = None\n",
        "paris_new_std['first_games_year'] = 2024\n",
        "paris_new_std['last_games_year'] = 2024\n",
        "paris_new_std['games_count'] = 1\n",
        "paris_new_std['games_types'] = 'Summer'\n",
        "paris_new_std['gold_count'] = 0\n",
        "paris_new_std['silver_count'] = 0\n",
        "paris_new_std['bronze_count'] = 0\n",
        "paris_new_std['total_medals'] = 0\n",
        "paris_new_std['source'] = 'paris2024'\n",
        "\n",
        "print(f\"New Paris 2024 Olympic athletes to add: {len(paris_new_std)}\")\n",
        "h = (paris_new_std['height_cm'].notna()).sum()\n",
        "w = (paris_new_std['weight_kg'].notna()).sum()\n",
        "print(f\"  With height: {h}  With weight: {w}\")"
      ],
      "metadata": {
        "id": "TuG_5tAgrlle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 3, Step 5: Stack everything into team_usa_athletes â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Define unified columns\n",
        "unified_cols = [\n",
        "    'name', 'gender', 'birth_date', 'birth_place', 'birth_country',\n",
        "    'height_cm', 'weight_kg', 'games_type', 'classification_code',\n",
        "    'primary_sport', 'first_games_year', 'last_games_year', 'games_count',\n",
        "    'gold_count', 'silver_count', 'bronze_count', 'total_medals', 'source'\n",
        "]\n",
        "bio_cols = ['hobbies', 'occupation', 'education', 'reason', 'hero',\n",
        "            'philosophy', 'other_sports', 'coach']\n",
        "\n",
        "# Prep Olympic backbone\n",
        "olympic_for_stack = olympic_athletes.copy()\n",
        "olympic_for_stack['classification_code'] = None\n",
        "olympic_for_stack['source'] = 'keithgalli'\n",
        "# Rename games_types â†’ games_seasons for consistency\n",
        "if 'games_types' in olympic_for_stack.columns:\n",
        "    olympic_for_stack['games_seasons'] = olympic_for_stack['games_types']\n",
        "\n",
        "# Ensure all dataframes have all columns\n",
        "all_dfs = {\n",
        "    'Olympic backbone': olympic_for_stack,\n",
        "    'Paris 2024 Olympic new': paris_new_std,\n",
        "    'Paralympic recent (Tokyo+Paris)': para_recent_all,\n",
        "    'Paralympic historical (katiepress)': katie_std\n",
        "}\n",
        "\n",
        "for label, df in all_dfs.items():\n",
        "    for col in unified_cols + bio_cols:\n",
        "        if col not in df.columns:\n",
        "            df[col] = None\n",
        "    print(f\"{label:45s} â†’ {len(df):>6,} rows\")\n",
        "\n",
        "# Stack\n",
        "team_usa_athletes = pd.concat(\n",
        "    [df[unified_cols + bio_cols] for df in all_dfs.values()],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "print(f\"\\n{'=' * 50}\")\n",
        "print(f\"STACKED TOTAL: {len(team_usa_athletes):,} athletes\")\n",
        "print(f\"  Olympic:    {(team_usa_athletes['games_type'] == 'Olympic').sum():,}\")\n",
        "print(f\"  Paralympic: {(team_usa_athletes['games_type'] == 'Paralympic').sum():,}\")"
      ],
      "metadata": {
        "id": "xR2pbPv9roCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 3, Step 6: Generate athlete_id and dedup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import hashlib\n",
        "\n",
        "def make_athlete_id(row):\n",
        "    \"\"\"Generate stable ID from name + birth_date + games_type.\"\"\"\n",
        "    parts = [\n",
        "        str(row.get('name', '')).upper().strip(),\n",
        "        str(row.get('birth_date', '')).strip(),\n",
        "        str(row.get('games_type', '')).strip()\n",
        "    ]\n",
        "    key = '|'.join(parts)\n",
        "    return hashlib.md5(key.encode()).hexdigest()[:12]\n",
        "\n",
        "team_usa_athletes['athlete_id'] = team_usa_athletes.apply(make_athlete_id, axis=1)\n",
        "\n",
        "# Check for ID collisions\n",
        "dupes = team_usa_athletes[team_usa_athletes['athlete_id'].duplicated(keep=False)]\n",
        "print(f\"Athlete ID collisions: {dupes['athlete_id'].nunique()} IDs affecting {len(dupes)} rows\")\n",
        "\n",
        "if len(dupes) > 0:\n",
        "    print(f\"\\nSample collisions:\")\n",
        "    for aid in dupes['athlete_id'].unique()[:5]:\n",
        "        subset = dupes[dupes['athlete_id'] == aid]\n",
        "        print(f\"\\n  ID: {aid}\")\n",
        "        print(subset[['name', 'birth_date', 'games_type', 'source', 'primary_sport']].to_string())\n",
        "\n",
        "# For true duplicates (same person from overlapping sources): keep the row with most data\n",
        "def completeness_score(row):\n",
        "    return row.notna().sum()\n",
        "\n",
        "team_usa_athletes['_completeness'] = team_usa_athletes.apply(completeness_score, axis=1)\n",
        "team_usa_athletes = team_usa_athletes.sort_values('_completeness', ascending=False)\n",
        "before_dedup = len(team_usa_athletes)\n",
        "team_usa_athletes = team_usa_athletes.drop_duplicates(subset='athlete_id', keep='first')\n",
        "team_usa_athletes = team_usa_athletes.drop(columns=['_completeness'])\n",
        "\n",
        "print(f\"\\nDedup: {before_dedup:,} â†’ {len(team_usa_athletes):,} (removed {before_dedup - len(team_usa_athletes):,})\")"
      ],
      "metadata": {
        "id": "my4sv-kGrqbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 3, Step 7: Checkpoint to GCS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "checkpoint_path = '/tmp/team_usa_athletes.csv'\n",
        "team_usa_athletes.to_csv(checkpoint_path, index=False)\n",
        "\n",
        "!gsutil cp {checkpoint_path} gs://class-demo/team-usa/processed/team_usa_athletes.csv\n",
        "\n",
        "print(f\"âœ… Checkpoint saved: gs://class-demo/team-usa/processed/team_usa_athletes.csv\")\n",
        "print(f\"   Rows: {len(team_usa_athletes):,}\")"
      ],
      "metadata": {
        "id": "YIfHkmVZru_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ PHASE 3 QC REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print('=' * 70)\n",
        "print('PHASE 3 QC: UNIFIED TEAM USA ATHLETES')\n",
        "print('=' * 70)\n",
        "\n",
        "print(f'\\nðŸ“Š COUNTS')\n",
        "print(f'  Total athletes:     {len(team_usa_athletes):,}')\n",
        "print(f'  Olympic:            {(team_usa_athletes[\"games_type\"] == \"Olympic\").sum():,}')\n",
        "print(f'  Paralympic:         {(team_usa_athletes[\"games_type\"] == \"Paralympic\").sum():,}')\n",
        "\n",
        "print(f'\\nðŸ“¦ SOURCE BREAKDOWN')\n",
        "print(team_usa_athletes['source'].value_counts().to_string())\n",
        "\n",
        "print(f'\\nðŸ“… TEMPORAL RANGE')\n",
        "oly = team_usa_athletes[team_usa_athletes['games_type'] == 'Olympic']\n",
        "para = team_usa_athletes[team_usa_athletes['games_type'] == 'Paralympic']\n",
        "print(f'  Olympic:    {oly[\"first_games_year\"].min()} â€” {oly[\"last_games_year\"].max()}')\n",
        "print(f'  Paralympic: {para[\"first_games_year\"].min()} â€” {para[\"last_games_year\"].max()}')\n",
        "\n",
        "print(f'\\nðŸ·ï¸ CLASSIFICATION CODES')\n",
        "has_class = team_usa_athletes['classification_code'].notna()\n",
        "print(f'  Total with classification: {has_class.sum():,}')\n",
        "print(f'  Paralympic with classification: {(has_class & (team_usa_athletes[\"games_type\"] == \"Paralympic\")).sum():,} / {len(para):,}')\n",
        "print(f'  Olympic with classification (should be 0): {(has_class & (team_usa_athletes[\"games_type\"] == \"Olympic\")).sum():,}')\n",
        "\n",
        "print(f'\\nðŸ“ PHYSICAL ATTRIBUTES')\n",
        "h = (team_usa_athletes['height_cm'].notna() & (team_usa_athletes['height_cm'] > 0)).sum()\n",
        "w = (team_usa_athletes['weight_kg'].notna() & (team_usa_athletes['weight_kg'] > 0)).sum()\n",
        "print(f'  Height: {h:,} / {len(team_usa_athletes):,} ({h/len(team_usa_athletes)*100:.1f}%)')\n",
        "print(f'  Weight: {w:,} / {len(team_usa_athletes):,} ({w/len(team_usa_athletes)*100:.1f}%)')\n",
        "print(f'  Olympic height:    {(oly[\"height_cm\"].notna() & (oly[\"height_cm\"] > 0)).sum():,} / {len(oly):,}')\n",
        "print(f'  Paralympic height: {(para[\"height_cm\"].notna() & (para[\"height_cm\"] > 0)).sum():,} / {len(para):,}')\n",
        "\n",
        "print(f'\\nðŸ… MEDALS')\n",
        "print(f'  Total medals:       {team_usa_athletes[\"total_medals\"].sum():,.0f}')\n",
        "print(f'  Athletes with 1+:   {(team_usa_athletes[\"total_medals\"] > 0).sum():,}')\n",
        "print(f'  Olympic medalists:  {(oly[\"total_medals\"] > 0).sum():,}')\n",
        "print(f'  Paralympic medalists: {(para[\"total_medals\"] > 0).sum():,}')\n",
        "\n",
        "print(f'\\nðŸ“ BIO FIELDS (from Paris 2024 Paralympic)')\n",
        "for field in ['reason', 'hero', 'philosophy', 'other_sports', 'coach']:\n",
        "    if field in team_usa_athletes.columns:\n",
        "        n = team_usa_athletes[field].notna().sum()\n",
        "        print(f'  {field:25s} {n:>5} / {len(team_usa_athletes):,}')\n",
        "\n",
        "print(f'\\nâš§ GENDER')\n",
        "print(team_usa_athletes['gender'].value_counts(dropna=False).to_string())\n",
        "\n",
        "print(f'\\nðŸ”‘ ATHLETE ID')\n",
        "print(f'  Unique IDs: {team_usa_athletes[\"athlete_id\"].nunique():,}')\n",
        "print(f'  Any remaining dupes: {team_usa_athletes[\"athlete_id\"].duplicated().sum()}')\n",
        "\n",
        "print(f'\\nðŸ“‹ SAMPLE ATHLETES (3 Olympic, 3 Paralympic):')\n",
        "sample_cols = ['athlete_id', 'name', 'games_type', 'classification_code',\n",
        "               'primary_sport', 'games_count', 'total_medals', 'source']\n",
        "print(oly[sample_cols].dropna(subset=['games_count']).sample(3, random_state=42).to_string())\n",
        "print()\n",
        "print(para[sample_cols].sample(3, random_state=42).to_string())\n",
        "\n",
        "print(f'\\n{\"=\" * 70}')\n",
        "print('Paste this output back for review before proceeding to Phase 4.')\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "id": "zSJJY_L-ry4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Phase 4: Unified Results Table + Athlete Backfill\n",
        "\n",
        "Build `team_usa_results` from all medal/event sources, then backfill career stats into the athletes table.\n",
        "\n",
        "**Sources:**\n",
        "- `keithgalli/results.csv` â†’ Olympic all participations 1896â€“2022 (21,353 USA rows)\n",
        "- `olympic-paris2024/medallists.csv` â†’ Olympic medal records 2024\n",
        "- `piterfm/2020_Tokyo/` + `2024_Paris/` medal files â†’ Paralympic 2020 + 2024\n",
        "- `katiepress/medal_athlete.csv` â†’ Paralympic medals 1960â€“2018 (already loaded)\n",
        "\n",
        "**Backfill strategy:** Recompute career stats from the unified results table as single source of truth. Also infer gender for katiepress athletes from event names."
      ],
      "metadata": {
        "id": "PYK1iJlzwhVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 4, Step 1: Standardize keithgalli Olympic results â”€â”€â”€â”€â”€â”€\n",
        "print(f\"keithgalli results columns: {list(usa_results.columns)}\")\n",
        "print(f\"USA rows: {len(usa_results):,}\\n\")\n",
        "\n",
        "# Create lookup: keithgalli numeric athlete_id â†’ unified hash ID\n",
        "# olympic_athletes still has the original keithgalli athlete_id\n",
        "kg_id_lookup = olympic_athletes[['athlete_id', 'name', 'birth_date']].copy()\n",
        "\n",
        "# Compute unified hash (same formula as Cell 21)\n",
        "kg_id_lookup['unified_id'] = kg_id_lookup.apply(\n",
        "    lambda r: hashlib.md5(\n",
        "        f\"{str(r['name']).upper().strip()}|{str(r['birth_date']).strip()}|Olympic\".encode()\n",
        "    ).hexdigest()[:12], axis=1)\n",
        "\n",
        "# Verify\n",
        "in_athletes = kg_id_lookup['unified_id'].isin(team_usa_athletes['athlete_id'])\n",
        "print(f\"ID verification: {in_athletes.sum():,} / {len(kg_id_lookup):,} found in team_usa_athletes\")\n",
        "\n",
        "# Build mapping dicts\n",
        "kg_to_unified = dict(zip(kg_id_lookup['athlete_id'], kg_id_lookup['unified_id']))\n",
        "kg_to_name = dict(zip(kg_id_lookup['athlete_id'], kg_id_lookup['name']))\n",
        "\n",
        "# Determine sport column\n",
        "sport_col = 'sport' if 'sport' in usa_results.columns else 'discipline'\n",
        "\n",
        "# Standardize\n",
        "results_keithgalli = pd.DataFrame({\n",
        "    'athlete_id': usa_results['athlete_id'].map(kg_to_unified),\n",
        "    'athlete_name': usa_results['athlete_id'].map(kg_to_name),\n",
        "    'games_year': usa_results['year'].fillna(0).astype(int),\n",
        "    'games_season': usa_results['type'],\n",
        "    'games_type': 'Olympic',\n",
        "    'sport': usa_results[sport_col],\n",
        "    'discipline': usa_results['discipline'],\n",
        "    'event': usa_results['event'],\n",
        "    'classification_code': None,\n",
        "    'medal': usa_results['medal'],\n",
        "    'source': 'keithgalli'\n",
        "})\n",
        "\n",
        "print(f\"\\nStandardized: {len(results_keithgalli):,} rows\")\n",
        "print(f\"  Mapped to unified ID: {results_keithgalli['athlete_id'].notna().sum():,}\")\n",
        "print(f\"  Unmapped: {results_keithgalli['athlete_id'].isna().sum()}\")\n",
        "print(f\"  Year range: {results_keithgalli['games_year'].min()} â€” {results_keithgalli['games_year'].max()}\")\n",
        "print(f\"  Medal dist:\\n{results_keithgalli['medal'].value_counts(dropna=False).head().to_string()}\")"
      ],
      "metadata": {
        "id": "s9WXt7Ltr3SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 4, Step 2: Paris 2024 Olympic medal results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "p2024_dir = os.path.join(LOCAL_DIR, 'olympic-paris2024')\n",
        "p2024_files = [f for f in os.listdir(p2024_dir) if f.endswith('.csv')]\n",
        "print(f\"Paris 2024 CSV files: {p2024_files}\")\n",
        "\n",
        "# Load medallists (prefer over medals â€” one row per athlete)\n",
        "med_file = 'medallists.csv' if 'medallists.csv' in p2024_files else 'medals.csv'\n",
        "paris_med_raw = pd.read_csv(os.path.join(p2024_dir, med_file), low_memory=False)\n",
        "print(f\"\\n{med_file}: {len(paris_med_raw):,} total rows\")\n",
        "print(f\"Columns: {list(paris_med_raw.columns)}\")\n",
        "\n",
        "# Find USA filter column\n",
        "usa_col = None\n",
        "for c in paris_med_raw.columns:\n",
        "    if paris_med_raw[c].astype(str).str.upper().eq('USA').any() and ('country' in c.lower() or 'code' in c.lower()):\n",
        "        usa_col = c\n",
        "        break\n",
        "if usa_col is None:\n",
        "    for c in paris_med_raw.columns:\n",
        "        if paris_med_raw[c].astype(str).str.upper().eq('USA').any():\n",
        "            usa_col = c\n",
        "            break\n",
        "\n",
        "paris_med_usa = paris_med_raw[paris_med_raw[usa_col].astype(str).str.upper().eq('USA')].copy()\n",
        "print(f\"USA rows (via '{usa_col}'): {len(paris_med_usa):,}\")\n",
        "\n",
        "# Identify columns\n",
        "name_col = next((c for c in ['name', 'athlete_name', 'Name', 'athlete'] if c in paris_med_usa.columns), None)\n",
        "medal_col = next((c for c in ['medal_type', 'medal_code', 'medal', 'Medal'] if c in paris_med_usa.columns), None)\n",
        "disc_col = next((c for c in ['discipline', 'sport', 'Discipline', 'Sport'] if c in paris_med_usa.columns), None)\n",
        "event_col = next((c for c in ['event', 'event_title', 'Event'] if c in paris_med_usa.columns), None)\n",
        "print(f\"Column mapping: name={name_col}, medal={medal_col}, discipline={disc_col}, event={event_col}\")\n",
        "\n",
        "# Print sample for verification\n",
        "print(f\"\\nSample rows:\")\n",
        "show_cols = [c for c in [name_col, medal_col, disc_col, event_col] if c]\n",
        "print(paris_med_usa[show_cols].head(3).to_string())\n",
        "\n",
        "# Normalize medal values\n",
        "medal_norm = {'Gold Medal': 'Gold', 'Silver Medal': 'Silver', 'Bronze Medal': 'Bronze',\n",
        "              'GOLD': 'Gold', 'SILVER': 'Silver', 'BRONZE': 'Bronze',\n",
        "              'Gold': 'Gold', 'Silver': 'Silver', 'Bronze': 'Bronze',\n",
        "              '1': 'Gold', '2': 'Silver', '3': 'Bronze'}\n",
        "\n",
        "results_paris2024 = pd.DataFrame({\n",
        "    'athlete_id': None,\n",
        "    'athlete_name': paris_med_usa[name_col] if name_col else None,\n",
        "    'games_year': 2024,\n",
        "    'games_season': 'Summer',\n",
        "    'games_type': 'Olympic',\n",
        "    'sport': paris_med_usa[disc_col] if disc_col else None,\n",
        "    'discipline': paris_med_usa[disc_col] if disc_col else None,\n",
        "    'event': paris_med_usa[event_col] if event_col else None,\n",
        "    'classification_code': None,\n",
        "    'medal': paris_med_usa[medal_col].map(medal_norm) if medal_col else None,\n",
        "    'source': 'paris2024'\n",
        "})\n",
        "\n",
        "# Check for unmapped medal values\n",
        "unmapped = results_paris2024['medal'].isna().sum()\n",
        "print(f\"\\nStandardized: {len(results_paris2024):,} rows\")\n",
        "print(f\"Medal dist:\\n{results_paris2024['medal'].value_counts(dropna=False).to_string()}\")\n",
        "if unmapped > 0:\n",
        "    raw_vals = paris_med_usa[medal_col].unique()\n",
        "    print(f\"âš ï¸ Unmapped medal values: {[v for v in raw_vals if v not in medal_norm]}\")"
      ],
      "metadata": {
        "id": "Tyn-eXAowmfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 4, Step 3: piterfm Paralympic medal results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "piterfm_dfs = []\n",
        "\n",
        "for subdir, year in [('2020_Tokyo', 2020), ('2024_Paris', 2024)]:\n",
        "    dirpath = os.path.join(LOCAL_DIR, 'paralympic-piterfm', subdir)\n",
        "    files = [f for f in os.listdir(dirpath) if f.endswith('.csv')]\n",
        "    print(f\"\\n{'=' * 50}\")\n",
        "    print(f\"{subdir} files: {files}\")\n",
        "\n",
        "    # Find medal file (prefer medallists over medals)\n",
        "    medal_file = None\n",
        "    for candidate in ['medallists.csv', 'medals.csv']:\n",
        "        if candidate in files:\n",
        "            medal_file = candidate\n",
        "            break\n",
        "    if medal_file is None:\n",
        "        print(\"  âš ï¸ No medal file found!\")\n",
        "        continue\n",
        "\n",
        "    df = pd.read_csv(os.path.join(dirpath, medal_file), low_memory=False)\n",
        "    print(f\"  {medal_file}: {len(df):,} rows, columns: {list(df.columns)}\")\n",
        "\n",
        "    # Filter to USA\n",
        "    usa_col = None\n",
        "    for c in df.columns:\n",
        "        if df[c].astype(str).str.upper().eq('USA').any() and ('country' in c.lower() or 'code' in c.lower()):\n",
        "            usa_col = c\n",
        "            break\n",
        "    if usa_col is None:\n",
        "        for c in df.columns:\n",
        "            if df[c].astype(str).str.contains('United States', case=False, na=False).any():\n",
        "                usa_col = c\n",
        "                break\n",
        "\n",
        "    if usa_col is None:\n",
        "        print(\"  âš ï¸ Could not find USA column!\")\n",
        "        continue\n",
        "\n",
        "    usa_df = df[df[usa_col].astype(str).str.contains('USA|United States', case=False, na=False)].copy()\n",
        "    print(f\"  USA rows (via '{usa_col}'): {len(usa_df):,}\")\n",
        "\n",
        "    if len(usa_df) == 0:\n",
        "        continue\n",
        "\n",
        "    # Map columns\n",
        "    name_col = next((c for c in ['name', 'athlete_name', 'Name'] if c in usa_df.columns), None)\n",
        "    medal_col = next((c for c in ['medal_type', 'medal_code', 'medal', 'Medal'] if c in usa_df.columns), None)\n",
        "    disc_col = next((c for c in ['discipline', 'sport', 'Discipline'] if c in usa_df.columns), None)\n",
        "    event_col = next((c for c in ['event', 'event_title', 'Event'] if c in usa_df.columns), None)\n",
        "    class_col = next((c for c in ['sport_class', 'classification'] if c in usa_df.columns), None)\n",
        "\n",
        "    medal_norm = {'Gold Medal': 'Gold', 'Silver Medal': 'Silver', 'Bronze Medal': 'Bronze',\n",
        "                  'GOLD': 'Gold', 'SILVER': 'Silver', 'BRONZE': 'Bronze',\n",
        "                  'Gold': 'Gold', 'Silver': 'Silver', 'Bronze': 'Bronze'}\n",
        "\n",
        "    std = pd.DataFrame({\n",
        "        'athlete_id': None,\n",
        "        'athlete_name': usa_df[name_col] if name_col else None,\n",
        "        'games_year': year,\n",
        "        'games_season': 'Summer',\n",
        "        'games_type': 'Paralympic',\n",
        "        'sport': usa_df[disc_col] if disc_col else None,\n",
        "        'discipline': usa_df[disc_col] if disc_col else None,\n",
        "        'event': usa_df[event_col] if event_col else None,\n",
        "        'classification_code': usa_df[class_col] if class_col else None,\n",
        "        'medal': usa_df[medal_col].map(medal_norm) if medal_col else None,\n",
        "        'source': f'piterfm_{year}'\n",
        "    })\n",
        "\n",
        "    piterfm_dfs.append(std)\n",
        "    print(f\"  Mapped: name={name_col}, medal={medal_col}, disc={disc_col}, event={event_col}, class={class_col}\")\n",
        "    print(f\"  Medal dist: {std['medal'].value_counts(dropna=False).head().to_string()}\")\n",
        "\n",
        "results_piterfm = pd.concat(piterfm_dfs, ignore_index=True) if piterfm_dfs else pd.DataFrame()\n",
        "print(f\"\\nTotal piterfm results: {len(results_piterfm):,}\")"
      ],
      "metadata": {
        "id": "hui9akgywpZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 4, Step 4: katiepress results + gender extraction â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Extract gender from event names (save for backfill)\n",
        "def extract_gender(event_str):\n",
        "    if pd.isna(event_str): return None\n",
        "    s = str(event_str).lower()\n",
        "    if 'women' in s or 'female' in s: return 'Female'\n",
        "    if s.startswith('men') or \"men's\" in s or ' men ' in s or 'male' in s: return 'Male'\n",
        "    if 'mixed' in s: return 'Mixed'\n",
        "    return None\n",
        "\n",
        "katie_usa['extracted_gender'] = katie_usa['event'].apply(extract_gender)\n",
        "\n",
        "# Save gender mapping for athlete backfill\n",
        "katie_gender = katie_usa.groupby('athlete_name')['extracted_gender'].agg(\n",
        "    lambda x: x.dropna().mode().iloc[0] if len(x.dropna().mode()) > 0 else None\n",
        ").reset_index()\n",
        "katie_gender.columns = ['athlete_name', 'inferred_gender']\n",
        "print(f\"Gender extraction: {katie_gender['inferred_gender'].notna().sum()} / {len(katie_gender)} athletes\")\n",
        "print(katie_gender['inferred_gender'].value_counts(dropna=False).to_string())\n",
        "\n",
        "# Standardize results\n",
        "results_katiepress = pd.DataFrame({\n",
        "    'athlete_id': None,\n",
        "    'athlete_name': katie_usa['athlete_name'],\n",
        "    'games_year': katie_usa['games_year'].astype(int),\n",
        "    'games_season': katie_usa['games_season'],\n",
        "    'games_type': 'Paralympic',\n",
        "    'sport': katie_usa['sport'],\n",
        "    'discipline': katie_usa['sport'],\n",
        "    'event': katie_usa['event'],\n",
        "    'classification_code': katie_usa['classification_code'],\n",
        "    'medal': katie_usa['medal'].str.strip().str.title(),\n",
        "    'source': 'katiepress'\n",
        "})\n",
        "\n",
        "print(f\"\\nKatiepress results: {len(results_katiepress):,}\")\n",
        "print(f\"Medal dist:\\n{results_katiepress['medal'].value_counts().to_string()}\")"
      ],
      "metadata": {
        "id": "Ud8j61Q1xYP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 4, Step 5: Stack results + assign athlete_ids â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Improved name normalization: replace non-alpha with space (fixes bullet chars)\n",
        "def norm_for_match(n):\n",
        "    if pd.isna(n): return ''\n",
        "    s = re.sub(r'[^A-Za-z\\s]', ' ', str(n)).upper().strip()\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    return ' '.join(sorted(s.split()))\n",
        "\n",
        "# Stack all results\n",
        "team_usa_results = pd.concat([\n",
        "    results_keithgalli,\n",
        "    results_paris2024,\n",
        "    results_piterfm,\n",
        "    results_katiepress\n",
        "], ignore_index=True)\n",
        "\n",
        "print(f\"Stacked results: {len(team_usa_results):,}\")\n",
        "print(f\"  With athlete_id (keithgalli): {team_usa_results['athlete_id'].notna().sum():,}\")\n",
        "print(f\"  Need ID assignment: {team_usa_results['athlete_id'].isna().sum():,}\")\n",
        "\n",
        "# Build athlete lookup: (normalized_name, games_type) â†’ athlete_id\n",
        "ath_lookup = team_usa_athletes[['athlete_id', 'name', 'games_type']].copy()\n",
        "ath_lookup['_norm'] = ath_lookup['name'].apply(norm_for_match)\n",
        "name_type_to_id = {}\n",
        "for _, row in ath_lookup.iterrows():\n",
        "    key = (row['_norm'], row['games_type'])\n",
        "    if key not in name_type_to_id:\n",
        "        name_type_to_id[key] = row['athlete_id']\n",
        "\n",
        "# Assign IDs to unmatched results\n",
        "needs_id = team_usa_results['athlete_id'].isna()\n",
        "norm_names = team_usa_results.loc[needs_id, 'athlete_name'].apply(norm_for_match)\n",
        "game_types = team_usa_results.loc[needs_id, 'games_type']\n",
        "\n",
        "team_usa_results.loc[needs_id, 'athlete_id'] = [\n",
        "    name_type_to_id.get((n, g)) for n, g in zip(norm_names, game_types)\n",
        "]\n",
        "\n",
        "# Report\n",
        "matched = team_usa_results['athlete_id'].notna().sum()\n",
        "unmatched = team_usa_results['athlete_id'].isna().sum()\n",
        "print(f\"\\nAfter ID assignment:\")\n",
        "print(f\"  Matched:   {matched:,}\")\n",
        "print(f\"  Unmatched: {unmatched:,}\")\n",
        "\n",
        "if unmatched > 0:\n",
        "    print(f\"\\n  Unmatched by source:\")\n",
        "    print(team_usa_results[team_usa_results['athlete_id'].isna()]['source'].value_counts().to_string())\n",
        "    print(f\"\\n  Sample unmatched (first 10):\")\n",
        "    print(team_usa_results[team_usa_results['athlete_id'].isna()][\n",
        "        ['athlete_name', 'games_type', 'games_year', 'event', 'source']].head(10).to_string())\n",
        "\n",
        "# Duplicate check\n",
        "dupes = team_usa_results.duplicated(subset=['athlete_id', 'games_year', 'games_type', 'event'], keep=False)\n",
        "print(f\"\\nPotential duplicate results: {dupes.sum()} rows\")\n",
        "if dupes.sum() > 0 and dupes.sum() < 20:\n",
        "    print(team_usa_results[dupes][['athlete_name', 'games_year', 'event', 'medal', 'source']].head(10).to_string())"
      ],
      "metadata": {
        "id": "u9uC0A4Vxa2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 4, Step 6: Backfill athletes table from results â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# 1. Recompute career stats from unified results (source of truth)\n",
        "career_new = team_usa_results[team_usa_results['athlete_id'].notna()].groupby('athlete_id').agg(\n",
        "    r_first_year=('games_year', 'min'),\n",
        "    r_last_year=('games_year', 'max'),\n",
        "    r_games_count=('games_year', 'nunique'),\n",
        "    r_gold=('medal', lambda x: (x == 'Gold').sum()),\n",
        "    r_silver=('medal', lambda x: (x == 'Silver').sum()),\n",
        "    r_bronze=('medal', lambda x: (x == 'Bronze').sum()),\n",
        ").reset_index()\n",
        "career_new['r_total'] = career_new['r_gold'] + career_new['r_silver'] + career_new['r_bronze']\n",
        "\n",
        "print(f\"Career stats recomputed for {len(career_new):,} athletes from results\")\n",
        "\n",
        "# 2. Merge and replace\n",
        "team_usa_athletes = team_usa_athletes.merge(career_new, on='athlete_id', how='left')\n",
        "\n",
        "# Prefer results-derived (covers all sources) over original (single source)\n",
        "field_map = {\n",
        "    'first_games_year': 'r_first_year',\n",
        "    'last_games_year': 'r_last_year',\n",
        "    'games_count': 'r_games_count',\n",
        "    'gold_count': 'r_gold',\n",
        "    'silver_count': 'r_silver',\n",
        "    'bronze_count': 'r_bronze',\n",
        "    'total_medals': 'r_total'\n",
        "}\n",
        "\n",
        "for orig, derived in field_map.items():\n",
        "    if derived in team_usa_athletes.columns:\n",
        "        team_usa_athletes[orig] = team_usa_athletes[derived].fillna(team_usa_athletes[orig])\n",
        "\n",
        "# Drop temp columns\n",
        "r_cols = [c for c in team_usa_athletes.columns if c.startswith('r_')]\n",
        "team_usa_athletes.drop(columns=r_cols, inplace=True)\n",
        "\n",
        "# 3. Ensure piterfm athletes have participation data even without medals\n",
        "for src_df, year, label in [(tokyo_usa, 2020, 'Tokyo 2020'), (paris_para_usa, 2024, 'Paris 2024')]:\n",
        "    src_norms = set(src_df['name'].apply(norm_for_match))\n",
        "    ath_norms = team_usa_athletes['name'].apply(norm_for_match)\n",
        "    mask = ath_norms.isin(src_norms) & (team_usa_athletes['games_type'] == 'Paralympic')\n",
        "\n",
        "    team_usa_athletes.loc[mask, 'first_games_year'] = team_usa_athletes.loc[mask, 'first_games_year'].apply(\n",
        "        lambda x: min(x, year) if pd.notna(x) else float(year))\n",
        "    team_usa_athletes.loc[mask, 'last_games_year'] = team_usa_athletes.loc[mask, 'last_games_year'].apply(\n",
        "        lambda x: max(x, year) if pd.notna(x) else float(year))\n",
        "    team_usa_athletes.loc[mask & team_usa_athletes['games_count'].isna(), 'games_count'] = 1\n",
        "    print(f\"  {label} participation ensured for {mask.sum()} athletes\")\n",
        "\n",
        "# Athletes in both Tokyo + Paris get games_count >= 2\n",
        "both_norms = set(tokyo_usa['name'].apply(norm_for_match)) & set(paris_para_usa['name'].apply(norm_for_match))\n",
        "both_mask = team_usa_athletes['name'].apply(norm_for_match).isin(both_norms) & \\\n",
        "            (team_usa_athletes['games_type'] == 'Paralympic')\n",
        "team_usa_athletes.loc[both_mask, 'games_count'] = team_usa_athletes.loc[both_mask, 'games_count'].apply(\n",
        "    lambda x: max(x, 2) if pd.notna(x) else 2)\n",
        "print(f\"  Dual-Games athletes (Tokyo+Paris): {both_mask.sum()}\")\n",
        "\n",
        "# 4. Fill NaN medals with 0 for athletes with known participation\n",
        "has_participation = team_usa_athletes['games_count'].notna() & (team_usa_athletes['games_count'] > 0)\n",
        "for col in ['gold_count', 'silver_count', 'bronze_count', 'total_medals']:\n",
        "    before_na = team_usa_athletes[col].isna().sum()\n",
        "    team_usa_athletes.loc[has_participation & team_usa_athletes[col].isna(), col] = 0\n",
        "    filled = before_na - team_usa_athletes[col].isna().sum()\n",
        "    if filled > 0:\n",
        "        print(f\"  {col}: filled {filled} NaNs â†’ 0\")\n",
        "\n",
        "# 5. Gender backfill from katiepress event names\n",
        "katie_gender['_norm'] = katie_gender['athlete_name'].apply(norm_for_match)\n",
        "gender_map = dict(zip(katie_gender['_norm'], katie_gender['inferred_gender']))\n",
        "\n",
        "team_usa_athletes['_norm'] = team_usa_athletes['name'].apply(norm_for_match)\n",
        "needs_gender = team_usa_athletes['gender'].isna()\n",
        "team_usa_athletes.loc[needs_gender, 'gender'] = team_usa_athletes.loc[needs_gender, '_norm'].map(gender_map)\n",
        "\n",
        "gender_filled = needs_gender.sum() - team_usa_athletes['gender'].isna().sum()\n",
        "team_usa_athletes.drop(columns=['_norm'], inplace=True)\n",
        "\n",
        "print(f\"\\n  Gender backfill: {gender_filled} athletes updated\")\n",
        "print(f\"  Remaining without gender: {team_usa_athletes['gender'].isna().sum()}\")\n",
        "\n",
        "print(f\"\\nâœ… Backfill complete\")"
      ],
      "metadata": {
        "id": "Y3RbU9pfxc8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 4, Step 6b: Cleanup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# 1. Fix year=0 artifacts (NaN years that were filled with 0)\n",
        "zero_year_results = (team_usa_results['games_year'] == 0).sum()\n",
        "zero_year_athletes = (team_usa_athletes['first_games_year'] == 0).sum()\n",
        "\n",
        "# In results: drop rows with year=0 (these have no usable data)\n",
        "team_usa_results = team_usa_results[team_usa_results['games_year'] != 0].reset_index(drop=True)\n",
        "\n",
        "# In athletes: set 0 back to NaN\n",
        "for col in ['first_games_year', 'last_games_year']:\n",
        "    team_usa_athletes.loc[team_usa_athletes[col] == 0, col] = None\n",
        "\n",
        "print(f\"Year=0 cleanup:\")\n",
        "print(f\"  Results rows dropped: {zero_year_results}\")\n",
        "print(f\"  Athletes years reset to NaN: {zero_year_athletes}\")\n",
        "\n",
        "# 2. Set \"Mixed\" gender to None (event descriptor, not athlete gender)\n",
        "mixed_count = (team_usa_athletes['gender'] == 'Mixed').sum()\n",
        "team_usa_athletes.loc[team_usa_athletes['gender'] == 'Mixed', 'gender'] = None\n",
        "print(f\"\\n  'Mixed' gender reset to None: {mixed_count}\")\n",
        "\n",
        "print(f\"\\nâœ… Cleanup complete\")\n",
        "print(f\"  Results: {len(team_usa_results):,} rows\")\n",
        "print(f\"  Athletes: {len(team_usa_athletes):,} rows\")"
      ],
      "metadata": {
        "id": "A7_9ldnCyOOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 4, Step 7: Checkpoint to GCS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ath_path = '/tmp/team_usa_athletes.csv'\n",
        "team_usa_athletes.to_csv(ath_path, index=False)\n",
        "!gsutil cp {ath_path} gs://class-demo/team-usa/processed/team_usa_athletes.csv\n",
        "\n",
        "res_path = '/tmp/team_usa_results.csv'\n",
        "team_usa_results.to_csv(res_path, index=False)\n",
        "!gsutil cp {res_path} gs://class-demo/team-usa/processed/team_usa_results.csv\n",
        "\n",
        "print(f\"âœ… Checkpoints saved:\")\n",
        "print(f\"   Athletes: {len(team_usa_athletes):,} rows â†’ gs://class-demo/team-usa/processed/team_usa_athletes.csv\")\n",
        "print(f\"   Results:  {len(team_usa_results):,} rows â†’ gs://class-demo/team-usa/processed/team_usa_results.csv\")"
      ],
      "metadata": {
        "id": "U7Oyz4QNxgE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ PHASE 4 QC REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print('=' * 70)\n",
        "print('PHASE 4 QC: UNIFIED RESULTS + BACKFILLED ATHLETES')\n",
        "print('=' * 70)\n",
        "\n",
        "print(f'\\n{\"â”€\" * 40}')\n",
        "print(f'RESULTS TABLE')\n",
        "print(f'{\"â”€\" * 40}')\n",
        "\n",
        "print(f'\\nðŸ“Š COUNTS')\n",
        "print(f'  Total result rows:     {len(team_usa_results):,}')\n",
        "print(f'  With athlete_id:       {team_usa_results[\"athlete_id\"].notna().sum():,}')\n",
        "print(f'  Orphan (no ID):        {team_usa_results[\"athlete_id\"].isna().sum():,}')\n",
        "\n",
        "print(f'\\nðŸ“¦ BY SOURCE')\n",
        "print(team_usa_results['source'].value_counts().to_string())\n",
        "\n",
        "print(f'\\nðŸ… MEDALS')\n",
        "print(team_usa_results['medal'].value_counts(dropna=False).to_string())\n",
        "\n",
        "print(f'\\nðŸ“… YEAR RANGE')\n",
        "for gt in ['Olympic', 'Paralympic']:\n",
        "    sub = team_usa_results[team_usa_results['games_type'] == gt]\n",
        "    if len(sub) > 0:\n",
        "        print(f'  {gt}: {sub[\"games_year\"].min()} â€” {sub[\"games_year\"].max()} ({len(sub):,} rows)')\n",
        "\n",
        "print(f'\\nðŸŸï¸ SEASON SPLIT')\n",
        "print(team_usa_results['games_season'].value_counts().to_string())\n",
        "\n",
        "print(f'\\n{\"â”€\" * 40}')\n",
        "print(f'ATHLETES TABLE (post-backfill)')\n",
        "print(f'{\"â”€\" * 40}')\n",
        "\n",
        "print(f'\\nðŸ“Š COUNTS')\n",
        "print(f'  Total athletes:        {len(team_usa_athletes):,}')\n",
        "oly = team_usa_athletes[team_usa_athletes['games_type'] == 'Olympic']\n",
        "para = team_usa_athletes[team_usa_athletes['games_type'] == 'Paralympic']\n",
        "print(f'  Olympic:               {len(oly):,}')\n",
        "print(f'  Paralympic:            {len(para):,}')\n",
        "\n",
        "print(f'\\nðŸ“… TEMPORAL (post-backfill)')\n",
        "print(f'  Olympic:    {oly[\"first_games_year\"].min()} â€” {oly[\"last_games_year\"].max()}')\n",
        "print(f'  Paralympic: {para[\"first_games_year\"].min()} â€” {para[\"last_games_year\"].max()}')\n",
        "\n",
        "print(f'\\nðŸ… MEDALS (post-backfill)')\n",
        "print(f'  Total medals:          {team_usa_athletes[\"total_medals\"].sum():,.0f}')\n",
        "print(f'  Olympic medalists:     {(oly[\"total_medals\"] > 0).sum():,}')\n",
        "print(f'  Paralympic medalists:  {(para[\"total_medals\"] > 0).sum():,}')\n",
        "\n",
        "print(f'\\nâš§ GENDER (post-backfill)')\n",
        "print(team_usa_athletes['gender'].value_counts(dropna=False).to_string())\n",
        "\n",
        "print(f'\\nðŸ“‹ NULL CHECK (athletes)')\n",
        "for col in ['name', 'gender', 'birth_date', 'games_count', 'first_games_year',\n",
        "            'last_games_year', 'total_medals', 'classification_code', 'primary_sport']:\n",
        "    nulls = team_usa_athletes[col].isna().sum()\n",
        "    pct = nulls / len(team_usa_athletes) * 100\n",
        "    flag = 'âš ï¸' if pct > 50 else '  '\n",
        "    print(f'  {flag} {col:25s} {nulls:>6,} null ({pct:.1f}%)')\n",
        "\n",
        "print(f'\\nðŸ”— CROSS-CHECK')\n",
        "athletes_in_results = set(team_usa_results['athlete_id'].dropna())\n",
        "athletes_in_table = set(team_usa_athletes['athlete_id'])\n",
        "print(f'  Athletes in both:      {len(athletes_in_results & athletes_in_table):,}')\n",
        "print(f'  In results only:       {len(athletes_in_results - athletes_in_table):,}')\n",
        "print(f'  In table only:         {len(athletes_in_table - athletes_in_results):,} (bios without results)')\n",
        "\n",
        "print(f'\\nðŸ“‹ SAMPLE PARALYMPIC (post-backfill, 5 random):')\n",
        "sample_cols = ['name', 'classification_code', 'primary_sport', 'games_count',\n",
        "               'total_medals', 'first_games_year', 'last_games_year', 'gender', 'source']\n",
        "print(para[sample_cols].sample(5, random_state=42).to_string())\n",
        "\n",
        "print(f'\\n{\"=\" * 70}')\n",
        "print('Paste this output back for review before proceeding to Phase 5.')\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "id": "hddg1gkbxjMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Phase 5: Enrichment â€” Gemini Profile Summaries + Embeddings\n",
        "\n",
        "Generate AI-powered athlete profiles using Gemini 2.5 Flash with Google Search grounding, then create vector embeddings for similarity search.\n",
        "\n",
        "**Two passes:**\n",
        "1. **Profile summaries** â€” Gemini generates 2-paragraph bios from structured data + web search\n",
        "2. **Vector embeddings** â€” Gemini Embedding model (3072 dims) encodes profiles for AlloyDB pgvector\n",
        "\n",
        "**SDK:** google-genai (unified SDK) via Vertex AI  \n",
        "**Performance:** ~50 concurrent workers with progress checkpointing and resume"
      ],
      "metadata": {
        "id": "X11DiLa946wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 5, Step 1: Setup + Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "!pip install -q google-genai tqdm\n",
        "\n",
        "import json\n",
        "import time\n",
        "import csv\n",
        "import threading\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, List\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class SummaryConfig:\n",
        "    \"\"\"Configuration for profile summary generation\"\"\"\n",
        "    MODEL_NAME = 'gemini-2.5-flash'\n",
        "    TEMPERATURE = 0.7\n",
        "    MAX_OUTPUT_TOKENS = 1000\n",
        "\n",
        "    BATCH_SIZE = 200\n",
        "    MAX_WORKERS = 50  # Conservative for grounded calls; bump to 75 if no rate limits\n",
        "    SAVE_INTERVAL = 200\n",
        "    MAX_RETRIES = 3\n",
        "    RETRY_DELAY = 2\n",
        "\n",
        "    MIN_LENGTH = 100\n",
        "    MAX_LENGTH = 3000\n",
        "\n",
        "    PROGRESS_FILE = '/tmp/athlete_summaries_progress.csv'\n",
        "    ERROR_LOG = '/tmp/athlete_summary_errors.csv'\n",
        "\n",
        "class EmbeddingConfig:\n",
        "    \"\"\"Configuration for embedding generation\"\"\"\n",
        "    MODEL_NAME = 'gemini-embedding-001'\n",
        "    OUTPUT_DIMENSION = 3072\n",
        "\n",
        "    BATCH_SIZE = 200\n",
        "    MAX_WORKERS = 75  # Embeddings are lighter, can run more\n",
        "    SAVE_INTERVAL = 200\n",
        "    MAX_RETRIES = 3\n",
        "    RETRY_DELAY = 2\n",
        "\n",
        "    PROGRESS_FILE = '/tmp/athlete_embeddings_progress.csv'\n",
        "    ERROR_LOG = '/tmp/athlete_embedding_errors.csv'\n",
        "\n",
        "print(f\"Summary config: {SummaryConfig.MODEL_NAME}, {SummaryConfig.MAX_WORKERS} workers\")\n",
        "print(f\"Embedding config: {EmbeddingConfig.MODEL_NAME}, {EmbeddingConfig.OUTPUT_DIMENSION} dims, {EmbeddingConfig.MAX_WORKERS} workers\")\n",
        "print(f\"Athletes to process: {len(team_usa_athletes):,}\")"
      ],
      "metadata": {
        "id": "0rEObu5y48tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 5, Step 2: Thread-local Gemini client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import google.auth\n",
        "\n",
        "_thread_local = threading.local()\n",
        "\n",
        "def get_client():\n",
        "    \"\"\"Get or create a thread-local Gemini client.\"\"\"\n",
        "    if getattr(_thread_local, \"client\", None) is None:\n",
        "        credentials, _ = google.auth.default()\n",
        "        _thread_local.client = genai.Client(\n",
        "            vertexai=True,\n",
        "            project=PROJECT_ID,\n",
        "            location=REGION,\n",
        "        )\n",
        "    return _thread_local.client\n",
        "\n",
        "# Test connection\n",
        "try:\n",
        "    test_response = get_client().models.generate_content(\n",
        "        model=SummaryConfig.MODEL_NAME,\n",
        "        contents='Say \"API connected\" and nothing else.',\n",
        "        config=types.GenerateContentConfig(max_output_tokens=20),\n",
        "    )\n",
        "    response_text = test_response.text if test_response.text else \"(empty response)\"\n",
        "    print(f\"âœ… Gemini API connected\")\n",
        "    print(f\"   Project: {PROJECT_ID}\")\n",
        "    print(f\"   Region: {REGION}\")\n",
        "    print(f\"   Model: {SummaryConfig.MODEL_NAME}\")\n",
        "    print(f\"   Response: {response_text.strip()}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Connection failed: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "CkBvatys4_Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 5, Step 3: Profile generation functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def create_profile_prompt(row) -> str:\n",
        "    \"\"\"Build a rich prompt from available athlete data.\"\"\"\n",
        "\n",
        "    name = str(row.get('name', 'Unknown')).replace('â€¢', ' ').strip()\n",
        "    games_type = row.get('games_type', 'Olympic')\n",
        "\n",
        "    parts = [f'Write a 2-paragraph profile for {name}, a Team USA {games_type} athlete.']\n",
        "\n",
        "    # Structured facts\n",
        "    facts = []\n",
        "    if pd.notna(row.get('primary_sport')):\n",
        "        facts.append(f\"Sport: {row['primary_sport']}\")\n",
        "    if pd.notna(row.get('classification_code')):\n",
        "        facts.append(f\"Paralympic classification: {row['classification_code']}\")\n",
        "    if pd.notna(row.get('gender')):\n",
        "        facts.append(f\"Gender: {row['gender']}\")\n",
        "    if pd.notna(row.get('games_count')) and row['games_count'] > 0:\n",
        "        year_str = \"\"\n",
        "        if pd.notna(row.get('first_games_year')) and pd.notna(row.get('last_games_year')):\n",
        "            fy, ly = int(row['first_games_year']), int(row['last_games_year'])\n",
        "            year_str = f\" ({fy})\" if fy == ly else f\" ({fy}â€“{ly})\"\n",
        "        facts.append(f\"Games appearances: {int(row['games_count'])}{year_str}\")\n",
        "\n",
        "    medal_parts = []\n",
        "    for label, col in [('Gold', 'gold_count'), ('Silver', 'silver_count'), ('Bronze', 'bronze_count')]:\n",
        "        if pd.notna(row.get(col)) and row[col] > 0:\n",
        "            medal_parts.append(f\"{int(row[col])} {label}\")\n",
        "    if medal_parts:\n",
        "        facts.append(f\"Medals: {', '.join(medal_parts)}\")\n",
        "\n",
        "    if facts:\n",
        "        parts.append(\"\\nKnown facts:\\n\" + \"\\n\".join(f\"- {f}\" for f in facts))\n",
        "\n",
        "    # Bio fields (Paris 2024 Paralympic)\n",
        "    bio_parts = []\n",
        "    for field, label in [('reason', 'Why they compete'), ('hero', 'Hero/inspiration'),\n",
        "                          ('philosophy', 'Philosophy'), ('other_sports', 'Other sports'),\n",
        "                          ('coach', 'Coach'), ('hobbies', 'Hobbies'),\n",
        "                          ('occupation', 'Occupation'), ('education', 'Education')]:\n",
        "        if pd.notna(row.get(field)):\n",
        "            bio_parts.append(f\"{label}: {row[field]}\")\n",
        "\n",
        "    if bio_parts:\n",
        "        parts.append(\"\\nAdditional background:\\n\" + \"\\n\".join(f\"- {b}\" for b in bio_parts))\n",
        "\n",
        "    # Instructions\n",
        "    instructions = \"\"\"\n",
        "Paragraph 1: Athletic career â€” key achievements, competition highlights, and what makes this athlete notable.\n",
        "Paragraph 2: Personal background â€” how they entered their sport, their training journey, and what drives them.\n",
        "\n",
        "Write 150-250 words total in an engaging, encyclopedic tone.\"\"\"\n",
        "\n",
        "    if pd.notna(row.get('classification_code')):\n",
        "        instructions += f\"\\nSince this is a Paralympic athlete classified as {row['classification_code']}, briefly explain what that classification means in the context of their sport.\"\n",
        "\n",
        "    parts.append(instructions)\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "\n",
        "def generate_profile(row, retries: int = SummaryConfig.MAX_RETRIES) -> Tuple[Optional[str], Optional[str]]:\n",
        "    \"\"\"Generate a profile summary with Google Search grounding.\"\"\"\n",
        "\n",
        "    prompt = create_profile_prompt(row)\n",
        "\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature=SummaryConfig.TEMPERATURE,\n",
        "        max_output_tokens=SummaryConfig.MAX_OUTPUT_TOKENS,\n",
        "        tools=[types.Tool(google_search=types.GoogleSearch())],\n",
        "    )\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = get_client().models.generate_content(\n",
        "                model=SummaryConfig.MODEL_NAME,\n",
        "                contents=prompt,\n",
        "                config=config,\n",
        "            )\n",
        "\n",
        "            summary = None\n",
        "            if hasattr(response, 'text') and response.text:\n",
        "                summary = response.text\n",
        "            elif hasattr(response, 'candidates') and response.candidates:\n",
        "                candidate = response.candidates[0]\n",
        "                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):\n",
        "                    parts = candidate.content.parts\n",
        "                    if parts and hasattr(parts[0], 'text'):\n",
        "                        summary = parts[0].text\n",
        "\n",
        "            if not summary:\n",
        "                if attempt < retries - 1:\n",
        "                    time.sleep(SummaryConfig.RETRY_DELAY)\n",
        "                    continue\n",
        "                return None, \"No text in API response\"\n",
        "\n",
        "            summary = summary.strip()\n",
        "\n",
        "            if len(summary) < SummaryConfig.MIN_LENGTH:\n",
        "                if attempt < retries - 1:\n",
        "                    time.sleep(SummaryConfig.RETRY_DELAY)\n",
        "                    continue\n",
        "                return None, f\"Too short ({len(summary)} chars)\"\n",
        "\n",
        "            if len(summary) > SummaryConfig.MAX_LENGTH:\n",
        "                summary = summary[:SummaryConfig.MAX_LENGTH] + \"...\"\n",
        "\n",
        "            # Clean for CSV\n",
        "            summary = re.sub(r'\\s+', ' ', summary).strip()\n",
        "\n",
        "            return summary, None\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"{type(e).__name__}: {str(e)}\"\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(SummaryConfig.RETRY_DELAY * (attempt + 1))\n",
        "                continue\n",
        "            return None, error_msg\n",
        "\n",
        "    return None, \"Max retries exceeded\"\n",
        "\n",
        "\n",
        "def log_error(athlete_id, name, error_msg, filename):\n",
        "    \"\"\"Log errors to CSV.\"\"\"\n",
        "    file_exists = Path(filename).exists()\n",
        "    with open(filename, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=['timestamp', 'athlete_id', 'name', 'error'])\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerow({\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'athlete_id': athlete_id,\n",
        "            'name': name,\n",
        "            'error': error_msg\n",
        "        })\n",
        "\n",
        "print(\"âœ… Profile generation functions defined\")"
      ],
      "metadata": {
        "id": "sSvitMIw5JpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 5, Step 4: Test with a single athlete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Pick one well-known athlete and one sparse one\n",
        "test_athletes = [\n",
        "    team_usa_athletes[team_usa_athletes['name'].str.contains('Simone', na=False)].head(1),\n",
        "    team_usa_athletes[team_usa_athletes['source'] == 'katiepress'].sample(1, random_state=42),\n",
        "    team_usa_athletes[(team_usa_athletes['games_type'] == 'Paralympic') &\n",
        "                       (team_usa_athletes['classification_code'].notna())].sample(1, random_state=42)\n",
        "]\n",
        "\n",
        "for test_df in test_athletes:\n",
        "    if len(test_df) == 0:\n",
        "        continue\n",
        "    row = test_df.iloc[0]\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"Testing: {row['name']} ({row['games_type']}, {row.get('primary_sport', 'N/A')})\")\n",
        "    print(f\"  Classification: {row.get('classification_code', 'N/A')}\")\n",
        "    print(f\"  Medals: {row.get('total_medals', 0)}, Games: {row.get('games_count', 'N/A')}\")\n",
        "    print(f\"  Source: {row.get('source', 'N/A')}\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "\n",
        "    print(f\"\\nPrompt preview (first 300 chars):\")\n",
        "    prompt = create_profile_prompt(row)\n",
        "    print(prompt[:300] + \"...\")\n",
        "\n",
        "    print(f\"\\nGenerating profile...\")\n",
        "    summary, error = generate_profile(row)\n",
        "\n",
        "    if summary:\n",
        "        print(f\"\\nâœ… Success ({len(summary)} chars):\")\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(f\"\\nâŒ Error: {error}\")"
      ],
      "metadata": {
        "id": "hgajvZAV5lxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 5, Step 5: Parallel summary processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def process_single_profile(idx_row) -> dict:\n",
        "    \"\"\"Process a single athlete for summary generation.\"\"\"\n",
        "    idx, row = idx_row\n",
        "    athlete_id = row['athlete_id']\n",
        "    name = str(row.get('name', 'Unknown')).replace('â€¢', ' ')\n",
        "\n",
        "    summary, error = generate_profile(row)\n",
        "\n",
        "    if summary:\n",
        "        return {'athlete_id': athlete_id, 'profile_summary': summary, 'error': None}\n",
        "    else:\n",
        "        log_error(athlete_id, name, error, SummaryConfig.ERROR_LOG)\n",
        "        return {\n",
        "            'athlete_id': athlete_id,\n",
        "            'profile_summary': f\"[Profile generation failed: {error}]\",\n",
        "            'error': error\n",
        "        }\n",
        "\n",
        "\n",
        "def save_summary_progress(results, filename):\n",
        "    \"\"\"Save summary progress to CSV.\"\"\"\n",
        "    df = pd.DataFrame(results)[['athlete_id', 'profile_summary']]\n",
        "    df.to_csv(filename, index=False, encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "    print(f\"  ðŸ’¾ Progress: {filename} ({len(df):,} profiles)\")\n",
        "\n",
        "\n",
        "def generate_all_profiles(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Generate profiles for all athletes with parallel processing + resume.\"\"\"\n",
        "\n",
        "    results = []\n",
        "    errors = []\n",
        "    processed_ids = set()\n",
        "\n",
        "    # Resume from progress\n",
        "    progress_path = Path(SummaryConfig.PROGRESS_FILE)\n",
        "    if progress_path.exists():\n",
        "        print(f\"\\nðŸ“‚ Found progress file: {SummaryConfig.PROGRESS_FILE}\")\n",
        "        df_progress = pd.read_csv(SummaryConfig.PROGRESS_FILE)\n",
        "\n",
        "        successful = df_progress[~df_progress['profile_summary'].str.contains(\n",
        "            r'\\[Profile generation failed', na=False, regex=True)]\n",
        "        processed_ids = set(successful['athlete_id'])\n",
        "\n",
        "        for _, row in successful.iterrows():\n",
        "            results.append({\n",
        "                'athlete_id': row['athlete_id'],\n",
        "                'profile_summary': row['profile_summary'],\n",
        "                'error': None\n",
        "            })\n",
        "\n",
        "        failed_count = len(df_progress) - len(successful)\n",
        "        print(f\"  âœ… Loaded {len(successful):,} successful profiles\")\n",
        "        print(f\"  ðŸ”„ Will retry {failed_count:,} failed profiles\")\n",
        "\n",
        "    # Filter to unprocessed\n",
        "    df_todo = df[~df['athlete_id'].isin(processed_ids)].copy()\n",
        "\n",
        "    if len(df_todo) == 0:\n",
        "        print(f\"\\nâœ… All profiles already generated!\")\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"Starting PARALLEL profile generation\")\n",
        "    print(f\"  Total athletes:      {len(df):,}\")\n",
        "    print(f\"  Already processed:   {len(processed_ids):,}\")\n",
        "    print(f\"  To process:          {len(df_todo):,}\")\n",
        "    print(f\"  Workers:             {SummaryConfig.MAX_WORKERS}\")\n",
        "    print(f\"  Model:               {SummaryConfig.MODEL_NAME} + Google Search\")\n",
        "    print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    rows_to_process = list(df_todo.iterrows())\n",
        "    total_batches = (len(rows_to_process) + SummaryConfig.BATCH_SIZE - 1) // SummaryConfig.BATCH_SIZE\n",
        "\n",
        "    with tqdm(total=len(rows_to_process), desc=\"Generating profiles\") as pbar:\n",
        "        for batch_num in range(total_batches):\n",
        "            start_idx = batch_num * SummaryConfig.BATCH_SIZE\n",
        "            end_idx = min(start_idx + SummaryConfig.BATCH_SIZE, len(rows_to_process))\n",
        "            batch = rows_to_process[start_idx:end_idx]\n",
        "\n",
        "            with ThreadPoolExecutor(max_workers=SummaryConfig.MAX_WORKERS) as executor:\n",
        "                futures = {executor.submit(process_single_profile, item): item for item in batch}\n",
        "\n",
        "                for future in as_completed(futures):\n",
        "                    result = future.result()\n",
        "                    results.append(result)\n",
        "                    if result['error']:\n",
        "                        errors.append(result['athlete_id'])\n",
        "                    pbar.update(1)\n",
        "\n",
        "            # Checkpoint\n",
        "            if len(results) % SummaryConfig.SAVE_INTERVAL == 0 or end_idx == len(rows_to_process):\n",
        "                save_summary_progress(results, SummaryConfig.PROGRESS_FILE)\n",
        "\n",
        "    # Final save\n",
        "    save_summary_progress(results, SummaryConfig.PROGRESS_FILE)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"Profile Generation Complete!\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "    print(f\"  Newly processed: {len(df_todo):,}\")\n",
        "    print(f\"  Total profiles:  {len(results):,}\")\n",
        "    print(f\"  Successful:      {len(results) - len(errors):,}\")\n",
        "    print(f\"  Errors:          {len(errors):,}\")\n",
        "    if len(df_todo) > 0:\n",
        "        print(f\"  Time:            {elapsed/60:.1f} minutes\")\n",
        "        print(f\"  Throughput:      {len(df_todo)/elapsed*60:.1f} athletes/minute\")\n",
        "    if errors:\n",
        "        print(f\"  Error rate:      {len(errors)/len(results)*100:.1f}%\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"âœ… Parallel summary processing functions defined\")"
      ],
      "metadata": {
        "id": "vE8w89PD5uSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 5, Step 6: Run profile generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "df_profiles = generate_all_profiles(team_usa_athletes)"
      ],
      "metadata": {
        "id": "rK-Tz3Wi56W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 5, Step 7: Summary QC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print('=' * 60)\n",
        "print('PHASE 5a QC: PROFILE SUMMARIES')\n",
        "print('=' * 60)\n",
        "\n",
        "successful = df_profiles[~df_profiles['profile_summary'].str.contains(\n",
        "    r'\\[Profile generation failed', na=False, regex=True)]\n",
        "failed = df_profiles[df_profiles['profile_summary'].str.contains(\n",
        "    r'\\[Profile generation failed', na=False, regex=True)]\n",
        "\n",
        "print(f'\\nðŸ“Š COUNTS')\n",
        "print(f'  Total profiles:     {len(df_profiles):,}')\n",
        "print(f'  Successful:         {len(successful):,}')\n",
        "print(f'  Failed:             {len(failed):,}')\n",
        "if len(failed) > 0:\n",
        "    print(f'  Error rate:         {len(failed)/len(df_profiles)*100:.1f}%')\n",
        "\n",
        "if len(successful) > 0:\n",
        "    lengths = successful['profile_summary'].str.len()\n",
        "    print(f'\\nðŸ“ LENGTH STATS')\n",
        "    print(f'  Min:    {lengths.min():,} chars')\n",
        "    print(f'  Max:    {lengths.max():,} chars')\n",
        "    print(f'  Mean:   {lengths.mean():,.0f} chars')\n",
        "    print(f'  Median: {lengths.median():,.0f} chars')\n",
        "\n",
        "# Check coverage by source\n",
        "merged_check = df_profiles.merge(\n",
        "    team_usa_athletes[['athlete_id', 'games_type', 'source']], on='athlete_id', how='left')\n",
        "print(f'\\nðŸ“¦ BY SOURCE')\n",
        "for src in merged_check['source'].unique():\n",
        "    sub = merged_check[merged_check['source'] == src]\n",
        "    success = (~sub['profile_summary'].str.contains(\n",
        "        r'\\[Profile generation failed', na=False, regex=True)).sum()\n",
        "    print(f'  {src:15s} {success:>6,} / {len(sub):>6,} successful')\n",
        "\n",
        "print(f'\\nðŸ“¦ BY TYPE')\n",
        "for gt in ['Olympic', 'Paralympic']:\n",
        "    sub = merged_check[merged_check['games_type'] == gt]\n",
        "    success = (~sub['profile_summary'].str.contains(\n",
        "        r'\\[Profile generation failed', na=False, regex=True)).sum()\n",
        "    print(f'  {gt:15s} {success:>6,} / {len(sub):>6,} successful')\n",
        "\n",
        "# Sample profiles\n",
        "print(f'\\nðŸ“‹ SAMPLE PROFILES (1 Olympic, 1 Paralympic):')\n",
        "for gt in ['Olympic', 'Paralympic']:\n",
        "    sub = merged_check[(merged_check['games_type'] == gt) &\n",
        "                        (~merged_check['profile_summary'].str.contains(\n",
        "                            r'\\[Profile generation failed', na=False, regex=True))]\n",
        "    if len(sub) > 0:\n",
        "        sample = sub.sample(1, random_state=42).iloc[0]\n",
        "        print(f'\\n  --- {gt} ({sample[\"source\"]}) ---')\n",
        "        print(f'  {sample[\"profile_summary\"][:400]}...')\n",
        "\n",
        "if len(failed) > 0:\n",
        "    print(f'\\nâš ï¸ SAMPLE FAILURES:')\n",
        "    print(failed[['athlete_id', 'profile_summary']].head(5).to_string())\n",
        "\n",
        "print(f'\\n{\"=\" * 60}')\n",
        "print('Paste this output back for review before running embeddings.')\n",
        "print('=' * 60)"
      ],
      "metadata": {
        "id": "t5B_sYMb6CSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 5b: Vector Embeddings\n",
        "\n",
        "Generate 3072-dimensional embeddings from structured athlete data + generated profile summaries. These power the AlloyDB pgvector similarity search in the lab."
      ],
      "metadata": {
        "id": "IBahhlxHIrHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 5, Step 8: Embedding functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def create_embedding_text(row, profile_summary: str) -> str:\n",
        "    \"\"\"Create rich text for embedding from structured data + profile.\"\"\"\n",
        "\n",
        "    name = str(row.get('name', 'Unknown')).replace('â€¢', ' ')\n",
        "    parts = [f\"Team USA {row.get('games_type', '')} athlete: {name}\"]\n",
        "\n",
        "    if pd.notna(row.get('primary_sport')):\n",
        "        parts.append(f\"Sport: {row['primary_sport']}\")\n",
        "    if pd.notna(row.get('classification_code')):\n",
        "        parts.append(f\"Classification: {row['classification_code']}\")\n",
        "    if pd.notna(row.get('games_count')) and row['games_count'] > 0:\n",
        "        year_str = \"\"\n",
        "        if pd.notna(row.get('first_games_year')) and pd.notna(row.get('last_games_year')):\n",
        "            fy, ly = int(row['first_games_year']), int(row['last_games_year'])\n",
        "            year_str = f\" ({fy}-{ly})\"\n",
        "        medal_str = \"\"\n",
        "        if pd.notna(row.get('total_medals')) and row['total_medals'] > 0:\n",
        "            medal_str = f\", {int(row['total_medals'])} medals\"\n",
        "        parts.append(f\"Career: {int(row['games_count'])} Games{year_str}{medal_str}\")\n",
        "\n",
        "    # Add profile summary if it's real (not an error message)\n",
        "    if profile_summary and not profile_summary.startswith('[Profile generation failed'):\n",
        "        parts.append(f\"Profile: {profile_summary}\")\n",
        "\n",
        "    return \". \".join(parts)\n",
        "\n",
        "\n",
        "def generate_embedding(text: str, retries: int = EmbeddingConfig.MAX_RETRIES) -> Tuple[Optional[List[float]], Optional[str]]:\n",
        "    \"\"\"Generate embedding for text.\"\"\"\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = get_client().models.embed_content(\n",
        "                model=EmbeddingConfig.MODEL_NAME,\n",
        "                contents=text,\n",
        "                config=types.EmbedContentConfig(\n",
        "                    output_dimensionality=EmbeddingConfig.OUTPUT_DIMENSION\n",
        "                )\n",
        "            )\n",
        "\n",
        "            if hasattr(response, 'embeddings') and response.embeddings:\n",
        "                embedding = response.embeddings[0]\n",
        "                if hasattr(embedding, 'values') and embedding.values:\n",
        "                    return list(embedding.values), None\n",
        "\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(EmbeddingConfig.RETRY_DELAY)\n",
        "                continue\n",
        "            return None, \"No embedding values in response\"\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(EmbeddingConfig.RETRY_DELAY * (attempt + 1))\n",
        "                continue\n",
        "            return None, f\"{type(e).__name__}: {str(e)}\"\n",
        "\n",
        "    return None, \"Max retries exceeded\"\n",
        "\n",
        "\n",
        "def process_single_embedding(item) -> dict:\n",
        "    \"\"\"Process a single athlete for embedding.\"\"\"\n",
        "    athlete_id, row, profile_summary = item\n",
        "\n",
        "    text = create_embedding_text(row, profile_summary)\n",
        "    embedding, error = generate_embedding(text)\n",
        "\n",
        "    if embedding:\n",
        "        return {'athlete_id': athlete_id, 'embedding': embedding, 'error': None}\n",
        "    else:\n",
        "        name = str(row.get('name', 'Unknown')).replace('â€¢', ' ')\n",
        "        log_error(athlete_id, name, error, EmbeddingConfig.ERROR_LOG)\n",
        "        return {'athlete_id': athlete_id, 'embedding': None, 'error': error}\n",
        "\n",
        "\n",
        "def save_embedding_progress(results, filename):\n",
        "    \"\"\"Save embedding progress to CSV.\"\"\"\n",
        "    df = pd.DataFrame([r for r in results if r['embedding'] is not None])\n",
        "    if len(df) > 0:\n",
        "        df['embedding'] = df['embedding'].apply(json.dumps)\n",
        "        df[['athlete_id', 'embedding']].to_csv(filename, index=False, encoding='utf-8')\n",
        "    print(f\"  ðŸ’¾ Progress: {filename} ({len(df):,} embeddings)\")\n",
        "\n",
        "print(\"âœ… Embedding functions defined\")"
      ],
      "metadata": {
        "id": "W8MVQ-CdIAzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 5, Step 9: Test single embedding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "test_row = team_usa_athletes.iloc[0]\n",
        "test_profile = df_profiles[df_profiles['athlete_id'] == test_row['athlete_id']]\n",
        "test_summary = test_profile.iloc[0]['profile_summary'] if len(test_profile) > 0 else \"\"\n",
        "\n",
        "text = create_embedding_text(test_row, test_summary)\n",
        "print(f\"Embedding text (first 300 chars):\\n{text[:300]}...\\n\")\n",
        "\n",
        "print(f\"Calling embedding API ({EmbeddingConfig.OUTPUT_DIMENSION} dims)...\")\n",
        "embedding, error = generate_embedding(text)\n",
        "\n",
        "if embedding:\n",
        "    print(f\"âœ… Success!\")\n",
        "    print(f\"   Dimensions: {len(embedding)}\")\n",
        "    print(f\"   Expected:   {EmbeddingConfig.OUTPUT_DIMENSION}\")\n",
        "    print(f\"   First 5:    {embedding[:5]}\")\n",
        "    print(f\"   âœ“ Dimension check: {'PASS' if len(embedding) == EmbeddingConfig.OUTPUT_DIMENSION else 'FAIL'}\")\n",
        "else:\n",
        "    print(f\"âŒ Error: {error}\")\n"
      ],
      "metadata": {
        "id": "N-zRvr9WIuVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 5, Step 10: Run embedding generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Build lookup: athlete_id â†’ profile_summary\n",
        "profile_lookup = dict(zip(df_profiles['athlete_id'], df_profiles['profile_summary']))\n",
        "\n",
        "def generate_all_embeddings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Generate embeddings for all athletes with parallel processing + resume.\"\"\"\n",
        "\n",
        "    results = []\n",
        "    errors = []\n",
        "    processed_ids = set()\n",
        "\n",
        "    # Resume from progress\n",
        "    progress_path = Path(EmbeddingConfig.PROGRESS_FILE)\n",
        "    if progress_path.exists():\n",
        "        print(f\"\\nðŸ“‚ Found progress file: {EmbeddingConfig.PROGRESS_FILE}\")\n",
        "        df_progress = pd.read_csv(EmbeddingConfig.PROGRESS_FILE)\n",
        "        df_progress['embedding'] = df_progress['embedding'].apply(\n",
        "            lambda x: json.loads(x) if isinstance(x, str) and x.startswith('[') else None)\n",
        "        successful = df_progress[df_progress['embedding'].notna()]\n",
        "        processed_ids = set(successful['athlete_id'])\n",
        "\n",
        "        for _, row in successful.iterrows():\n",
        "            results.append({\n",
        "                'athlete_id': row['athlete_id'],\n",
        "                'embedding': row['embedding'],\n",
        "                'error': None\n",
        "            })\n",
        "        print(f\"  âœ… Loaded {len(successful):,} embeddings\")\n",
        "\n",
        "    # Filter to unprocessed\n",
        "    df_todo = df[~df['athlete_id'].isin(processed_ids)].copy()\n",
        "\n",
        "    if len(df_todo) == 0:\n",
        "        print(f\"\\nâœ… All embeddings already generated!\")\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"Starting PARALLEL embedding generation\")\n",
        "    print(f\"  Total athletes:      {len(df):,}\")\n",
        "    print(f\"  Already processed:   {len(processed_ids):,}\")\n",
        "    print(f\"  To process:          {len(df_todo):,}\")\n",
        "    print(f\"  Workers:             {EmbeddingConfig.MAX_WORKERS}\")\n",
        "    print(f\"  Model:               {EmbeddingConfig.MODEL_NAME}\")\n",
        "    print(f\"  Dimensions:          {EmbeddingConfig.OUTPUT_DIMENSION}\")\n",
        "    print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Build items: (athlete_id, row, profile_summary)\n",
        "    items = []\n",
        "    for idx, row in df_todo.iterrows():\n",
        "        profile = profile_lookup.get(row['athlete_id'], '')\n",
        "        items.append((row['athlete_id'], row, profile))\n",
        "\n",
        "    total_batches = (len(items) + EmbeddingConfig.BATCH_SIZE - 1) // EmbeddingConfig.BATCH_SIZE\n",
        "\n",
        "    with tqdm(total=len(items), desc=\"Generating embeddings\") as pbar:\n",
        "        for batch_num in range(total_batches):\n",
        "            start_idx = batch_num * EmbeddingConfig.BATCH_SIZE\n",
        "            end_idx = min(start_idx + EmbeddingConfig.BATCH_SIZE, len(items))\n",
        "            batch = items[start_idx:end_idx]\n",
        "\n",
        "            with ThreadPoolExecutor(max_workers=EmbeddingConfig.MAX_WORKERS) as executor:\n",
        "                futures = {executor.submit(process_single_embedding, item): item for item in batch}\n",
        "\n",
        "                for future in as_completed(futures):\n",
        "                    result = future.result()\n",
        "                    results.append(result)\n",
        "                    if result['error']:\n",
        "                        errors.append(result['athlete_id'])\n",
        "                    pbar.update(1)\n",
        "\n",
        "            if len(results) % EmbeddingConfig.SAVE_INTERVAL == 0 or end_idx == len(items):\n",
        "                save_embedding_progress(results, EmbeddingConfig.PROGRESS_FILE)\n",
        "\n",
        "    save_embedding_progress(results, EmbeddingConfig.PROGRESS_FILE)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"Embedding Generation Complete!\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "    print(f\"  Newly processed: {len(df_todo):,}\")\n",
        "    print(f\"  Total embeddings: {len([r for r in results if r['embedding']]):,}\")\n",
        "    print(f\"  Errors:           {len(errors):,}\")\n",
        "    if len(df_todo) > 0:\n",
        "        print(f\"  Time:             {elapsed/60:.1f} minutes\")\n",
        "        print(f\"  Throughput:       {len(df_todo)/elapsed*60:.1f} athletes/minute\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run it\n",
        "df_embeddings = generate_all_embeddings(team_usa_athletes)"
      ],
      "metadata": {
        "id": "oreDwF2UIx6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 5, Step 11: Merge enrichments into athletes table â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Add profile summaries\n",
        "profile_map = dict(zip(df_profiles['athlete_id'], df_profiles['profile_summary']))\n",
        "team_usa_athletes['profile_summary'] = team_usa_athletes['athlete_id'].map(profile_map)\n",
        "\n",
        "# Add embeddings\n",
        "embedding_map = dict(zip(\n",
        "    df_embeddings[df_embeddings['embedding'].notna()]['athlete_id'],\n",
        "    df_embeddings[df_embeddings['embedding'].notna()]['embedding']\n",
        "))\n",
        "team_usa_athletes['embedding'] = team_usa_athletes['athlete_id'].map(embedding_map)\n",
        "\n",
        "print(f\"Enrichment merged:\")\n",
        "print(f\"  Profile summaries: {team_usa_athletes['profile_summary'].notna().sum():,} / {len(team_usa_athletes):,}\")\n",
        "print(f\"  Embeddings:        {team_usa_athletes['embedding'].notna().sum():,} / {len(team_usa_athletes):,}\")\n",
        "\n",
        "# Save enriched athletes (embeddings as JSON)\n",
        "enriched = team_usa_athletes.copy()\n",
        "enriched['embedding'] = enriched['embedding'].apply(\n",
        "    lambda x: json.dumps(x) if isinstance(x, list) else None)\n",
        "\n",
        "ath_path = '/tmp/team_usa_athletes_enriched.csv'\n",
        "enriched.to_csv(ath_path, index=False, encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "!gsutil cp {ath_path} gs://class-demo/team-usa/enriched/team_usa_athletes_enriched.csv\n",
        "\n",
        "# Also save results table (unchanged but checkpoint alongside)\n",
        "res_path = '/tmp/team_usa_results.csv'\n",
        "team_usa_results.to_csv(res_path, index=False)\n",
        "!gsutil cp {res_path} gs://class-demo/team-usa/enriched/team_usa_results.csv\n",
        "\n",
        "print(f\"\\nâœ… Enriched checkpoints saved to gs://class-demo/team-usa/enriched/\")"
      ],
      "metadata": {
        "id": "QTkKAx6ZI139"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ PHASE 5 QC REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print('=' * 70)\n",
        "print('PHASE 5 QC: ENRICHED ATHLETES')\n",
        "print('=' * 70)\n",
        "\n",
        "print(f'\\nðŸ“Š PROFILE SUMMARIES')\n",
        "has_profile = team_usa_athletes['profile_summary'].notna()\n",
        "has_real_profile = has_profile & ~team_usa_athletes['profile_summary'].str.contains(\n",
        "    r'\\[Profile generation failed', na=False, regex=True)\n",
        "print(f'  With profile:        {has_real_profile.sum():,} / {len(team_usa_athletes):,} ({has_real_profile.sum()/len(team_usa_athletes)*100:.1f}%)')\n",
        "print(f'  Failed:              {(has_profile & ~has_real_profile).sum():,}')\n",
        "\n",
        "if has_real_profile.sum() > 0:\n",
        "    lengths = team_usa_athletes.loc[has_real_profile, 'profile_summary'].str.len()\n",
        "    print(f'  Length â€” min: {lengths.min():,}, max: {lengths.max():,}, median: {lengths.median():,.0f}')\n",
        "\n",
        "print(f'\\nðŸ“Š EMBEDDINGS')\n",
        "has_emb = team_usa_athletes['embedding'].notna()\n",
        "print(f'  With embedding:      {has_emb.sum():,} / {len(team_usa_athletes):,} ({has_emb.sum()/len(team_usa_athletes)*100:.1f}%)')\n",
        "if has_emb.sum() > 0:\n",
        "    sample_emb = team_usa_athletes.loc[has_emb, 'embedding'].iloc[0]\n",
        "    dim = len(sample_emb) if isinstance(sample_emb, list) else 'N/A'\n",
        "    print(f'  Dimensions:          {dim}')\n",
        "    print(f'  Expected:            {EmbeddingConfig.OUTPUT_DIMENSION}')\n",
        "\n",
        "print(f'\\nðŸ“¦ COVERAGE BY TYPE')\n",
        "for gt in ['Olympic', 'Paralympic']:\n",
        "    sub = team_usa_athletes[team_usa_athletes['games_type'] == gt]\n",
        "    prof = (sub['profile_summary'].notna() & ~sub['profile_summary'].str.contains(\n",
        "        r'\\[Profile generation failed', na=False, regex=True)).sum()\n",
        "    emb = sub['embedding'].notna().sum()\n",
        "    print(f'  {gt:15s} profiles: {prof:>6,}/{len(sub):>6,}  embeddings: {emb:>6,}/{len(sub):>6,}')\n",
        "\n",
        "print(f'\\nðŸ“¦ COVERAGE BY SOURCE')\n",
        "for src in team_usa_athletes['source'].unique():\n",
        "    sub = team_usa_athletes[team_usa_athletes['source'] == src]\n",
        "    prof = (sub['profile_summary'].notna() & ~sub['profile_summary'].str.contains(\n",
        "        r'\\[Profile generation failed', na=False, regex=True)).sum()\n",
        "    emb = sub['embedding'].notna().sum()\n",
        "    print(f'  {src:15s} profiles: {prof:>6,}/{len(sub):>6,}  embeddings: {emb:>6,}/{len(sub):>6,}')\n",
        "\n",
        "print(f'\\nðŸ“‹ SAMPLE ENRICHED ATHLETES:')\n",
        "for gt in ['Olympic', 'Paralympic']:\n",
        "    sub = team_usa_athletes[(team_usa_athletes['games_type'] == gt) & has_real_profile]\n",
        "    if len(sub) > 0:\n",
        "        s = sub.sample(1, random_state=99).iloc[0]\n",
        "        print(f'\\n  --- {gt}: {s[\"name\"]} ({s.get(\"primary_sport\", \"N/A\")}) ---')\n",
        "        print(f'  Classification: {s.get(\"classification_code\", \"N/A\")}')\n",
        "        print(f'  Profile: {str(s[\"profile_summary\"])[:300]}...')\n",
        "        emb = s.get('embedding')\n",
        "        if isinstance(emb, list):\n",
        "            print(f'  Embedding: [{emb[0]:.6f}, {emb[1]:.6f}, ... ] ({len(emb)} dims)')\n",
        "\n",
        "print(f'\\nðŸ“Š FINAL TABLE SHAPE')\n",
        "print(f'  Athletes: {len(team_usa_athletes):,} rows Ã— {len(team_usa_athletes.columns)} columns')\n",
        "print(f'  Results:  {len(team_usa_results):,} rows Ã— {len(team_usa_results.columns)} columns')\n",
        "print(f'  Columns:  {list(team_usa_athletes.columns)}')\n",
        "\n",
        "print(f'\\nâœ… FILES ON GCS:')\n",
        "print(f'  gs://class-demo/team-usa/enriched/team_usa_athletes_enriched.csv')\n",
        "print(f'  gs://class-demo/team-usa/enriched/team_usa_results.csv')\n",
        "\n",
        "print(f'\\n{\"=\" * 70}')\n",
        "print('Paste this output back for review before proceeding to Phase 6.')\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "id": "u0u5oDDWRmU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Phase 6: Final Validation & Data Card\n",
        "\n",
        "Sanity queries that preview what students will run in BigQuery, a vector similarity smoke test, and the dataset's official data card."
      ],
      "metadata": {
        "id": "KGFfxU0WU_lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6, Step 1: Dataset overview queries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import numpy as np\n",
        "\n",
        "print('=' * 70)\n",
        "print('DATASET OVERVIEW')\n",
        "print('=' * 70)\n",
        "\n",
        "# Temporal span\n",
        "print('\\nðŸ“… TEMPORAL COVERAGE')\n",
        "for gt in ['Olympic', 'Paralympic']:\n",
        "    sub = team_usa_athletes[team_usa_athletes['games_type'] == gt]\n",
        "    first = sub['first_games_year'].dropna()\n",
        "    last = sub['last_games_year'].dropna()\n",
        "    if len(first) > 0:\n",
        "        print(f'  {gt:15s} {int(first.min())}â€“{int(last.max())}')\n",
        "\n",
        "# Gender breakdown\n",
        "print('\\nðŸ‘¥ GENDER DISTRIBUTION')\n",
        "gender_by_type = team_usa_athletes.groupby(['games_type', 'gender']).size().unstack(fill_value=0)\n",
        "print(gender_by_type.to_string())\n",
        "\n",
        "# Sport diversity\n",
        "print('\\nðŸ… SPORT DIVERSITY')\n",
        "for gt in ['Olympic', 'Paralympic']:\n",
        "    sub = team_usa_athletes[team_usa_athletes['games_type'] == gt]\n",
        "    n_sports = sub['primary_sport'].nunique()\n",
        "    top_3 = sub['primary_sport'].value_counts().head(3)\n",
        "    print(f'  {gt}: {n_sports} sports â€” top 3: {\", \".join(f\"{s} ({c})\" for s, c in top_3.items())}')\n",
        "\n",
        "# Medal leaders\n",
        "print('\\nðŸ¥‡ TOP 10 MEDALISTS (ALL-TIME)')\n",
        "top = team_usa_athletes.nlargest(10, 'total_medals')[\n",
        "    ['name', 'games_type', 'primary_sport', 'gold_count', 'total_medals', 'games_count']\n",
        "].reset_index(drop=True)\n",
        "top.index = top.index + 1\n",
        "print(top.to_string())\n",
        "\n",
        "# Multi-Games athletes\n",
        "print('\\nðŸ” MULTI-GAMES ATHLETES')\n",
        "for gt in ['Olympic', 'Paralympic']:\n",
        "    sub = team_usa_athletes[(team_usa_athletes['games_type'] == gt) & (team_usa_athletes['games_count'] >= 2)]\n",
        "    multi4 = (team_usa_athletes['games_type'] == gt) & (team_usa_athletes['games_count'] >= 4)\n",
        "    print(f'  {gt:15s} 2+ Games: {len(sub):,}   4+ Games: {multi4.sum():,}')"
      ],
      "metadata": {
        "id": "pXuqUmGsR6nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6, Step 2: Paralympic classification analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "print('=' * 70)\n",
        "print('PARALYMPIC CLASSIFICATION ANALYSIS')\n",
        "print('=' * 70)\n",
        "\n",
        "para = team_usa_athletes[team_usa_athletes['games_type'] == 'Paralympic']\n",
        "\n",
        "print(f'\\nðŸ“Š COVERAGE')\n",
        "has_class = para['classification_code'].notna()\n",
        "print(f'  Athletes with classification: {has_class.sum():,} / {len(para):,} ({has_class.sum()/len(para)*100:.1f}%)')\n",
        "\n",
        "# Top classification codes\n",
        "print(f'\\nðŸ“‹ TOP 15 CLASSIFICATION CODES')\n",
        "class_counts = para['classification_code'].value_counts().head(15)\n",
        "for code, count in class_counts.items():\n",
        "    print(f'  {str(code):20s} {count:>4} athletes')\n",
        "\n",
        "# Classification by sport\n",
        "print(f'\\nðŸŠ CLASSIFICATION CODES BY SPORT (top 5 sports)')\n",
        "para_with_class = para[has_class]\n",
        "top_sports = para_with_class['primary_sport'].value_counts().head(5)\n",
        "for sport in top_sports.index:\n",
        "    sub = para_with_class[para_with_class['primary_sport'] == sport]\n",
        "    codes = sub['classification_code'].value_counts().head(5)\n",
        "    code_str = ', '.join(f'{c}({n})' for c, n in codes.items())\n",
        "    print(f'  {sport:20s} [{len(sub)} athletes] {code_str}')\n",
        "\n",
        "# Classification gap analysis\n",
        "print(f'\\nâš ï¸ MISSING CLASSIFICATION')\n",
        "missing_class = para[~has_class]\n",
        "by_source = missing_class['source'].value_counts()\n",
        "for src, count in by_source.items():\n",
        "    print(f'  {src:15s} {count:>4} athletes without classification')\n",
        "decades = missing_class['first_games_year'].dropna().apply(lambda y: f\"{int(y)//10*10}s\")\n",
        "if len(decades) > 0:\n",
        "    print(f'  Decades: {decades.value_counts().sort_index().to_dict()}')"
      ],
      "metadata": {
        "id": "gCv5Ya38VE3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6, Step 3: Vector similarity smoke test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
        "    a, b = np.array(a), np.array(b)\n",
        "    return np.dot(a, b) / (norm(a) * norm(b))\n",
        "\n",
        "# Get athletes with embeddings\n",
        "has_emb = team_usa_athletes[team_usa_athletes['embedding'].apply(lambda x: isinstance(x, list))]\n",
        "\n",
        "print('=' * 70)\n",
        "print('VECTOR SIMILARITY SMOKE TEST')\n",
        "print('=' * 70)\n",
        "print(f'Athletes with valid embeddings: {len(has_emb):,}\\n')\n",
        "\n",
        "# Test 1: Find athletes similar to a well-known swimmer\n",
        "query_name = 'Swimming'\n",
        "swimmers = has_emb[has_emb['primary_sport'].str.contains(query_name, na=False, case=False)]\n",
        "if len(swimmers) >= 2:\n",
        "    # Pick the swimmer with the most medals as anchor\n",
        "    anchor = swimmers.nlargest(1, 'total_medals').iloc[0]\n",
        "    anchor_emb = anchor['embedding']\n",
        "\n",
        "    print(f'ðŸŠ TEST 1: Athletes similar to {anchor[\"name\"]} ({anchor[\"games_type\"]}, {int(anchor.get(\"total_medals\", 0))} medals)')\n",
        "\n",
        "    # Compare against all athletes\n",
        "    sims = []\n",
        "    for _, row in has_emb.iterrows():\n",
        "        if row['athlete_id'] == anchor['athlete_id']:\n",
        "            continue\n",
        "        sim = cosine_similarity(anchor_emb, row['embedding'])\n",
        "        sims.append({\n",
        "            'name': row['name'], 'sport': row.get('primary_sport', 'N/A'),\n",
        "            'type': row['games_type'], 'medals': int(row.get('total_medals', 0) if pd.notna(row.get('total_medals')) else 0),\n",
        "            'classification': row.get('classification_code', None),\n",
        "            'similarity': sim\n",
        "        })\n",
        "\n",
        "    sims_df = pd.DataFrame(sims).nlargest(10, 'similarity')\n",
        "    print(f'\\n  Top 10 most similar:')\n",
        "    for i, (_, r) in enumerate(sims_df.iterrows(), 1):\n",
        "        cls = f' [{r[\"classification\"]}]' if pd.notna(r.get('classification')) else ''\n",
        "        print(f'  {i:>2}. {r[\"similarity\"]:.4f}  {r[\"name\"]:30s} {r[\"sport\"]:20s} {r[\"type\"]}{cls}  ({r[\"medals\"]} medals)')\n",
        "\n",
        "print()\n",
        "\n",
        "# Test 2: Cross-type similarity â€” find Paralympic athletes similar to an Olympic athlete\n",
        "print(f'ðŸ¤ TEST 2: Paralympic athletes most similar to {anchor[\"name\"]}')\n",
        "para_sims = [s for s in sims if s['type'] == 'Paralympic']\n",
        "para_top = sorted(para_sims, key=lambda x: x['similarity'], reverse=True)[:5]\n",
        "for i, r in enumerate(para_top, 1):\n",
        "    cls = f' [{r[\"classification\"]}]' if pd.notna(r.get('classification')) else ''\n",
        "    print(f'  {i}. {r[\"similarity\"]:.4f}  {r[\"name\"]:30s} {r[\"sport\"]:20s}{cls}  ({r[\"medals\"]} medals)')\n",
        "\n",
        "print()\n",
        "\n",
        "# Test 3: Sanity check â€” dissimilar sports should have lower similarity\n",
        "print(f'ðŸŽ¯ TEST 3: Similarity sanity check (same sport vs different)')\n",
        "random_other = has_emb[~has_emb['primary_sport'].str.contains(query_name, na=False, case=False)].sample(50, random_state=42)\n",
        "same_sport_sims = [s['similarity'] for s in sims if query_name.lower() in str(s.get('sport', '')).lower()][:50]\n",
        "diff_sport_sims = [cosine_similarity(anchor_emb, row['embedding']) for _, row in random_other.iterrows()]\n",
        "\n",
        "print(f'  Same sport avg similarity:  {np.mean(same_sport_sims):.4f} (n={len(same_sport_sims)})')\n",
        "print(f'  Diff sport avg similarity:  {np.mean(diff_sport_sims):.4f} (n={len(diff_sport_sims)})')\n",
        "print(f'  Delta:                      {np.mean(same_sport_sims) - np.mean(diff_sport_sims):.4f}')\n",
        "print(f'  âœ“ Embeddings discriminate by sport: {\"PASS âœ…\" if np.mean(same_sport_sims) > np.mean(diff_sport_sims) else \"FAIL âŒ\"}')"
      ],
      "metadata": {
        "id": "K1TV2MU7VIA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6, Step 4: Column cleanup + schema freeze â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Define final column order for downstream consumers\n",
        "ATHLETE_COLUMNS = [\n",
        "    # Identity\n",
        "    'athlete_id', 'name', 'gender', 'birth_date', 'birth_place', 'birth_country',\n",
        "    # Physical\n",
        "    'height_cm', 'weight_kg',\n",
        "    # Competition\n",
        "    'games_type', 'primary_sport', 'classification_code',\n",
        "    # Career stats\n",
        "    'first_games_year', 'last_games_year', 'games_count',\n",
        "    'gold_count', 'silver_count', 'bronze_count', 'total_medals',\n",
        "    # Bio fields\n",
        "    'reason', 'hero', 'philosophy', 'other_sports', 'coach',\n",
        "    'hobbies', 'occupation', 'education',\n",
        "    # Enrichment\n",
        "    'profile_summary', 'embedding',\n",
        "    # Metadata\n",
        "    'source',\n",
        "]\n",
        "\n",
        "RESULT_COLUMNS = [\n",
        "    'athlete_id', 'athlete_name', 'games_year', 'games_season',\n",
        "    'games_type', 'sport', 'discipline', 'event', 'medal',\n",
        "    'classification_code',\n",
        "]\n",
        "\n",
        "# Apply column order (drop any extras, keep only what's defined)\n",
        "athletes_final = team_usa_athletes[[c for c in ATHLETE_COLUMNS if c in team_usa_athletes.columns]].copy()\n",
        "results_final = team_usa_results[[c for c in RESULT_COLUMNS if c in team_usa_results.columns]].copy()\n",
        "\n",
        "# Check for missing expected columns\n",
        "missing_ath = [c for c in ATHLETE_COLUMNS if c not in team_usa_athletes.columns]\n",
        "missing_res = [c for c in RESULT_COLUMNS if c not in team_usa_results.columns]\n",
        "if missing_ath:\n",
        "    print(f'âš ï¸ Athletes missing columns: {missing_ath}')\n",
        "if missing_res:\n",
        "    print(f'âš ï¸ Results missing columns: {missing_res}')\n",
        "\n",
        "print(f'Athletes schema: {len(athletes_final.columns)} columns')\n",
        "print(f'Results schema:  {len(results_final.columns)} columns')\n",
        "print(f'\\nAthletes columns:\\n  {list(athletes_final.columns)}')\n",
        "print(f'\\nResults columns:\\n  {list(results_final.columns)}')"
      ],
      "metadata": {
        "id": "95ZhpypdVMUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6, Step 5: Dataset Data Card â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "print('â•”' + 'â•' * 68 + 'â•—')\n",
        "print('â•‘' + ' TEAM USA OLYMPIC & PARALYMPIC ATHLETES â€” DATA CARD'.center(68) + 'â•‘')\n",
        "print('â•š' + 'â•' * 68 + 'â•')\n",
        "\n",
        "print(f\"\"\"\n",
        "ðŸ“‹ DATASET SUMMARY\n",
        "  Name:               Team USA Olympic & Paralympic Athletes\n",
        "  Version:            1.0\n",
        "  Created:            {datetime.now().strftime('%Y-%m-%d')}\n",
        "  Pipeline:           Colab Enterprise notebook (6 phases)\n",
        "\n",
        "ðŸ“Š TABLES\n",
        "  Athletes:           {len(athletes_final):,} rows Ã— {len(athletes_final.columns)} columns\n",
        "  Results:            {len(results_final):,} rows Ã— {len(results_final.columns)} columns\n",
        "\n",
        "ðŸ‘¥ ATHLETE COVERAGE\n",
        "  Olympic:            {len(athletes_final[athletes_final['games_type']=='Olympic']):,} athletes ({athletes_final[athletes_final['games_type']=='Olympic']['first_games_year'].dropna().min():.0f}â€“{athletes_final[athletes_final['games_type']=='Olympic']['last_games_year'].dropna().max():.0f})\n",
        "  Paralympic:         {len(athletes_final[athletes_final['games_type']=='Paralympic']):,} athletes ({athletes_final[athletes_final['games_type']=='Paralympic']['first_games_year'].dropna().min():.0f}â€“{athletes_final[athletes_final['games_type']=='Paralympic']['last_games_year'].dropna().max():.0f})\n",
        "\"\"\")\n",
        "\n",
        "# Compute stats\n",
        "oly = athletes_final[athletes_final['games_type'] == 'Olympic']\n",
        "para = athletes_final[athletes_final['games_type'] == 'Paralympic']\n",
        "\n",
        "has_height = athletes_final['height_cm'].notna().sum()\n",
        "has_weight = athletes_final['weight_kg'].notna().sum()\n",
        "has_class = athletes_final['classification_code'].notna().sum()\n",
        "has_profile = athletes_final['profile_summary'].notna().sum()\n",
        "has_emb = athletes_final['embedding'].apply(lambda x: isinstance(x, list)).sum()\n",
        "total_medals = athletes_final['total_medals'].sum()\n",
        "medalists = (athletes_final['total_medals'] > 0).sum()\n",
        "n_sports = athletes_final['primary_sport'].nunique()\n",
        "\n",
        "print(f\"\"\"ðŸ“ˆ FEATURE COVERAGE\n",
        "  Height:             {has_height:,} / {len(athletes_final):,} ({has_height/len(athletes_final)*100:.0f}%) â€” Olympic only\n",
        "  Weight:             {has_weight:,} / {len(athletes_final):,} ({has_weight/len(athletes_final)*100:.0f}%) â€” Olympic only\n",
        "  Classification:     {has_class:,} / {len(para):,} Paralympic ({has_class/len(para)*100:.0f}%)\n",
        "  Profile summary:    {has_profile:,} / {len(athletes_final):,} ({has_profile/len(athletes_final)*100:.0f}%)\n",
        "  Embedding (3072d):  {has_emb:,} / {len(athletes_final):,} ({has_emb/len(athletes_final)*100:.0f}%)\n",
        "\n",
        "ðŸ… COMPETITION STATS\n",
        "  Total medals:       {int(total_medals):,} across {medalists:,} medalists\n",
        "  Sports:             {n_sports} unique\n",
        "  Results records:    {len(results_final):,} (individual event participations/medals)\n",
        "\n",
        "ðŸ“¦ DATA SOURCES\n",
        "  keithgalli          Olympic bios + results (1896â€“2022)\n",
        "  paris2024           Olympic + Paralympic athletes (2024)\n",
        "  piterfm             Paralympic Tokyo 2020 + Paris 2024\n",
        "  katiepress          Paralympic medal records (1960â€“2018)\n",
        "\n",
        "ðŸ¤– AI ENRICHMENT\n",
        "  Model (summaries):  Gemini 2.5 Flash + Google Search grounding\n",
        "  Model (embeddings): gemini-embedding-001 (3072 dimensions)\n",
        "  Profiles:           2-paragraph athlete bios, 150â€“250 words\n",
        "\n",
        "ðŸ“ FILES ON GCS\n",
        "  gs://class-demo/team-usa/final/team_usa_athletes.csv\n",
        "  gs://class-demo/team-usa/final/team_usa_results.csv\n",
        "\n",
        "ðŸŽ¯ INTENDED USE\n",
        "  Lab: \"Build an AI Sports Analytics Agent with Team USA\"\n",
        "  BigQuery:     Clustering, BQML, analytical queries\n",
        "  AlloyDB:      pgvector similarity search (3072-dim)\n",
        "  ADK Agent:    Conversational sports analytics with tool use\n",
        "\n",
        "âš ï¸ KNOWN LIMITATIONS\n",
        "  - Paralympic physical attributes (height/weight) effectively zero\n",
        "  - Classification coverage 47% for Paralympic (pre-1990s gap)\n",
        "  - Katiepress names abbreviated (LAST I format), no cross-matching\n",
        "  - 119 athletes (~1%) with unknown gender\n",
        "  - Beijing 2022 Winter Paralympics not covered (data gap)\n",
        "\"\"\")\n",
        "\n",
        "print('â•' * 70)\n",
        "print('NOTEBOOK COMPLETE â€” Ready for BigQuery + AlloyDB load')\n",
        "print('â•' * 70)"
      ],
      "metadata": {
        "id": "ek60nK0Add0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Phase 6.5: Name Cleanup\n",
        "\n",
        "Fix three issues before students see the data:\n",
        "1. **Bullet separators** (`â€¢`) â†’ spaces (keithgalli source, ~10K athletes)\n",
        "2. **ALL CAPS abbreviated** (`ZORN T`) â†’ full proper names via Gemini extraction from existing profiles (katiepress, ~1,165 athletes)\n",
        "3. **Inconsistent casing** â†’ standardized title case across all sources"
      ],
      "metadata": {
        "id": "2ySmkKmfVfzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6.5, Step 0: Load enriched data from GCS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import json\n",
        "\n",
        "# Load the enriched athletes with profiles and embeddings\n",
        "!gsutil cp gs://class-demo/team-usa/enriched/team_usa_athletes_enriched.csv /tmp/\n",
        "!gsutil cp gs://class-demo/team-usa/enriched/team_usa_results.csv /tmp/\n",
        "\n",
        "team_usa_athletes = pd.read_csv('/tmp/team_usa_athletes_enriched.csv')\n",
        "team_usa_results = pd.read_csv('/tmp/team_usa_results.csv')\n",
        "\n",
        "# Parse embeddings back from JSON strings to lists\n",
        "team_usa_athletes['embedding'] = team_usa_athletes['embedding'].apply(\n",
        "    lambda x: json.loads(x) if isinstance(x, str) and x.startswith('[') else None)\n",
        "\n",
        "print(f'Loaded from GCS:')\n",
        "print(f'  Athletes: {len(team_usa_athletes):,} rows')\n",
        "print(f'  Results:  {len(team_usa_results):,} rows')\n",
        "print(f'  Profiles: {team_usa_athletes[\"profile_summary\"].notna().sum():,}')\n",
        "print(f'  Embeddings: {team_usa_athletes[\"embedding\"].apply(lambda x: isinstance(x, list)).sum():,}')"
      ],
      "metadata": {
        "id": "_MDaSEBmWyyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6.5, Step 1: Name cleanup utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def smart_title_case(name):\n",
        "    \"\"\"Title case with handling for Mc/O' prefixes and suffixes.\"\"\"\n",
        "    if not name or not isinstance(name, str):\n",
        "        return name\n",
        "\n",
        "    result = name.lower().title()\n",
        "\n",
        "    # Fix Mc prefix: Mcdonald â†’ McDonald, Mcconnell â†’ McConnell\n",
        "    result = re.sub(r'\\bMc([a-z])', lambda m: 'Mc' + m.group(1).upper(), result)\n",
        "\n",
        "    # Fix O' prefix (usually handled by title() but ensure it)\n",
        "    result = re.sub(r\"\\bO'([a-z])\", lambda m: \"O'\" + m.group(1).upper(), result)\n",
        "\n",
        "    # Fix Roman numeral suffixes: Ii â†’ II, Iii â†’ III, Iv â†’ IV\n",
        "    result = re.sub(r'\\bIi\\b', 'II', result)\n",
        "    result = re.sub(r'\\bIii\\b', 'III', result)\n",
        "    result = re.sub(r'\\bIv\\b', 'IV', result)\n",
        "\n",
        "    return result.strip()\n",
        "\n",
        "\n",
        "def clean_name_mechanical(name):\n",
        "    \"\"\"Mechanical cleanup: bullet separators â†’ spaces, then smart title case.\"\"\"\n",
        "    if not name or not isinstance(name, str):\n",
        "        return name\n",
        "\n",
        "    # Replace bullet separator with space\n",
        "    name = name.replace('â€¢', ' ')\n",
        "\n",
        "    # Collapse multiple spaces\n",
        "    name = re.sub(r'\\s+', ' ', name).strip()\n",
        "\n",
        "    # Apply smart title case\n",
        "    return smart_title_case(name)\n",
        "\n",
        "\n",
        "# Apply mechanical cleanup to ALL athletes first\n",
        "team_usa_athletes['name_original'] = team_usa_athletes['name']\n",
        "team_usa_athletes['name'] = team_usa_athletes['name'].apply(clean_name_mechanical)\n",
        "\n",
        "# Preview\n",
        "print('Mechanical cleanup applied to all 12,222 athletes\\n')\n",
        "print('Sample keithgalli fixes:')\n",
        "kg = team_usa_athletes[team_usa_athletes['source'] == 'keithgalli'].sample(5, random_state=42)\n",
        "for _, r in kg.iterrows():\n",
        "    if r['name'] != r['name_original']:\n",
        "        print(f'  {r[\"name_original\"]:40s} â†’ {r[\"name\"]}')\n",
        "\n",
        "print('\\nSample katiepress (still abbreviated, will fix next):')\n",
        "kp = team_usa_athletes[team_usa_athletes['source'] == 'katiepress'].sample(5, random_state=42)\n",
        "for _, r in kp.iterrows():\n",
        "    print(f'  {r[\"name_original\"]:40s} â†’ {r[\"name\"]}')"
      ],
      "metadata": {
        "id": "2ovT4Ch6dhWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6.5, Step 1b: Fix name order for paris2024/piterfm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def flip_name_order(name):\n",
        "    \"\"\"Flip 'Last First' to 'First Last' for 2-word names.\"\"\"\n",
        "    if not name or not isinstance(name, str):\n",
        "        return name\n",
        "    words = name.split()\n",
        "    if len(words) == 2:\n",
        "        return f\"{words[1]} {words[0]}\"\n",
        "    return name  # Don't flip 3+ word names, too risky\n",
        "\n",
        "# Apply to paris2024 and piterfm sources only\n",
        "mask = team_usa_athletes['source'].isin(['paris2024', 'piterfm'])\n",
        "team_usa_athletes.loc[mask, 'name'] = team_usa_athletes.loc[mask, 'name'].apply(flip_name_order)\n",
        "\n",
        "# Verify\n",
        "print('Name order fixed for paris2024 and piterfm:')\n",
        "for src in ['paris2024', 'piterfm']:\n",
        "    samples = team_usa_athletes[team_usa_athletes['source'] == src].sample(3, random_state=42)\n",
        "    names = ', '.join(samples['name'].tolist())\n",
        "    print(f'  {src:15s} {names}')"
      ],
      "metadata": {
        "id": "OyFO02x-iyH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6.5, Step 2: Extract full names from profiles (katiepress) â”€â”€\n",
        "\n",
        "NAME_PROGRESS_FILE = '/tmp/katiepress_name_extraction.csv'\n",
        "\n",
        "def is_abbreviated(name):\n",
        "    \"\"\"Check if name is 'LastName I' format or single word.\"\"\"\n",
        "    if not name or not isinstance(name, str):\n",
        "        return False\n",
        "    words = name.split()\n",
        "    if len(words) == 1:\n",
        "        return True\n",
        "    if len(words) == 2 and len(words[1]) == 1:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def extract_name(athlete_id, original_name, profile, retries=3):\n",
        "    \"\"\"Extract full name from profile text.\"\"\"\n",
        "\n",
        "    if not profile or profile.startswith('[Profile generation failed'):\n",
        "        return None, \"No profile\"\n",
        "\n",
        "    prompt = f\"\"\"Extract the athlete's full name from this profile.\n",
        "\n",
        "Database name: \"{original_name}\"\n",
        "\n",
        "Profile:\n",
        "{str(profile)[:1500]}\n",
        "\n",
        "Return ONLY the full name as it appears in the profile, or \"NOT_FOUND\".\"\"\"\n",
        "\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature=0.0,\n",
        "        max_output_tokens=256,\n",
        "    )\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = get_client().models.generate_content(\n",
        "                model=SummaryConfig.MODEL_NAME,\n",
        "                contents=prompt,\n",
        "                config=config,\n",
        "            )\n",
        "\n",
        "            extracted = response.text.strip() if response.text else None\n",
        "\n",
        "            if not extracted or 'NOT_FOUND' in extracted.upper():\n",
        "                return None, \"Not found in profile\"\n",
        "\n",
        "            # Clean up\n",
        "            extracted = extracted.strip('\"\\'.,')\n",
        "            words = extracted.split()\n",
        "\n",
        "            # Validate: 2+ words, no single-letter words (truncation)\n",
        "            if len(words) >= 2 and all(len(w) >= 2 for w in words):\n",
        "                # Verify it actually appears in profile\n",
        "                profile_lower = profile.lower()\n",
        "                name_lower = extracted.lower()\n",
        "                if name_lower in profile_lower:\n",
        "                    return smart_title_case(extracted), None\n",
        "                # Try first + last only\n",
        "                simple = f\"{words[0]} {words[-1]}\"\n",
        "                if simple.lower() in profile_lower:\n",
        "                    return smart_title_case(simple), None\n",
        "                # Both parts appear separately\n",
        "                if words[0].lower() in profile_lower and words[-1].lower() in profile_lower:\n",
        "                    return smart_title_case(extracted), None\n",
        "                return None, f\"Not validated in profile: {extracted}\"\n",
        "\n",
        "            return None, f\"Invalid format: {extracted}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            if '429' in str(e) or 'RESOURCE_EXHAUSTED' in str(e):\n",
        "                time.sleep(5 * (attempt + 1))\n",
        "                if attempt == retries - 1:\n",
        "                    return None, \"Rate limited\"\n",
        "                continue\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(2)\n",
        "                continue\n",
        "            return None, str(type(e).__name__)\n",
        "\n",
        "    return None, \"Max retries\"\n",
        "\n",
        "\n",
        "# Load previous progress\n",
        "previous = {}\n",
        "if Path(NAME_PROGRESS_FILE).exists():\n",
        "    df_prev = pd.read_csv(NAME_PROGRESS_FILE)\n",
        "    for _, row in df_prev.iterrows():\n",
        "        if pd.notna(row.get('clean_name')) and row['clean_name'] != row['original']:\n",
        "            previous[row['athlete_id']] = row['clean_name']\n",
        "            team_usa_athletes.loc[team_usa_athletes['athlete_id'] == row['athlete_id'], 'name'] = row['clean_name']\n",
        "\n",
        "print(f'Loaded {len(previous)} successful extractions from previous run')\n",
        "\n",
        "# Find remaining abbreviated names\n",
        "katiepress = team_usa_athletes[team_usa_athletes['source'] == 'katiepress']\n",
        "abbreviated = katiepress[katiepress['name'].apply(is_abbreviated)].copy()\n",
        "\n",
        "print(f'Found {len(abbreviated)} remaining abbreviated names')\n",
        "\n",
        "if len(abbreviated) == 0:\n",
        "    print('âœ… All katiepress names are now complete!')\n",
        "else:\n",
        "    print(f'Sample: {abbreviated[\"name\"].head(10).tolist()}\\n')\n",
        "\n",
        "    results = []\n",
        "    successful_count = 0\n",
        "\n",
        "    with tqdm(total=len(abbreviated), desc=\"Extracting names\") as pbar:\n",
        "        with ThreadPoolExecutor(max_workers=15) as executor:\n",
        "            futures = {}\n",
        "            for _, row in abbreviated.iterrows():\n",
        "                future = executor.submit(\n",
        "                    extract_name,\n",
        "                    row['athlete_id'],\n",
        "                    row['name_original'],\n",
        "                    row.get('profile_summary', '')\n",
        "                )\n",
        "                futures[future] = row\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                row = futures[future]\n",
        "                extracted, error = future.result()\n",
        "\n",
        "                result = {\n",
        "                    'athlete_id': row['athlete_id'],\n",
        "                    'original': row['name'],\n",
        "                    'clean_name': extracted if extracted else row['name'],\n",
        "                    'error': error\n",
        "                }\n",
        "                results.append(result)\n",
        "\n",
        "                if extracted:\n",
        "                    team_usa_athletes.loc[\n",
        "                        team_usa_athletes['athlete_id'] == row['athlete_id'], 'name'] = extracted\n",
        "                    successful_count += 1\n",
        "\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix({'found': successful_count})\n",
        "\n",
        "    # Save progress\n",
        "    all_results = results + [{'athlete_id': aid, 'original': '', 'clean_name': name, 'error': None}\n",
        "                              for aid, name in previous.items()]\n",
        "    pd.DataFrame(all_results).to_csv(NAME_PROGRESS_FILE, index=False)\n",
        "\n",
        "    # Stats\n",
        "    print(f'\\n{\"=\" * 60}')\n",
        "    print(f'ðŸ“Š EXTRACTION RESULTS')\n",
        "    print(f'  Processed:  {len(results)}')\n",
        "    print(f'  Found:      {successful_count} ({successful_count/len(results)*100:.1f}%)')\n",
        "    print(f'  Remaining:  {len(results) - successful_count}')\n",
        "\n",
        "    successes = [r for r in results if r['clean_name'] != r['original']]\n",
        "    if successes:\n",
        "        print(f'\\nâœ… SUCCESSFUL EXTRACTIONS')\n",
        "        for r in successes[:20]:\n",
        "            print(f'  {r[\"original\"]:25s} â†’ {r[\"clean_name\"]}')\n",
        "\n",
        "    failed = [r for r in results if r['clean_name'] == r['original']]\n",
        "    if failed:\n",
        "        error_types = pd.Series([r['error'] for r in failed]).value_counts()\n",
        "        print(f'\\nâš ï¸ FAILURE REASONS')\n",
        "        for err, count in error_types.items():\n",
        "            print(f'  {err}: {count}')\n",
        "        print(f'\\n  Examples:')\n",
        "        for r in failed[:5]:\n",
        "            print(f'  {r[\"original\"]:25s} ({r[\"error\"]})')\n",
        "\n",
        "    # Final check\n",
        "    still_abbrev = team_usa_athletes[\n",
        "        (team_usa_athletes['source'] == 'katiepress') &\n",
        "        (team_usa_athletes['name'].apply(is_abbreviated))\n",
        "    ]\n",
        "    print(f'\\nðŸ“‹ FINAL: {len(still_abbrev)} still abbreviated out of {len(katiepress)}')"
      ],
      "metadata": {
        "id": "kqFN-ZNMVi1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6.5, Step 2c: Rescue names from mismatch errors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Reload the search results\n",
        "df_search = pd.read_csv(SEARCH_PROGRESS_FILE)\n",
        "\n",
        "# Find \"last name mismatch\" and \"invalid\" entries\n",
        "rescuable = df_search[df_search['error'].str.contains('Last name mismatch|Invalid', na=False)]\n",
        "print(f'Attempting to rescue {len(rescuable)} names from failed extractions\\n')\n",
        "\n",
        "rescued = 0\n",
        "for _, row in rescuable.iterrows():\n",
        "    error = row['error']\n",
        "    original = row['original']\n",
        "\n",
        "    # Extract the name from the error message\n",
        "    raw = error.replace('Last name mismatch: ', '').replace('Invalid: ', '')\n",
        "\n",
        "    # Aggressive cleanup\n",
        "    raw = re.sub(r'\\[cite.*', '', raw)           # Remove citation artifacts\n",
        "    raw = re.sub(r'\\*\\*', '', raw)               # Remove markdown bold\n",
        "    raw = re.sub(r'\\s*\\.?\\s*$', '', raw)         # Trailing periods/spaces\n",
        "\n",
        "    # Remove common preambles\n",
        "    for prefix in ['The full name of the US Paralympic athlete is ',\n",
        "                   'The full name of the US Paralympic athlete ',\n",
        "                   'The full name of the US Paralympic ',\n",
        "                   'The full name of the US ',\n",
        "                   'The US Paralympic athlete is ',\n",
        "                   \"The US Paralympic athlete's full name is \"]:\n",
        "        if raw.startswith(prefix):\n",
        "            raw = raw[len(prefix):]\n",
        "\n",
        "    # Remove suffixes like \"is the full name...\"\n",
        "    for suffix in [' is the full name', '. While the search results',\n",
        "                   \"'s full name is not explicitly\"]:\n",
        "        idx = raw.find(suffix)\n",
        "        if idx > 0:\n",
        "            raw = raw[:idx]\n",
        "\n",
        "    raw = raw.strip('\"\\'.,;: ')\n",
        "\n",
        "    if not raw:\n",
        "        continue\n",
        "\n",
        "    # Take first two or three words as the name\n",
        "    words = raw.split()\n",
        "    if len(words) >= 2 and all(len(w) >= 2 for w in words[:2]):\n",
        "        # Use first + last (skip middle names/suffixes)\n",
        "        if len(words) > 3:\n",
        "            name = f\"{words[0]} {words[1]}\"\n",
        "        else:\n",
        "            name = ' '.join(words)\n",
        "\n",
        "        clean = smart_title_case(name)\n",
        "\n",
        "        # Apply\n",
        "        team_usa_athletes.loc[\n",
        "            team_usa_athletes['athlete_id'] == row['athlete_id'], 'name'] = clean\n",
        "        rescued += 1\n",
        "        print(f'  {original:25s} -> {clean}')\n",
        "\n",
        "print(f'\\nâœ… Rescued {rescued} names')\n",
        "\n",
        "# Final count\n",
        "still_abbrev = team_usa_athletes[\n",
        "    (team_usa_athletes['source'] == 'katiepress') &\n",
        "    (team_usa_athletes['name'].apply(is_abbreviated))\n",
        "]\n",
        "print(f'ðŸ“‹ Still abbreviated: {len(still_abbrev)} out of {len(team_usa_athletes[team_usa_athletes[\"source\"] == \"katiepress\"])}')"
      ],
      "metadata": {
        "id": "afQknqt0l__R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6.5, Step 2c: Manual rescue from error messages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Direct mappings where we can clearly see the correct name\n",
        "manual_fixes = {\n",
        "    'Lewis B': 'Brett Lewis',\n",
        "    'Kelly D': 'Daniel Kelly',\n",
        "    'Stein R': 'Ronald Stein',\n",
        "    'Smith J': 'June Smith',\n",
        "    'Burkett H': 'Harold Burkett',\n",
        "    'Quinlan D': 'Darleen Quinlan',\n",
        "    'Brinkmann C': 'Curt Brinkman',\n",
        "    'Reed R': 'Raphew Tyrone Reed Jr',\n",
        "    'Stuewevandello D': 'Donald Vandello',\n",
        "    'Classonlevis R': 'Reno Levis',\n",
        "    'Crasedonaldson': 'Crase Donaldson',\n",
        "    'Cameron': 'James Cameron',\n",
        "    'Noe M': 'Michael Noe',\n",
        "    'Martin P': 'Patricia Martin',\n",
        "    'Parker M': 'Martin Parker',\n",
        "    'Lewellyn J': 'J. Llewellyn',  # Only initial available\n",
        "}\n",
        "\n",
        "applied = 0\n",
        "for current_name, fix in manual_fixes.items():\n",
        "    mask = (team_usa_athletes['source'] == 'katiepress') & (team_usa_athletes['name'] == current_name)\n",
        "    matches = mask.sum()\n",
        "    if matches > 0:\n",
        "        team_usa_athletes.loc[mask, 'name'] = smart_title_case(fix)\n",
        "        applied += 1\n",
        "        print(f'  {current_name:30s} -> {smart_title_case(fix)}')\n",
        "    else:\n",
        "        print(f'  {current_name:30s} -> NOT FOUND in dataframe')\n",
        "\n",
        "print(f'\\nâœ… Applied {applied} manual fixes')\n",
        "\n",
        "# Final count\n",
        "still_abbrev = team_usa_athletes[\n",
        "    (team_usa_athletes['source'] == 'katiepress') &\n",
        "    (team_usa_athletes['name'].apply(is_abbreviated))\n",
        "]\n",
        "print(f'ðŸ“‹ Still abbreviated: {len(still_abbrev)} out of {len(team_usa_athletes[team_usa_athletes[\"source\"] == \"katiepress\"])}')\n",
        "\n",
        "# Show what's left\n",
        "if len(still_abbrev) > 0:\n",
        "    print(f'\\nRemaining abbreviated names:')\n",
        "    for name in sorted(still_abbrev['name'].tolist()):\n",
        "        print(f'  {name}')"
      ],
      "metadata": {
        "id": "egu5cLB0BKG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6.5, Step 3: Update results table + QC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Sync athlete_name in results table\n",
        "name_lookup = dict(zip(team_usa_athletes['athlete_id'], team_usa_athletes['name']))\n",
        "team_usa_results['athlete_name'] = team_usa_results['athlete_id'].map(name_lookup).fillna(\n",
        "    team_usa_results['athlete_name'].apply(clean_name_mechanical))\n",
        "\n",
        "# Also update athletes_final and results_final if they exist\n",
        "if 'athletes_final' in dir():\n",
        "    athletes_final['name'] = team_usa_athletes['name']\n",
        "if 'results_final' in dir():\n",
        "    results_final['athlete_name'] = team_usa_results['athlete_name']\n",
        "\n",
        "# QC Report\n",
        "print('=' * 70)\n",
        "print('PHASE 6.5 QC: NAME CLEANUP')\n",
        "print('=' * 70)\n",
        "\n",
        "# Check for remaining bullets\n",
        "bullets_remaining = team_usa_athletes['name'].str.contains('â€¢', na=False).sum()\n",
        "print(f'\\nðŸ” REMAINING ISSUES')\n",
        "print(f'  Bullet chars remaining:   {bullets_remaining}')\n",
        "\n",
        "# Check for all-caps names (3+ consecutive uppercase letters suggest uncleaned)\n",
        "all_caps = team_usa_athletes['name'].apply(\n",
        "    lambda x: bool(re.search(r'[A-Z]{3,}', str(x))) if pd.notna(x) else False).sum()\n",
        "print(f'  All-caps patterns:        {all_caps}')\n",
        "\n",
        "# Casing distribution\n",
        "has_upper_lower = team_usa_athletes['name'].apply(\n",
        "    lambda x: bool(re.search(r'[a-z]', str(x)) and re.search(r'[A-Z]', str(x)))\n",
        "    if pd.notna(x) else False).sum()\n",
        "print(f'  Mixed case (normal):      {has_upper_lower:,} / {len(team_usa_athletes):,}')\n",
        "\n",
        "# Katiepress extraction stats\n",
        "kp_athletes = team_usa_athletes[team_usa_athletes['source'] == 'katiepress']\n",
        "kp_multiword = kp_athletes['name'].apply(lambda x: len(str(x).split()) >= 2).sum()\n",
        "print(f'\\nðŸ“‹ KATIEPRESS NAME RECOVERY')\n",
        "kp_athletes = team_usa_athletes[team_usa_athletes['source'] == 'katiepress']\n",
        "still_abbrev = kp_athletes['name'].apply(is_abbreviated).sum()\n",
        "print(f'  Total katiepress:         {len(kp_athletes):,}')\n",
        "print(f'  Full names recovered:     {len(kp_athletes) - still_abbrev:,} ({(len(kp_athletes) - still_abbrev)/len(kp_athletes)*100:.1f}%)')\n",
        "print(f'  Still abbreviated:        {still_abbrev}')\n",
        "\n",
        "# Source-by-source samples\n",
        "print(f'\\nðŸ“¦ SAMPLES BY SOURCE')\n",
        "for src in ['keithgalli', 'paris2024', 'piterfm', 'katiepress']:\n",
        "    sub = team_usa_athletes[team_usa_athletes['source'] == src]\n",
        "    if len(sub) > 0:\n",
        "        samples = sub.sample(min(3, len(sub)), random_state=42)\n",
        "        names = ', '.join(samples['name'].tolist())\n",
        "        print(f'  {src:15s} {names}')\n",
        "\n",
        "print(f'\\n{\"=\" * 70}')"
      ],
      "metadata": {
        "id": "VHJ8Xe57Vlzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Find and fix names with embedded newlines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "newline_names = team_usa_athletes[team_usa_athletes['name'].str.contains('\\n', na=False)]\n",
        "print(f'Names with embedded newlines: {len(newline_names)}\\n')\n",
        "\n",
        "for _, row in newline_names.iterrows():\n",
        "    print(f'  athlete_id: {row[\"athlete_id\"]}')\n",
        "    print(f'  name: {repr(row[\"name\"])}')\n",
        "    print(f'  source: {row[\"source\"]}')\n",
        "    print()\n",
        "\n",
        "# Also check profiles\n",
        "newline_profiles = team_usa_athletes[team_usa_athletes['profile_summary'].str.contains('\\n', na=False)]\n",
        "print(f'Profiles with embedded newlines: {len(newline_profiles)}')"
      ],
      "metadata": {
        "id": "ZpLKkF3OIKyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Phase 6.5, Step 4: Re-save cleaned data to GCS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "ATHLETE_COLUMNS = [\n",
        "    'athlete_id', 'name', 'gender', 'birth_date', 'birth_place', 'birth_country',\n",
        "    'height_cm', 'weight_kg',\n",
        "    'games_type', 'primary_sport', 'classification_code',\n",
        "    'first_games_year', 'last_games_year', 'games_count',\n",
        "    'gold_count', 'silver_count', 'bronze_count', 'total_medals',\n",
        "    'reason', 'hero', 'philosophy', 'other_sports', 'coach',\n",
        "    'hobbies', 'occupation', 'education',\n",
        "    'profile_summary', 'embedding',\n",
        "    'source',\n",
        "]\n",
        "\n",
        "RESULT_COLUMNS = [\n",
        "    'athlete_id', 'athlete_name', 'games_year', 'games_season',\n",
        "    'games_type', 'sport', 'discipline', 'event', 'medal',\n",
        "    'classification_code',\n",
        "]\n",
        "\n",
        "# Drop temporary columns\n",
        "if 'name_original' in team_usa_athletes.columns:\n",
        "    team_usa_athletes.drop(columns=['name_original'], inplace=True)\n",
        "\n",
        "# Strip newlines from all text fields (BQ doesn't handle quoted newlines by default)\n",
        "text_cols = ['name', 'profile_summary', 'reason', 'hero', 'philosophy',\n",
        "             'other_sports', 'coach', 'hobbies', 'occupation', 'education']\n",
        "for col in text_cols:\n",
        "    if col in team_usa_athletes.columns:\n",
        "        count = team_usa_athletes[col].str.contains('\\n', na=False).sum()\n",
        "        if count > 0:\n",
        "            print(f'  Stripped newlines from {col}: {count} rows')\n",
        "            team_usa_athletes[col] = team_usa_athletes[col].str.replace('\\n', ' ', regex=False)\n",
        "\n",
        "# Sync athlete_name in results table\n",
        "name_lookup = dict(zip(team_usa_athletes['athlete_id'], team_usa_athletes['name']))\n",
        "team_usa_results['athlete_name'] = team_usa_results['athlete_id'].map(name_lookup).fillna(\n",
        "    team_usa_results['athlete_name'])\n",
        "\n",
        "# Apply column ordering\n",
        "athletes_export = team_usa_athletes[[c for c in ATHLETE_COLUMNS if c in team_usa_athletes.columns]].copy()\n",
        "results_export = team_usa_results[[c for c in RESULT_COLUMNS if c in team_usa_results.columns]].copy()\n",
        "\n",
        "# Serialize embeddings as JSON\n",
        "athletes_export['embedding'] = athletes_export['embedding'].apply(\n",
        "    lambda x: json.dumps(x) if isinstance(x, list) else None)\n",
        "\n",
        "ath_path = '/tmp/team_usa_athletes_final.csv'\n",
        "res_path = '/tmp/team_usa_results_final.csv'\n",
        "\n",
        "athletes_export.to_csv(ath_path, index=False, encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "results_export.to_csv(res_path, index=False, encoding='utf-8')\n",
        "\n",
        "!gcloud storage cp {ath_path} gs://class-demo/team-usa/final/team_usa_athletes.csv\n",
        "!gcloud storage cp {res_path} gs://class-demo/team-usa/final/team_usa_results.csv\n",
        "\n",
        "!gcloud storage ls -l gs://class-demo/team-usa/final/\n",
        "\n",
        "print(f'\\nâœ… Cleaned data saved to gs://class-demo/team-usa/final/')\n",
        "print(f'   Athletes: {len(athletes_export):,} rows')\n",
        "print(f'   Results:  {len(results_export):,} rows')"
      ],
      "metadata": {
        "id": "KkiRMnWeVpe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8T8_kPO_VsI-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "name": "team_usa_data_profiling.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}