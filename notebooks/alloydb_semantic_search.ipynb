{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "alloydb_semantic_search.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AlloyDB Semantic Search: Team USA Athletes\n\nIn this notebook, you'll set up **AlloyDB** as the operational search layer for the Team USA analytics platform.\n\nWhile BigQuery handles analytical queries (aggregations, clustering, trends), AlloyDB provides the **fast, real-time similarity search** that an AI agent needs \u2014 answering questions like *\"which athletes had careers most similar to Simone Biles?\"* in milliseconds.\n\n**What you'll do:**\n1. Create the `team_usa` database on your AlloyDB cluster\n2. Load 12,222 athletes with pre-computed vector embeddings\n3. Build a ScaNN index for Google-scale similarity search\n4. Run semantic queries that find athletes by meaning, not keywords\n5. Search by natural language description \u2014 the same pattern the agent will use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Step 1: Configure Your Environment\n\nUpdate the fields below with your lab-specific values. The cluster and instance IDs should match your Terraform deployment from Task 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\nREGION = \"us-central1\"  # @param {type:\"string\"}\nUSER_EMAIL = \"student@qwiklabs.net\"  # @param {type:\"string\"}\n\n# These should match your Terraform deployment\nCLUSTER_ID = \"team-usa-cluster\"  # @param {type:\"string\"}\nINSTANCE_ID = \"team-usa-primary\"  # @param {type:\"string\"}\n\n# Derived values\nDATABASE_NAME = \"team_usa\"\nINSTANCE_URI = f\"projects/{PROJECT_ID}/locations/{REGION}/clusters/{CLUSTER_ID}/instances/{INSTANCE_ID}\"\n\nprint(\"\ud83d\udccb Configuration:\")\nprint(f\"   Project:  {PROJECT_ID}\")\nprint(f\"   Region:   {REGION}\")\nprint(f\"   Cluster:  {CLUSTER_ID}\")\nprint(f\"   Instance: {INSTANCE_ID}\")\nprint(f\"   Database: {DATABASE_NAME}\")\nprint(f\"   User:     {USER_EMAIL}\")\nprint(f\"\\n   Instance URI: {INSTANCE_URI}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Step 2: Install Dependencies & Connect to AlloyDB\n\nThe **AlloyDB Python Connector** handles secure IAM authentication \u2014 no passwords needed. Your Google Cloud identity *is* your database identity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install --quiet google-cloud-alloydb-connector[pg8000] sqlalchemy pandas"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.cloud.alloydb.connector import Connector, IPTypes\nimport pg8000\nimport sqlalchemy\nfrom sqlalchemy import text\n\nconnector = Connector()\n\ndef get_connection(database=\"postgres\"):\n    \"\"\"Create a connection to AlloyDB.\"\"\"\n    conn = connector.connect(\n        INSTANCE_URI,\n        \"pg8000\",\n        user=USER_EMAIL,\n        db=database,\n        enable_iam_auth=True,\n        ip_type=IPTypes.PUBLIC,\n    )\n    return conn\n\n# Test the connection against the default postgres database\nconn = get_connection(\"postgres\")\ncursor = conn.cursor()\ncursor.execute(\"SELECT version()\")\nversion = cursor.fetchone()[0]\nconn.close()\n\nprint(f\"\u2705 Connected to AlloyDB!\")\nprint(f\"\ud83d\udd10 Authenticated as: {USER_EMAIL}\")\nprint(f\"\ud83d\udcca {version[:70]}...\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Step 3: Create the Database\n\nThe AlloyDB cluster has been running since Task 1, but only with the default `postgres` database. Let's create a dedicated `team_usa` database for our athlete data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CREATE DATABASE can't run inside a transaction, so we use autocommit\nconn = get_connection(\"postgres\")\nconn.autocommit = True\ncursor = conn.cursor()\n\ncursor.execute(\"SELECT 1 FROM pg_database WHERE datname = %s\", (DATABASE_NAME,))\nif cursor.fetchone() is None:\n    cursor.execute(f\"CREATE DATABASE {DATABASE_NAME}\")\n    print(f\"\u2705 Database '{DATABASE_NAME}' created!\")\nelse:\n    print(f\"\u2139\ufe0f  Database '{DATABASE_NAME}' already exists.\")\n\nconn.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now connect to the new `team_usa` database. This is the connection you'll use for everything that follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a SQLAlchemy engine connected to team_usa\nengine = sqlalchemy.create_engine(\n    \"postgresql+pg8000://\",\n    creator=lambda: get_connection(DATABASE_NAME),\n)\n\n# Verify the connection\nwith engine.connect() as conn:\n    result = conn.execute(text(\"SELECT current_database(), current_user\"))\n    db, user = result.fetchone()\n    print(f\"\u2705 Connected to database: {db}\")\n    print(f\"   Authenticated as: {user}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Step 4: Enable Extensions\n\nTwo extensions give AlloyDB its vector search capabilities:\n\n- **vector** \u2014 Adds the `VECTOR` data type for storing embeddings\n- **alloydb_scann** \u2014 Google's [ScaNN](https://github.com/google-research/google-research/tree/master/scann) (Scalable Nearest Neighbors) algorithm for high-performance vector similarity search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with engine.connect() as conn:\n    conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector\"))\n    conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS alloydb_scann\"))\n    conn.commit()\n\n    # Verify\n    result = conn.execute(text(\n        \"SELECT extname, extversion FROM pg_extension \"\n        \"WHERE extname IN ('vector', 'alloydb_scann') ORDER BY extname\"\n    ))\n    for row in result:\n        print(f\"\u2705 {row[0]} v{row[1]}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Step 5: Create the Athletes Table\n\nThe table includes standard columns for display and filtering, plus a `VECTOR(3072)` column for the pre-computed embeddings.\n\nWe're loading 13 of the 29 columns from the full dataset \u2014 the ones needed for search results display, filtering, and vector search. Columns like birth place, height/weight, and sparse Paralympic personal details are left in BigQuery where they're available if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with engine.connect() as conn:\n    conn.execute(text(\"DROP TABLE IF EXISTS athletes\"))\n    conn.execute(text(\"\"\"\n        CREATE TABLE athletes (\n            athlete_id          VARCHAR(12)  PRIMARY KEY,\n            name                VARCHAR      NOT NULL,\n            gender              VARCHAR,\n            games_type          VARCHAR      NOT NULL,\n            primary_sport       VARCHAR      NOT NULL,\n            classification_code VARCHAR,\n            total_medals        INTEGER      DEFAULT 0,\n            gold_count          INTEGER      DEFAULT 0,\n            games_count         INTEGER      DEFAULT 0,\n            first_games_year    INTEGER,\n            last_games_year     INTEGER,\n            profile_summary     TEXT,\n            embedding           VECTOR(3072)\n        )\n    \"\"\"))\n    conn.commit()\n\nprint(\"\u2705 Athletes table created!\")\nprint(\"   Columns: athlete_id, name, gender, games_type, primary_sport,\")\nprint(\"            classification_code, total_medals, gold_count, games_count,\")\nprint(\"            first_games_year, last_games_year, profile_summary, embedding\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Step 6: Load Athlete Data\n\nWe'll load data using AlloyDB's **GCS import** \u2014 a server-side bulk load that's significantly faster than inserting rows through a client connection. The process:\n\n1. Download the source CSV and select the columns we need\n2. Stage the prepared data in a Cloud Storage bucket\n3. Import directly into AlloyDB via server-side load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6.1: Download and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n\n# Download the full athletes CSV from the lab's GCS bucket\nprint(\"\u23f3 Downloading athlete data from Cloud Storage...\")\ndf = pd.read_csv(\"gs://class-demo/team-usa/final/team_usa_athletes.csv\")\nprint(f\"   Downloaded {len(df):,} athletes with {len(df.columns)} columns\")\n\n# Select the 13 columns we need, in table column order\ncolumns = [\n    'athlete_id', 'name', 'gender', 'games_type', 'primary_sport',\n    'classification_code', 'total_medals', 'gold_count', 'games_count',\n    'first_games_year', 'last_games_year', 'profile_summary', 'embedding'\n]\ndf_alloydb = df[columns].copy()\n\n# Save without headers \u2014 the GCS import maps columns positionally\nlocal_path = \"/tmp/athletes_for_alloydb.csv\"\ndf_alloydb.to_csv(local_path, index=False, header=False)\n\nprint(f\"\u2705 Prepared {len(df_alloydb):,} athletes ({len(columns)} columns) for import\")\nprint(f\"   Saved to {local_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6.2: Stage data to Cloud Storage\n\nAlloyDB's GCS import reads directly from Cloud Storage, so we need to stage our prepared file in a bucket that the AlloyDB service agent can access."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get the project number (needed to identify the AlloyDB service agent)\nproject_number = !gcloud projects describe {PROJECT_ID} --format=\"value(projectNumber)\"\nproject_number = project_number[0].strip()\nservice_agent = f\"service-{project_number}@gcp-sa-alloydb.iam.gserviceaccount.com\"\n\nSTAGING_BUCKET = f\"{PROJECT_ID}-team-usa-staging\"\n\n# Create the staging bucket\n!gcloud storage buckets create gs://{STAGING_BUCKET} --location={REGION} 2>/dev/null || true\n\n# Grant AlloyDB service agent read access\n!gcloud storage buckets add-iam-policy-binding gs://{STAGING_BUCKET} \\\n    --member=\"serviceAccount:{service_agent}\" \\\n    --role=\"roles/storage.objectViewer\" --quiet\n\n# Upload the prepared CSV\n!gcloud storage cp {local_path} gs://{STAGING_BUCKET}/athletes_for_alloydb.csv --quiet\n\nprint(f\"\\n\u2705 Data staged to gs://{STAGING_BUCKET}/athletes_for_alloydb.csv\")\nprint(f\"   AlloyDB service agent ({service_agent[:40]}...) granted read access\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6.3: Import into AlloyDB\n\nThis triggers a **server-side import** \u2014 AlloyDB reads the CSV directly from Cloud Storage. Much faster than sending rows through a client connection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\u23f3 Importing athletes into AlloyDB (this may take 1-3 minutes)...\")\n\n!gcloud alloydb clusters import csv {CLUSTER_ID} \\\n    --project={PROJECT_ID} \\\n    --region={REGION} \\\n    --database={DATABASE_NAME} \\\n    --table=athletes \\\n    --gcs-uri=gs://{STAGING_BUCKET}/athletes_for_alloydb.csv \\\n    --columns=athlete_id,name,gender,games_type,primary_sport,classification_code,total_medals,gold_count,games_count,first_games_year,last_games_year,profile_summary,embedding\n\nprint(\"\\n\u2705 Import complete!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Step 7: Verify the Data\n\nLet's confirm everything loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with engine.connect() as conn:\n    # Total count\n    result = conn.execute(text(\"SELECT COUNT(*) FROM athletes\"))\n    total = result.scalar()\n\n    # Embedding count\n    result = conn.execute(text(\"SELECT COUNT(*) FROM athletes WHERE embedding IS NOT NULL\"))\n    with_embeddings = result.scalar()\n\n    # Games type breakdown\n    result = conn.execute(text(\n        \"SELECT games_type, COUNT(*) FROM athletes GROUP BY games_type ORDER BY games_type\"\n    ))\n    breakdown = result.fetchall()\n\n    print(f\"\u2705 Athletes loaded: {total:,}\")\n    print(f\"   With embeddings: {with_embeddings:,}\")\n    for games_type, count in breakdown:\n        print(f\"   {games_type}: {count:,}\")\n\n    # Sample some athletes\n    print(\"\\n\ud83d\udccb Sample athletes:\")\n    result = conn.execute(text(\"\"\"\n        SELECT name, primary_sport, games_type, gold_count, total_medals,\n               first_games_year || '-' || last_games_year AS career\n        FROM athletes\n        WHERE total_medals > 5\n        ORDER BY total_medals DESC\n        LIMIT 8\n    \"\"\"))\n    print(f\"   {'Name':<30s} {'Sport':<25s} {'Type':<12s} {'Gold':>4s} {'Total':>5s} {'Career'}\")\n    print(f\"   {'-'*30} {'-'*25} {'-'*12} {'-'*4} {'-'*5} {'-'*9}\")\n    for row in result:\n        print(f\"   {row[0]:<30s} {row[1]:<25s} {row[2]:<12s} {row[3]:>4d} {row[4]:>5d} {row[5]}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Step 8: Create the ScaNN Index\n\nWithout an index, every similarity search compares your query against all 12,222 embeddings \u2014 a brute-force scan. The **ScaNN** index pre-organizes vectors into partitions so queries search only the most promising candidates.\n\n- **cosine** distance: Measures the angle between vectors. Ideal for text embeddings.\n- **num_leaves = 50**: Partitions vectors into 50 clusters for search.\n- **quantizer = 'sq8'**: Compresses dimensions to 8-bit, reducing memory while preserving accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n\nwith engine.connect() as conn:\n    print(\"\u23f3 Creating ScaNN index...\")\n    start = time.time()\n    conn.execute(text(\"\"\"\n        CREATE INDEX athletes_embedding_idx\n        ON athletes USING scann (embedding cosine)\n        WITH (num_leaves = 50, quantizer = 'sq8')\n    \"\"\"))\n    conn.commit()\n    elapsed = time.time() - start\n    print(f\"\u2705 ScaNN index created in {elapsed:.1f}s\")\n\n    # Verify the index exists\n    result = conn.execute(text(\"\"\"\n        SELECT indexname, indexdef\n        FROM pg_indexes\n        WHERE tablename = 'athletes' AND indexname = 'athletes_embedding_idx'\n    \"\"\"))\n    idx = result.fetchone()\n    if idx:\n        print(f\"   Index: {idx[0]}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Step 9: Find Similar Athletes\n\nNow for the payoff. This query looks up a specific athlete's embedding and finds the nearest neighbors in vector space \u2014 athletes whose career \"fingerprints\" are closest in 3,072-dimensional space.\n\nThe `<=>` operator computes **cosine distance** between vectors (lower = more similar). We convert to a similarity score (higher = more similar) with `1 - distance`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simone Biles \u2014 Dominant multi-Games gymnast"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with engine.connect() as conn:\n    result = conn.execute(text(\"\"\"\n        SELECT\n            a2.name,\n            a2.primary_sport,\n            a2.games_type,\n            a2.classification_code,\n            a2.gold_count,\n            a2.total_medals,\n            a2.first_games_year || '-' || a2.last_games_year AS career,\n            ROUND((1 - (a1.embedding <=> a2.embedding))::numeric, 3) AS similarity\n        FROM athletes a1\n        CROSS JOIN athletes a2\n        WHERE a1.name = 'Simone Biles'\n          AND a2.athlete_id != a1.athlete_id\n          AND a2.embedding IS NOT NULL\n        ORDER BY a1.embedding <=> a2.embedding\n        LIMIT 10\n    \"\"\"))\n\n    print(\"\ud83d\udd0d Athletes most similar to Simone Biles:\\n\")\n    print(f\"   {'Name':<30s} {'Sport':<22s} {'Type':<10s} {'Gold':>4s} {'Medals':>6s} {'Career':<10s} {'Sim':>5s}\")\n    print(f\"   {'-'*30} {'-'*22} {'-'*10} {'-'*4} {'-'*6} {'-'*10} {'-'*5}\")\n    for row in result:\n        cls = f\" ({row[3]})\" if row[3] else \"\"\n        print(f\"   {row[0]:<30s} {row[1]:<22s} {row[2]:<10s} {row[4]:>4d} {row[5]:>6d} {row[6]:<10s} {row[7]:>5.3f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trischa Zorn \u2014 Most decorated Paralympic athlete (blind swimmer, 55 medals)\n\nThis result is especially interesting. Watch for visually impaired athletes from *different sports* appearing in the results \u2014 the embeddings capture the disability classification signal, connecting athletes across sport boundaries by shared experience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with engine.connect() as conn:\n    result = conn.execute(text(\"\"\"\n        SELECT\n            a2.name,\n            a2.primary_sport,\n            a2.games_type,\n            a2.classification_code,\n            a2.gold_count,\n            a2.total_medals,\n            a2.first_games_year || '-' || a2.last_games_year AS career,\n            ROUND((1 - (a1.embedding <=> a2.embedding))::numeric, 3) AS similarity\n        FROM athletes a1\n        CROSS JOIN athletes a2\n        WHERE a1.name = 'Trischa Zorn'\n          AND a2.athlete_id != a1.athlete_id\n          AND a2.embedding IS NOT NULL\n        ORDER BY a1.embedding <=> a2.embedding\n        LIMIT 10\n    \"\"\"))\n\n    print(\"\ud83d\udd0d Athletes most similar to Trischa Zorn:\\n\")\n    print(f\"   {'Name':<30s} {'Sport':<22s} {'Type':<10s} {'Class':<8s} {'Gold':>4s} {'Medals':>6s} {'Career':<10s} {'Sim':>5s}\")\n    print(f\"   {'-'*30} {'-'*22} {'-'*10} {'-'*8} {'-'*4} {'-'*6} {'-'*10} {'-'*5}\")\n    for row in result:\n        cls = row[3] if row[3] else \"\"\n        print(f\"   {row[0]:<30s} {row[1]:<22s} {row[2]:<10s} {cls:<8s} {row[4]:>4d} {row[5]:>6d} {row[6]:<10s} {row[7]:>5.3f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Step 10: Search by Natural Language Description\n\nHere's what the agent will do in Task 5: instead of looking up a specific athlete's embedding, **generate an embedding from a text description on the fly** using AlloyDB's built-in `embedding()` function, then find the nearest athletes.\n\nThis means you can search by *meaning* \u2014 describe what you're looking for in plain English, and AlloyDB finds the closest matches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Try different descriptions by changing this variable!\nsearch_description = \"dominant swimmer with many gold medals across multiple Olympics\"\n\nwith engine.connect() as conn:\n    result = conn.execute(text(\"\"\"\n        SELECT\n            name,\n            primary_sport,\n            games_type,\n            classification_code,\n            gold_count,\n            total_medals,\n            first_games_year || '-' || last_games_year AS career,\n            ROUND((1 - (embedding <=> embedding('gemini-embedding-001', :query)::vector))::numeric, 3) AS similarity\n        FROM athletes\n        WHERE embedding IS NOT NULL\n        ORDER BY embedding <=> embedding('gemini-embedding-001', :query)::vector\n        LIMIT 10\n    \"\"\"), {\"query\": search_description})\n\n    print(f\"\ud83d\udd0d Athletes matching: \\\"{search_description}\\\"\\n\")\n    print(f\"   {'Name':<30s} {'Sport':<22s} {'Type':<10s} {'Gold':>4s} {'Medals':>6s} {'Career':<10s} {'Sim':>5s}\")\n    print(f\"   {'-'*30} {'-'*22} {'-'*10} {'-'*4} {'-'*6} {'-'*10} {'-'*5}\")\n    for row in result:\n        print(f\"   {row[0]:<30s} {row[1]:<22s} {row[2]:<10s} {row[4]:>4d} {row[5]:>6d} {row[6]:<10s} {row[7]:>5.3f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Try other descriptions!\n\nChange the `search_description` variable above and re-run the cell. Some ideas:\n\n- `\"Paralympic track and field athlete competing in wheelchair events\"`\n- `\"Winter sport athlete with a long career spanning many Games\"`\n- `\"Young gymnast who became a cultural icon\"`\n- `\"Team sport athlete who won gold in basketball or volleyball\"`\n\nThis is exactly the query pattern the agent will use in Task 5 \u2014 converting a user's natural language question into a vector and searching AlloyDB for the most relevant athletes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## \u2705 Task 4 Complete!\n\nYou've set up AlloyDB as the operational search layer for the Team USA platform:\n\n- **Created** the `team_usa` database with vector extensions\n- **Loaded** 12,222 athletes with 3,072-dimension embeddings\n- **Built** a ScaNN index for fast similarity search\n- **Searched** by specific athlete (Simone Biles, Trischa Zorn)\n- **Searched** by natural language description using `embedding()`\n\n**What's next:** In Task 5, you'll build an AI agent that combines BigQuery (analytics, clustering) with AlloyDB (similarity search) \u2014 choosing the right tool for each question automatically."
      ]
    }
  ]
}